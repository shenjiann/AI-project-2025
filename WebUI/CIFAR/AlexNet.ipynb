{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81bea74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82485689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),  # LRN层\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),  # LRN层\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61de1bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_list.append([])\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}')\n",
    "                loss_list[-1].append(loss.item())\n",
    "    return loss_list\n",
    "\n",
    "\n",
    "# 测试模型\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa12a928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:10<00:00, 15.6MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Epoch 0, Batch 0, Loss: 2.3025145530700684\n",
      "Epoch 0, Batch 100, Loss: 1.8794265985488892\n",
      "Epoch 0, Batch 200, Loss: 1.476253867149353\n",
      "Epoch 0, Batch 300, Loss: 1.2999252080917358\n",
      "Epoch 1, Batch 0, Loss: 1.448592185974121\n",
      "Epoch 1, Batch 100, Loss: 1.1500862836837769\n",
      "Epoch 1, Batch 200, Loss: 1.1847513914108276\n",
      "Epoch 1, Batch 300, Loss: 1.2623569965362549\n",
      "Epoch 2, Batch 0, Loss: 1.0048755407333374\n",
      "Epoch 2, Batch 100, Loss: 0.7863324880599976\n",
      "Epoch 2, Batch 200, Loss: 0.9242383241653442\n",
      "Epoch 2, Batch 300, Loss: 0.8784202933311462\n",
      "Epoch 3, Batch 0, Loss: 0.923408031463623\n",
      "Epoch 3, Batch 100, Loss: 0.8275366425514221\n",
      "Epoch 3, Batch 200, Loss: 0.8603533506393433\n",
      "Epoch 3, Batch 300, Loss: 0.7879496216773987\n",
      "Epoch 4, Batch 0, Loss: 0.7173369526863098\n",
      "Epoch 4, Batch 100, Loss: 0.6450088024139404\n",
      "Epoch 4, Batch 200, Loss: 0.7726433873176575\n",
      "Epoch 4, Batch 300, Loss: 0.7827979326248169\n",
      "Epoch 5, Batch 0, Loss: 0.58682781457901\n",
      "Epoch 5, Batch 100, Loss: 0.584318995475769\n",
      "Epoch 5, Batch 200, Loss: 0.6284078359603882\n",
      "Epoch 5, Batch 300, Loss: 0.582612156867981\n",
      "Epoch 6, Batch 0, Loss: 0.6319862604141235\n",
      "Epoch 6, Batch 100, Loss: 0.5994828343391418\n",
      "Epoch 6, Batch 200, Loss: 0.6845492720603943\n",
      "Epoch 6, Batch 300, Loss: 0.5888873934745789\n",
      "Epoch 7, Batch 0, Loss: 0.5473992228507996\n",
      "Epoch 7, Batch 100, Loss: 0.4887164831161499\n",
      "Epoch 7, Batch 200, Loss: 0.5107249617576599\n",
      "Epoch 7, Batch 300, Loss: 0.6477738618850708\n",
      "Epoch 8, Batch 0, Loss: 0.4495954215526581\n",
      "Epoch 8, Batch 100, Loss: 0.4225381016731262\n",
      "Epoch 8, Batch 200, Loss: 0.40890270471572876\n",
      "Epoch 8, Batch 300, Loss: 0.3832603991031647\n",
      "Epoch 9, Batch 0, Loss: 0.4019513428211212\n",
      "Epoch 9, Batch 100, Loss: 0.2830228805541992\n",
      "Epoch 9, Batch 200, Loss: 0.475725919008255\n",
      "Epoch 9, Batch 300, Loss: 0.41419053077697754\n",
      "Epoch 10, Batch 0, Loss: 0.3228982090950012\n",
      "Epoch 10, Batch 100, Loss: 0.3410005569458008\n",
      "Epoch 10, Batch 200, Loss: 0.44836172461509705\n",
      "Epoch 10, Batch 300, Loss: 0.27658453583717346\n",
      "Epoch 11, Batch 0, Loss: 0.2781320810317993\n",
      "Epoch 11, Batch 100, Loss: 0.4247899651527405\n",
      "Epoch 11, Batch 200, Loss: 0.20133759081363678\n",
      "Epoch 11, Batch 300, Loss: 0.20276932418346405\n",
      "Epoch 12, Batch 0, Loss: 0.418125718832016\n",
      "Epoch 12, Batch 100, Loss: 0.36237236857414246\n",
      "Epoch 12, Batch 200, Loss: 0.3574797511100769\n",
      "Epoch 12, Batch 300, Loss: 0.28855597972869873\n",
      "Epoch 13, Batch 0, Loss: 0.2128104567527771\n",
      "Epoch 13, Batch 100, Loss: 0.2802365720272064\n",
      "Epoch 13, Batch 200, Loss: 0.3207281529903412\n",
      "Epoch 13, Batch 300, Loss: 0.28754571080207825\n",
      "Epoch 14, Batch 0, Loss: 0.21760547161102295\n",
      "Epoch 14, Batch 100, Loss: 0.2975245416164398\n",
      "Epoch 14, Batch 200, Loss: 0.21402961015701294\n",
      "Epoch 14, Batch 300, Loss: 0.2933212220668793\n",
      "Epoch 15, Batch 0, Loss: 0.20482993125915527\n",
      "Epoch 15, Batch 100, Loss: 0.1838177889585495\n",
      "Epoch 15, Batch 200, Loss: 0.2439332753419876\n",
      "Epoch 15, Batch 300, Loss: 0.2955552339553833\n",
      "Epoch 16, Batch 0, Loss: 0.1915416568517685\n",
      "Epoch 16, Batch 100, Loss: 0.18666872382164001\n",
      "Epoch 16, Batch 200, Loss: 0.25364169478416443\n",
      "Epoch 16, Batch 300, Loss: 0.1389031559228897\n",
      "Epoch 17, Batch 0, Loss: 0.18981188535690308\n",
      "Epoch 17, Batch 100, Loss: 0.2557083070278168\n",
      "Epoch 17, Batch 200, Loss: 0.1280679851770401\n",
      "Epoch 17, Batch 300, Loss: 0.24630418419837952\n",
      "Epoch 18, Batch 0, Loss: 0.25915172696113586\n",
      "Epoch 18, Batch 100, Loss: 0.18609867990016937\n",
      "Epoch 18, Batch 200, Loss: 0.25960713624954224\n",
      "Epoch 18, Batch 300, Loss: 0.19292545318603516\n",
      "Epoch 19, Batch 0, Loss: 0.17645221948623657\n",
      "Epoch 19, Batch 100, Loss: 0.09822472184896469\n",
      "Epoch 19, Batch 200, Loss: 0.2576307952404022\n",
      "Epoch 19, Batch 300, Loss: 0.18088041245937347\n",
      "Epoch 20, Batch 0, Loss: 0.19464211165905\n",
      "Epoch 20, Batch 100, Loss: 0.13611723482608795\n",
      "Epoch 20, Batch 200, Loss: 0.1946602314710617\n",
      "Epoch 20, Batch 300, Loss: 0.14723269641399384\n",
      "Epoch 21, Batch 0, Loss: 0.06524849683046341\n",
      "Epoch 21, Batch 100, Loss: 0.2230450063943863\n",
      "Epoch 21, Batch 200, Loss: 0.2597705125808716\n",
      "Epoch 21, Batch 300, Loss: 0.16889426112174988\n",
      "Epoch 22, Batch 0, Loss: 0.10541728138923645\n",
      "Epoch 22, Batch 100, Loss: 0.08432602882385254\n",
      "Epoch 22, Batch 200, Loss: 0.1405903398990631\n",
      "Epoch 22, Batch 300, Loss: 0.15127214789390564\n",
      "Epoch 23, Batch 0, Loss: 0.0871112197637558\n",
      "Epoch 23, Batch 100, Loss: 0.2541000545024872\n",
      "Epoch 23, Batch 200, Loss: 0.10879744589328766\n",
      "Epoch 23, Batch 300, Loss: 0.2619551718235016\n",
      "Epoch 24, Batch 0, Loss: 0.12431631982326508\n",
      "Epoch 24, Batch 100, Loss: 0.10383884608745575\n",
      "Epoch 24, Batch 200, Loss: 0.11836180835962296\n",
      "Epoch 24, Batch 300, Loss: 0.12420975416898727\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "test() missing 1 required positional argument: 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m)\n\u001b[0;32m     29\u001b[0m loss_list \u001b[38;5;241m=\u001b[39m train(model, trainloader, criterion, optimizer, \u001b[38;5;241m25\u001b[39m, device)\n\u001b[1;32m---> 30\u001b[0m test(model, testloader, criterion)\n",
      "\u001b[1;31mTypeError\u001b[0m: test() missing 1 required positional argument: 'device'"
     ]
    }
   ],
   "source": [
    "# 设置随机数种子\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# 设置设备\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=10)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=10)\n",
    "\n",
    "model = AlexNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "loss_list = train(model, trainloader, criterion, optimizer, 25, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a70ee21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0058, Accuracy: 80.43%\n"
     ]
    }
   ],
   "source": [
    "test(model, testloader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4159d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'AlexNet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64494a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
