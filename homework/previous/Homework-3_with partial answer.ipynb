{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homewok 3: Computation Graph\n",
    "\n",
    "Welcome to the course **AI and Deep learning**!\n",
    "\n",
    "Computation graph, especially the backpropagation, is of great importance in deep learning, and it makes the training of various neural networks possible. Since it is so important, backpropagation has been coded for famous deep learning platforms, such as TensorFlow and PyTorch. That is, we only need to define the forward propagation, which is the architecture of the neural network. Then, the backpropagation will be performed automatically. However, in this homework, we will manually code up both forward propagation and backpropagation, so that we will have a better understanding about the computation graph. Hope you enjoy the third homework!  \n",
    "\n",
    "**Learning Goal**: In this homework, we first revisit the logistic regression and use it to illustrate the basic procedure for forward propagation and backpropagation. Then, we move to general fully connected neural networks and use computation graph to optimize it. After this homework, you will know:\n",
    " * The basic procedure to train a model using computation graph\n",
    " * How badly a wrongly specified model performs\n",
    " * How to code up a neural network with one hidden layer.\n",
    " * How differently neural networks perform with different hidden nodes.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content\n",
    "* [1 - Packages](#1)\n",
    "* [2 - Generate a training dataset](#2)\n",
    "  * [2.1 - Generate a training dataset](#2.1)\n",
    "  * [2.2 - Parameter estimation](#2.2)\n",
    "  * [2.3 - Integration](#2.3)\n",
    "* [3 - Neural Network](#3) \n",
    "  * [3.1 - Data generation](#3.1)\n",
    "  * [3.2 - Architecture of a fully connected neural network with one hidden layer](#3.2)\n",
    "  * [3.3 - Integration](#3.3)\n",
    "  * [3.4 - Play by yourself!](#3.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1- Packages\n",
    "\n",
    "In order to finish a task, we need commands from certain **Python** packages. Again, one of the commonly used package is **numpy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # for plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Logistic regression revisit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "### 2.1 - Generate a training dataset\n",
    "\n",
    "First, we generate a training dataset from a pre-specified logistic regression model. **In order to guarantee that our simulation results are reproducible, we need to control the random seed.** That is, after controlling the seed, others can generate the **SAME** random variables as we did, so our simulation results can be reproduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following logistic regression model \n",
    "$$\n",
    "y^{(i)}\\sim\\mbox{Bernoulli}\\{\\pi(x^{(i)})\\},\\\\\n",
    "\\mathrm{logit}\\{\\pi(x^{(i)})\\} = b_0 +w_{00}x^{(1i)}+w_{01}x^{(2i)},\n",
    "$$\n",
    "where $\\mbox{Bernoulli}(p)$ is a Bernoulli distribution with success probability $p$, $x^{(i)} = (x^{(1i)},x^{(2i)})^T$, $b_0=-0.5$, $w_{00}=0.1$, $w_{01}=-0.1$, $x^{(ki)}\\sim N(2,2^2)$ $k=1,2$.\n",
    "Let us write a function to generate a training dataset of size $n$ with a random number $rn$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "def sigmoid(x):\n",
    "    # x: input\n",
    "    \n",
    "    sig = 1/(1 + np.exp(-x))\n",
    "\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "def train_data_generation(n, rn):\n",
    "    # n: sample size\n",
    "    # rn: random seed\n",
    "    \n",
    "    np.random.seed(rn)\n",
    "    x = np.random.normal(2,2**2, (n,2))\n",
    "    z = -0.5 + 0.1*x[:,0] - 0.1 * x[:,1]\n",
    "    a = sigmoid(z)\n",
    "    y = [np.random.binomial(1,prob,1) for prob in a]\n",
    "    y = np.array(y)    \n",
    "    \n",
    "    return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize your data, you may would like to run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "x, y = train_data_generation(1000, 100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(x[:,0], x[:,1],  c=y[:,0])\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                    loc=\"lower right\", title=\"Classes\")\n",
    "ax.set_title('Simulated training dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 - Parameter estimation\n",
    "\n",
    "Different from what we have done in Homework 2, we implement a computation graph for parameter estimation based on vectorization and a (batch) gradient descent algorithm. Check the slides for Section 1.2 for details. In this part, we separately code up the forward propagation and backpropagation, and we use `dictionary` to return the corresponding values with meaningful keys names. \n",
    "\n",
    "The following code is useful to briefly understand the `dictionary` structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "dic = {'W[0]': 0 , 'W[1]':1} # Construct a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "i=0\n",
    "dic['W['+str(i)+']'] #use variables to extract the values for `W[0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a logistic regression model, we will use a dictionary `par` to store the values for the parameters and use a dictionary `grad` to store those for the gradients. Please notice that the two dictionaries are updated until convergence.\n",
    "\n",
    "First, we need to initilize the two dictionaries. We implement the following strategies for initialization. \n",
    "   * Initialize the weights by a random vector, whose elements are independently generated from a normal distribution with mean zero and standard deviation one. Please use a random seed to keep the code reproducible.\n",
    "   * Initialize the bias by zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE EXISTING CODE\n",
    "def Initialize_pars(d,rn):\n",
    "    # d: the dimension of the feature.\n",
    "    # rn: random seed\n",
    "    \n",
    "    # Step 1. Set random seed\n",
    "    # Step 2. Initialize w with size (d,1)\n",
    "    # Step 3. Initialize b with size (1,1)\n",
    "\n",
    "    \n",
    "    ### YOUR CODE BEGINS HERE (approximately 3 lines)\n",
    "    np.random.seed(rn)\n",
    "    w = np.random.normal(0,1,(d,1))\n",
    "    b = np.zeros((1,1))\n",
    "    ### YOUR CODE ENDS\n",
    "    \n",
    "    par = {\n",
    "        'w': w,\n",
    "        'b': b\n",
    "          }\n",
    "    \n",
    "    return par\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "d = x.shape[1]\n",
    "rn = 1234\n",
    "par = Initialize_pars(d,rn)\n",
    "print(par['w'])\n",
    "print('Your result should be:\\n [[ 0.47143516]\\n  [-1.19097569]] ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already introduced the vectorization for logistic regression models. Specifically, we have (check the second homework for details)\n",
    "$$\n",
    "\\nabla J(\\tilde{w}) =n^{-1}X^T(A-Y), \\quad H(\\tilde{w}) = n^{-1}X^TWX,\n",
    "$$\n",
    "where $Y=(y^{(1)},\\ldots,y^{(n)})^T$, $A=(a^{(1)},\\ldots,a^{(n)})^T$ and $W = \\mbox{diag}((a^{(1)}(1-a^{(1)}),\\ldots, a^{(n)}(1-a^{(n)})))$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A computation graph consists of forward propagation and backpropagation.\n",
    "    * Forward propagation computes the cost function as well as others given the current parameters. \n",
    "    * Backpropagation computes the derivatives based on the values computed from the forward propagation. \n",
    "**Please notice that it is enough to \"cache\" a's.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we consider the forward propagation. Given the current model parameters, we need to calculate \n",
    " * $Z = X w + b$, where  \n",
    " * $A = \\sigma(Z)$\n",
    " * An informal vectorization for the cost $J$  is $ n^{-1}\\{-(Y\\log A + (1-Y) \\log (1-A))\\}^{T}1_{n}$, where $1_{n}=(1,\\ldots,1)^T$ is a vector of 1's with length $n$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE EXISTING CODE\n",
    "def forward(x, y, par):\n",
    "    # x: feature matrix of size nX2\n",
    "    # y: target vector of size nX1\n",
    "    # par: dictionary with currect parameters.\n",
    "\n",
    "    # Step 1. Obtain the sample size n\n",
    "    # Step 2. Obtain Z\n",
    "    # Step 3. Obtain A\n",
    "    # Step 4. Obtain J\n",
    "    # Step 5. Cache J and A. The reason we cache J is that we would like to monitor the value of the cost function \n",
    "    #         as iterations goes by.\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE BEGINS HERE (approximately 4 lines)\n",
    "    n = x.shape[0]\n",
    "    Z = x @ par['w'] + par['b']\n",
    "    A = sigmoid(Z)\n",
    "    J = - (y * np.log(A) + (1-y) * np.log(1-A)).transpose() @ np.ones((n,1))/n    \n",
    "    ### YOUR CODE ENDS\n",
    "    \n",
    "    cache = {\n",
    "        'J': J,\n",
    "        'A': A\n",
    "    }\n",
    "    return cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "cache = forward(x, y, par)\n",
    "print(cache['J'])\n",
    "print('Your result should be:\\n[[1.62409418]]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we consider the backpropagation. Given the values obtained from the forward propagation, we need to obtain the following results. \n",
    " * 'Error term': $err = A-Y$\n",
    " * Gradient for weights: $dw = X^{T}err/n$\n",
    " * Gradient for bias: $db = err^{T}1_n/n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE EXISTING CODE\n",
    "def backprop(x, y, cache):\n",
    "    # x: feature matrix of size nX2\n",
    "    # y: target vector of size nX1\n",
    "    # cache: cached values for A\n",
    "    \n",
    "    # Step 1. Obtain the sample size n\n",
    "    # Step 2. Obtain Z\n",
    "    # Step 3. Obtain A\n",
    "    # Step 4. Obtain J\n",
    "    # Step 5. Cache J and A. The reason we cache J is that we would like to monitor the value of the cost function \n",
    "    #         as iterations goes by.\n",
    "    \n",
    "    ### YOUR CODE BEGINS HERE (approximately 4 lines)\n",
    "    n = x.shape[0]\n",
    "    err = cache['A'] - y\n",
    "    dw = (x.transpose() @ err) / n \n",
    "    db = (err.transpose() @ np.ones((n,1))) / n\n",
    "    ### YOUR CODE ENDS\n",
    "    \n",
    "    grad = {\n",
    "        'dw': dw,\n",
    "        'db': db\n",
    "    }\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "grad = backprop(x, y, cache)\n",
    "print(\"Your dw is:\")\n",
    "print(grad['dw'])\n",
    "print(\"The expected dw is:\\n[[ 0.34420118]\\n [-1.04006133]]\\n\")\n",
    "print(\"Your dw is:\")\n",
    "print(grad['db'])\n",
    "print(\"The expected db is:\\n[[0.02902235]]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we finish the computation graph, we need to update the model parameters using a **learning rate** $\\alpha$ by \n",
    "$$w = w - \\alpha dw,\\quad b = b - \\alpha db$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE EXISTING CODE\n",
    "def update_par(par, grad, alpha):\n",
    "    # par: dictionary with currect parameters.\n",
    "    # grad: dictionary with gradients\n",
    "    # alpha: learning rate\n",
    "    \n",
    "    # Step 1. Update w\n",
    "    # Step 2. Update b\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE BEGINS HERE (approximately 2 lines)\n",
    "    par['w'] -= alpha * grad['dw']\n",
    "    par['b'] -= alpha * grad['db']\n",
    "    ### YOUR CODE ENDS\n",
    "    \n",
    "    return par\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "alpha = 0.01\n",
    "par = update_par(par, grad, alpha)\n",
    "print(\"Your updated w is\")\n",
    "print(par['w'])\n",
    "print(\"The expected w is:\\n[[ 0.46455114]\\n [-1.17017447]]\\n\")\n",
    "print(\"Your updated b is:\")\n",
    "print(par['b'])\n",
    "print(\"The expected b is:\\n[[-0.00087067]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "### 2.3 - Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have finished one iteration for the (batch) gradient descent algorithm. We need to put things together to obtain the estimators for the model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE EXISTING CODE\n",
    "def est_par_logistic(x, y, alpha, M, rn):\n",
    "    # x: feature matrix of size nX2\n",
    "    # y: target vector of size nX1\n",
    "    # alpha: learning rate\n",
    "    # M: maximum number of iterations\n",
    "    # rn: random seed for the initialization\n",
    " \n",
    "    # Step 1. Obtain the dimension of features\n",
    "    # Step 2. Initialize the parameters\n",
    "    # Step 3. Iteration \n",
    "    #        Step 3.1 Forward propagation\n",
    "    #        Step 3.2 Backpropagation\n",
    "    #        Step 3.3 Update parameter\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE BEGINS HERE (approximately 5 lines)\n",
    "    d = x.shape[1]\n",
    "    par = Initialize_pars(d,rn)\n",
    "    \n",
    "    for i in range(M):\n",
    "        cache = forward(x, y, par)\n",
    "        grad = backprop(x, y, cache)\n",
    "        par = update_par(par, grad, alpha)\n",
    "     ### YOUR CODE ENDS\n",
    "    \n",
    "        if i % 500 == 0:\n",
    "            print(\"After %4d iterations, the cost is %10.8f\" % (i, cache['J']))\n",
    "\n",
    "    return par\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "x, y = train_data_generation(5000, 100)\n",
    "par = est_par_logistic(x, y, 0.005, 10000, 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your cost function should decrease. The estimation procedure can be stopped if the cost function remains stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "print(\"Your estimated w is\")\n",
    "print(par['w'])\n",
    "print('The expected value for w is\\n[[ 0.09970773]\\n [-0.09344206]]\\n')\n",
    "print(\"Your estimated b is\")\n",
    "print(par['b'])\n",
    "print('The expected value for w is\\n[[-0.49004407]]\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please notice that those values shoule be very close to the truth. Next, let's visualize the estimation result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1_margin = np.linspace(-15,20,200)\n",
    "x2_margin = np.linspace(-15,20,200)\n",
    "x1_grid, x2_grid = np.meshgrid(x1_margin,x2_margin)\n",
    "y_grid = sigmoid(par['b'] + par['w'][0] * x1_grid + par['w'][1]*x2_grid)\n",
    "y_grid[y_grid>=0.5] = 1\n",
    "y_grid[y_grid<0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(x1_grid, x2_grid, y_grid, cmap=plt.cm.Spectral)\n",
    "scatter = plt.scatter(x[:,0], x[:,1], c = y[:,0], cmap=plt.cm.Spectral,s=1)\n",
    "plt.legend(*scatter.legend_elements()) # add legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Neural Network\n",
    "In the previous section, we have discussed how to train a logistic regression model using computation graph. As we have mentioned in the class, parametric models suffer from model mis-specification. If the logistic regression model is wrongly specified, the inference may be wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "### 3.1 - Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following logistic regression model \n",
    "$$\n",
    "y^{(i)}\\sim\\mbox{Bernoulli}\\{\\pi(x^{(i)})\\},\\\\\n",
    "\\pi(x^{(i)}) = \\lVert x^{(i)}\\rVert/2,$$\n",
    "where $\\mbox{Bernoulli}(p)$ is a Bernoulli distribution with success probability $p$, $x^{(i)} = (x^{(1i)},x^{(2i)})^T$, $x^{(1i)} = r^{(i)}\\cos(\\theta^{(i)})$, $x^{(2i)} = r^{(i)}\\sin(\\theta^{(i)})$, $r^{(i)}\\sim\\mbox{Uniform}(0,2)$, $\\theta^{(i)}\\sim\\mbox{Uniform}(0,2\\pi)$, $\\mbox{Uniform}(a,b)$ is a uniform distribution over the interval $(a,b)$, and $\\lVert x\\rVert = (x_1^2+x_2^2)^{1/2}$ is the Euclidean norm for a vector $x=(x_1,x_2)^T$. Clearly, the training data is not generated from a logistic regression model.\n",
    "\n",
    "Let us write a function to generate a training dataset of size $n$ with a random number $rn$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE EXISTING CODE\n",
    "def train_data_generation_nn(n, rn):\n",
    "    # n: sample size\n",
    "    # rn: random seed\n",
    "    \n",
    "    # Step 1. Set random seed\n",
    "    # Step 2. Generate r\n",
    "    # Step 3. Generate theta\n",
    "    # Step 4. Generate x\n",
    "    # Step 5. Generate y\n",
    "    \n",
    "    ### YOUR CODE BEGINS HERE (approximately 5 lines)\n",
    "    np.random.seed(rn)\n",
    "    r = np.random.uniform(0,2,(n,1))\n",
    "    theta2 = np.random.uniform(0,2*np.pi,(n,1))\n",
    "    x = np.concatenate((r * np.cos(theta2), r * np.sin(theta2)),axis = 1)\n",
    "    y = np.random.binomial(1, r/2, (n,1))\n",
    "    ### YOUR CODE ENDS\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the generated data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "x, y = train_data_generation_nn(1000, 100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(x[:,0], x[:,1],  c=y[:,0])\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                    loc=\"lower right\", title=\"Classes\")\n",
    "ax.set_title('Simulated training dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above figure, we can see that there is no linear boundary such that the two classes can be separated nicely. That is, the logistic regression model is intuitively wrongly specified for this kind of dataset. Now, let's blindly fit a logistic regression model to this dataset and visualize the fitted result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "par = est_par_logistic(x, y, 0.01, 10000, 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 10\\,000 iterations, the cost decreases to 0.69, and the estimated model parameters are very close to zero. The estimated parameter indicates that the features does not contribute to the response too much in fitted logistic regression model. Clearly, this is not the case, but the features contribute in a non-linear manner.\n",
    "\n",
    "Next, let's look at the estimation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "x1_margin = np.linspace(-2,2,200)\n",
    "x2_margin = np.linspace(-2,2,200)\n",
    "x1_grid, x2_grid = np.meshgrid(x1_margin,x2_margin)\n",
    "y_grid = sigmoid(par['b'] + par['w'][0] * x1_grid + par['w'][1]*x2_grid)\n",
    "y_grid[y_grid>=0.5] = 1\n",
    "y_grid[y_grid<0.5] = 0\n",
    "\n",
    "\n",
    "\n",
    "plt.contourf(x1_grid, x2_grid, y_grid, cmap=plt.cm.Spectral)\n",
    "scatter = plt.scatter(x[:,0], x[:,1], c = y[:,0], cmap=plt.cm.Spectral,s=1)\n",
    "plt.legend(*scatter.legend_elements()) # add legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated logistic regression wrongly predict all the points in the rectangle area  $[-2,2]\\times[-2,2]$ to be 0, which does not make any sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "### 3.2 - Architecture of a fully connected neural network with one hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the slides, we would like to try a neural network with one hidden layer, and use computation graph to train the model parameters. Specifically, we need to follow the following steps. \n",
    "\n",
    " * Initialize the model parameters. Random initialization is used for weights, and the bias terms are set to be 0.\n",
    " * Based on the current parameters, we use forward propagation to calculate the cost function as well as other intermediate terms. Please remember, we need to cache the cost function as well as \"A\"'s\n",
    " * Use backpropagation to obtain the derivations with respect to the model parameters. \n",
    " * Update the model parameter by (batch) gradient descent method with a learning rate $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize the model parameters. Please notice that different from the ones in the logistic regression model, we use weight matrices with rows indicating a certain weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE EXISTING CODE\n",
    "def Initialize_pars_nn(d,na,rn):\n",
    "    # d: the dimension of the feature.\n",
    "    # na: the dimension of the hidden layer. That is, the number of neurons.\n",
    "    # rn: random seed\n",
    "    \n",
    "    # Step 1. Set random seed\n",
    "    # Step 2. Initialize W1 (Pay attention to the dimension)\n",
    "    # Step 3. Initialize b1 (Pay attention to the dimension)\n",
    "    # Step 4. Initialize W2 (Pay attention to the dimension)\n",
    "    # Step 5. Initialize b2 (Pay attention to the dimension)\n",
    "    \n",
    "    ### YOUR CODE BEGINS HERE (approximately 5 lines)\n",
    "    np.random.seed(rn)\n",
    "    W1 = np.random.normal(0,1,(na,d))\n",
    "    b1 = np.zeros((na,1))\n",
    "    W2 = np.random.normal(0,1,(1,na))\n",
    "    b2 = np.zeros((1,1))\n",
    "    ### YOUR CODE ENDS\n",
    "    \n",
    "    par = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2,\n",
    "          }\n",
    "    \n",
    "    return par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "d = x.shape[1]\n",
    "rn = 1234\n",
    "par = Initialize_pars_nn(d,4,rn)\n",
    "print(par['W1'][0,:])\n",
    "print('Your result should be:\\n[ 0.47143516 -1.19097569]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the current parameters, we next need a forward propagation to obtain the cost as well as \"A\"s for the backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the current parameters, we next need a forward propagation to obtain the cost as well as \"A\"s for the backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE EXISTING CODE\n",
    "def forward_nn(x, y, par):\n",
    "    # x: feature matrix of size nX2\n",
    "    # y: target vector of size nX1\n",
    "    # par: dictionary with currect parameters.\n",
    "\n",
    "    # Step 1. Obtain W1 from par\n",
    "    # Step 2. Obtain b1 from par\n",
    "    # Step 3. Obtain W2 from par\n",
    "    # Step 4. Obtain W2 from par\n",
    "    # Step 5. Obtain Z1\n",
    "    # Step 6. Obtain A1\n",
    "    # Step 7. Obtain Z2\n",
    "    # Step 8. Obtain A2\n",
    "    # Step 9. Obtain J\n",
    "    # Step 10. Cache J, A1 and A2. The reason we cache J is that we would like to monitor the value of the cost function \n",
    "    #         as iterations goes by.\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE BEGINS HERE (approximately 9 lines)\n",
    "    W1 = par[\"W1\"]\n",
    "    b1 = par[\"b1\"]\n",
    "    W2 = par[\"W2\"]\n",
    "    b2 = par[\"b2\"]\n",
    "    \n",
    "    Z1 = x @ W1.transpose() + b1.transpose() # W1 X + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = A1 @ W2.transpose() + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    J = - np.mean(y * np.log(A2) + (1-y) * np.log(1-A2))\n",
    "    ### YOUR CODE ENDS\n",
    "    \n",
    "    cache = {\n",
    "        'J': J,\n",
    "        'A1': A1,\n",
    "        'A2': A2\n",
    "    }\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "cache = forward_nn(x, y, par)\n",
    "print(cache['J'])\n",
    "print('Your result should be:\\n0.7317784853362318')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we consider the backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE EXISTING CODE\n",
    "def backprop_nn(x, y, par, cache):\n",
    "    # x: feature matrix of size nX2\n",
    "    # y: target vector of size nX1\n",
    "    # par: dictionary containing the current parameters.\n",
    "    # cache: cached values for A\n",
    "    \n",
    "    # Step 1. Obtain the sample size n\n",
    "    # Step 3. Obtain A1 from the cache\n",
    "    # Step 3. Obtain A2 from the cache\n",
    "    # Step 3. Obtain dZ2\n",
    "    # Step 3. Obtain dW2\n",
    "    # Step 3. Obtain db2\n",
    "    # Step 3. Obtain dZ1 using W2 from par\n",
    "    # Step 2. Obtain dW1\n",
    "    # Step 3. Obtain db1\n",
    "    # Step 5. Cache dW1, db1, dW2, db2\n",
    "    \n",
    "    ### YOUR CODE BEGINS HERE (approximately 9 lines)\n",
    "    n = x.shape[0]\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    dZ2 = A2 - y\n",
    "    dW2 = dZ2.transpose() @ A1 / n\n",
    "    db2 = np.mean(dZ2,  keepdims=True)\n",
    "    dZ1 = (dZ2 @ par['W2']) * (A1*(1-A1)) \n",
    "    dW1 = dZ1.transpose() @ x / n\n",
    "    db1 = np.mean(dZ1,axis = 0, keepdims = True).transpose()\n",
    "    ### YOUR CODE ENDS\n",
    "    \n",
    "    grad = {\n",
    "        'dW1': dW1,\n",
    "        'db1': db1,\n",
    "        'dW2': dW2,\n",
    "        'db2': db2,\n",
    "    }\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "grad = backprop_nn(x, y, par, cache)\n",
    "print(\"The first row of your dw1 is:\")\n",
    "print(grad['dW1'][0,:])\n",
    "print(\"The expected value is:\\n[-3.19236049e-04  2.16698484e-05]\\n\")\n",
    "print(\"The inverse of your db1 is:\")\n",
    "print(grad['db1'].transpose())\n",
    "print(\"The expected value is:\\n[[ 5.96842958e-05 -1.42505549e-02  2.40350654e-03  1.45257415e-03]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, update your model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE EXISTING CODE\n",
    "def update_par_nn(par, grad, alpha):\n",
    "    # par: dictionary with currect parameters.\n",
    "    # grad: dictionary with gradients\n",
    "    # alpha: learning rate\n",
    "    \n",
    "    # Step 1. Update W1\n",
    "    # Step 2. Update b1\n",
    "    # Step 3. Update W2\n",
    "    # Step 4. Update b2    \n",
    "    \n",
    "    ### YOUR CODE BEGINS HERE (approximately 4 lines)\n",
    "    par['W1'] -= alpha * grad['dW1']\n",
    "    par['b1'] -= alpha * grad['db1']\n",
    "    par['W2'] -= alpha * grad['dW2']\n",
    "    par['b2'] -= alpha * grad['db2']\n",
    "    ### YOUR CODE ENDS\n",
    "    \n",
    "    return par\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "alpha = 0.01\n",
    "par = update_par_nn(par, grad, alpha)\n",
    "print(\"The first row of your updated w1 is:\")\n",
    "print(par['W1'][0,:])\n",
    "print(\"The expected value is:\\n[ 0.47143836 -1.19097591]\\n\")\n",
    "print(\"The transpose of your updated b1 is:\")\n",
    "print(par['b1'].transpose())\n",
    "print(\"The expected b is:\\n[[-1.19368592e-06  2.85011098e-04 -4.80701309e-05 -2.90514829e-05]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.3'></a>\n",
    "### 3.3 - Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE EXISTING CODE\n",
    "def est_par_nn(x, y, na, alpha, M, rn):\n",
    "    # x: feature matrix of size nX2\n",
    "    # y: target vector of size nX1\n",
    "    # na: the dimension of the hidden layer. That is, the number of neurons.\n",
    "    # alpha: learning rate\n",
    "    # M: maximum number of iterations\n",
    "    # rn: random seed for the initialization\n",
    " \n",
    "    # Step 1. Obtain the dimension of features\n",
    "    # Step 2. Initialize the parameters\n",
    "    # Step 3. Iteration \n",
    "    #        Step 3.1 Forward propagation\n",
    "    #        Step 3.2 Backpropagation\n",
    "    #        Step 3.3 Update parameter\n",
    "     \n",
    "    \n",
    "    ### YOUR CODE BEGINS HERE (approximately 5 lines)\n",
    "    d = x.shape[1]\n",
    "    par = Initialize_pars_nn(d,na, rn)\n",
    "    \n",
    "    for i in range(M):\n",
    "        cache = forward_nn(x, y, par)\n",
    "        grad = backprop_nn(x, y, par, cache)\n",
    "        par = update_par_nn(par, grad, alpha)\n",
    "    ### YOUR CODE ENDS        \n",
    "        if i % 500 == 0:\n",
    "            print(\"After %4d iterations, the cost is %10.8f\" % (i, cache['J']))\n",
    "    \n",
    "    return par\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "x, y = train_data_generation_nn(1000, 100)\n",
    "par = est_par_nn(x, y, 4, 0.01, 10000, 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 10\\,000 iterations, the cost decreases to 0.63, which is much better than the 0.69 for the wrongly specified logistic regression model.\n",
    "\n",
    "We have obtained the trained neural network, we need another function for prediction. Please notice that \"A2\" in the cache is the prediction based on the currect parameter. Thus, you only need to copy your code for \"forward_nn\" except the last line for the cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE EXISTING CODE\n",
    "def prediction_nn(x_test, par):\n",
    "    # x_new: a test feature matrix of size n_testX2\n",
    "    # par: the trained parameter dictionary\n",
    " \n",
    "    # Step 1. Obtain W1 from par\n",
    "    # Step 2. Obtain b1 from par\n",
    "    # Step 3. Obtain W2 from par\n",
    "    # Step 4. Obtain W2 from par\n",
    "    # Step 5. Obtain Z1\n",
    "    # Step 6. Obtain A1\n",
    "    # Step 7. Obtain Z2\n",
    "    # Step 8. Obtain A2\n",
    "     \n",
    "    \n",
    "    ### YOUR CODE BEGINS HERE (approximately 8 lines)\n",
    "    W1 = par[\"W1\"]\n",
    "    b1 = par[\"b1\"]\n",
    "    W2 = par[\"W2\"]\n",
    "    b2 = par[\"b2\"]\n",
    "    \n",
    "    Z1 = x_test @ W1.transpose() + b1.transpose() # W1 X + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = A1 @ W2.transpose() + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    ### YOUR CODE ENDS        \n",
    "\n",
    "    return A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "x1_margin = np.linspace(-2,2,200)\n",
    "x2_margin = np.linspace(-2,2,200)\n",
    "x1_grid, x2_grid = np.meshgrid(x1_margin,x2_margin)\n",
    "x_test = np.c_[x1_grid.ravel(), x2_grid.ravel()]\n",
    "y_pred = prediction_nn(x_test, par)\n",
    "y_pred[y_pred>=0.5]=1\n",
    "y_pred[y_pred<0.5]=0\n",
    "\n",
    "y_cont = y_pred.reshape(x1_grid.shape)\n",
    "\n",
    "\n",
    "plt.contourf(x1_grid, x2_grid, y_cont, cmap=plt.cm.Spectral)\n",
    "scatter = plt.scatter(x[:,0], x[:,1], c = y[:,0], cmap=plt.cm.Spectral,s=0.5)\n",
    "plt.legend(*scatter.legend_elements()) # add legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.4'></a>\n",
    "### 3.4 - Play by yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 4 hidden nodes, we cannot get a good boundary. How about we try a neural network with 10 hidden nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "x, y = train_data_generation_nn(1000, 100)\n",
    "par = est_par_nn(x, y, 10, 0.01, 10000, 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT CHANGE THE FOLLOWING CODE\n",
    "x1_margin = np.linspace(-2,2,200)\n",
    "x2_margin = np.linspace(-2,2,200)\n",
    "x1_grid, x2_grid = np.meshgrid(x1_margin,x2_margin)\n",
    "x_test = np.c_[x1_grid.ravel(), x2_grid.ravel()]\n",
    "y_pred = prediction_nn(x_test, par)\n",
    "y_pred[y_pred>=0.5]=1\n",
    "y_pred[y_pred<0.5]=0\n",
    "\n",
    "y_cont = y_pred.reshape(x1_grid.shape)\n",
    "\n",
    "\n",
    "plt.contourf(x1_grid, x2_grid, y_cont, cmap=plt.cm.Spectral)\n",
    "scatter = plt.scatter(x[:,0], x[:,1], c = y[:,0], cmap=plt.cm.Spectral,s=0.5)\n",
    "plt.legend(*scatter.legend_elements()) # add legend\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
