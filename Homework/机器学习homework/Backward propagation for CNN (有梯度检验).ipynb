{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e3db164-81b3-4aef-81a0-c9641d762aa0",
   "metadata": {},
   "source": [
    "# Homework 6  Backward propagation for CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce210bcb-980c-4afd-81b6-499fd01c5a64",
   "metadata": {},
   "source": [
    "#### Welcome to the course AI and Deep Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ffa95-4a13-4db6-b226-69a5e82d1196",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNNs) are a class of deep learning models mainly used for image and spatial data processing. They automatically learn to detect important features such as edges, textures, and shapes by applying convolutional filters over input data.Backpropagation is the training algorithm used to adjust weights in a CNN.It works by:\n",
    "\n",
    "1.Forward pass – computes the output and loss based on current weights.\n",
    "\n",
    "2.Backward pass – calculates gradients of the loss with respect to each parameter using the chain rule.(Here you should pay attention to the difference between parameter that needs to update and hyperparameter that needs to select)\n",
    "\n",
    "3.Parameter update – applies gradient descent (or variants) to minimize the loss.\n",
    "\n",
    "This process is repeated over many iterations to train the CNN to make accurate predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cc40ff-243a-446f-a805-9d1bad2032c2",
   "metadata": {},
   "source": [
    "#### Learning Goal:  In this homework, we aim to understand and implement the complete structure of Convolutional Neural Networks (CNNs),which includes convolution layers,pooling layer.The backward propagation is also clearly illustrated and you can write your code to check how gradients are computed through each layer.You are expected to clearly illustrate the mathematical steps of backpropagation and write corresponding Python code (from scratch or using NumPy) to solidify your understanding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9db71-d11b-4f54-8807-db602db7f92d",
   "metadata": {},
   "source": [
    "# 1- Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e87d6-7ec0-48e9-9698-6c17317d4a78",
   "metadata": {},
   "source": [
    "$[Input Image] → [Conv2D] → [Activation] → [Pooling]→[Flatten]→[Fully - Connected - layer]→Output$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc7e3c9-5d1d-4ef5-92b1-4ac6659d02ea",
   "metadata": {},
   "source": [
    "# 2- Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e937e1-1943-45c9-a672-44fb78fabf12",
   "metadata": {},
   "source": [
    "### 2.1CNN Forward Propagation Process(multiple samples)\n",
    "Below is a general example for multiple samples, you can also define N=1 to review the one-sample senario for simplicity.\n",
    "\n",
    "#### A. Input Layer\n",
    "- **Input Data**: Assume the input is $Z^{l-1}$ with dimensions $(N, C^{l-1}, H^{l-1}, W^{l-1})$, where:\n",
    "  - $N$ is the batch size\n",
    "  - $C^{l-1}$ is the number of channels (3 for RGB)\n",
    "  - $H^{l-1}$ is the height\n",
    "  - $W^{l-1}$ is the width\n",
    "  - e.g.（MNIST）：\\( (1, 1, 28, 28) \\)\n",
    "\n",
    "#### B. Convolutional Layer\n",
    "\n",
    "| parameters       | dimension                | details                  |\n",
    "|----------------|--------------------------|--------------------------|\n",
    "| **weights**       | $(C^{l}, C^{l-1}, K_h, K_w)$ | $K_h \\times K_w$为kernel size |\n",
    "| **bias**       | $(C^{l})$              | one bias each output channel       |\n",
    "| **stride**       | $(S_h, S_w)$             | default(1,1)               |\n",
    "| **padding**       | $(P_h, P_w)$             | default(0,0)               |\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H^{l} &= \\left\\lfloor \\frac{H^{l-1} + 2P_h - K_h}{S_h} + 1 \\right\\rfloor \\\\\n",
    "W^{l} &= \\left\\lfloor \\frac{W^{l-1} + 2P_w - K_w}{S_w} + 1 \\right\\rfloor\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**output dimension**：$(N, C^{l}, H^{l}, W^{l})$\n",
    "\n",
    "\n",
    "**example**：\n",
    "- input：\\( (1, 1, 28, 28) \\)\n",
    "- parameters：\\$( C^{(l)}=32 \\), \\( K_h=K_w=3 \\), \\( S_h=S_w=1 \\), \\( P_h=P_w=1 \\)$\n",
    "  kernel $W_{l}:(32,1,3,3)$\n",
    "  bias $b_{l}；（32）$\n",
    "- output：\\( (1, 32, 28, 28) \\)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### C. Activation Layer (ReLU)\n",
    "- **Operation**:\n",
    "  $$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "- **Output**:$A^{l}$ Same as input dimensions $(N, C_{out}, H_{\\text{out}}, W_{\\text{out}})$\n",
    "\n",
    "#### D. Pooling Layer (MaxPool)\n",
    "| parameters       | dimension         | details|\n",
    "|----------------|------------------|-------------|\n",
    "| **pooling**   | $(P_h, P_w)$     | default(2,2)   |\n",
    "| **stride**       | $(S_h, S_w)$     | 通常等于窗口大小 |\n",
    "| **padding**       | $(P_{h}, P_{w})$ | default(0,0) |\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H_{out} &= \\left\\lfloor \\frac{H^{l} - P_h}{S_h} + 1 \\right\\rfloor \\\\\n",
    "W_{out} &= \\left\\lfloor \\frac{W^{l} - P_w}{S_w} + 1 \\right\\rfloor\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**output dimension**：$Z^{l},dimension(N, C^{l}, H^{out}, W^{out})$\n",
    "**example**：\n",
    "- input：\\( (1, 32, 28, 28) \\)\n",
    "- output：\\( (1, 32, 14, 14) \\)\n",
    "\n",
    "#### E. Flatten\n",
    "- **NO Parameters**\n",
    "- **output dimension**：$(N, D)$  \n",
    "   $D = C^{l} \\times H^{out} \\times W^{out}$\n",
    "\n",
    "**example**：\n",
    "- input：\\( (1, 32, 14, 14) \\)\n",
    "- output：\\( (1, 6272) \\)\n",
    "\n",
    "\n",
    "#### F. Fully-connected layer\n",
    "| parameters   | dimension           | details                     |\n",
    "|------------|--------------------|--------------------------|\n",
    "| **weights**   | $(D, D_{out})$ | $D$为输入特征维度    |\n",
    "| **bias**   | $(D_{out})$         | 每个输出神经元一个偏置    |\n",
    "\n",
    "**output dimension**：$(N, D_{out})$\n",
    "\n",
    "**example**：\n",
    "- input：\\( (1, 6272) \\)\n",
    "- output：\\( (1, 512) \\)\n",
    "\n",
    "\n",
    "#### G. Output Layer\n",
    "| parameters   | dimension           | details                     |\n",
    "|------------|--------------------|--------------------------|\n",
    "| **weight**   | $(D_{out}, M)$      | $M$为分类类别数           |\n",
    "| **bias**   | $(M)$              | 每个类别一个偏置          |\n",
    "\n",
    "**output dimension**：$(N, M)$\n",
    "\n",
    "\n",
    "**example**（MNIST分类）：\n",
    "- input：\\( (1, 512) \\)\n",
    "- output：\\( (1, 10) \\)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ecd7c2-8d46-47d9-a087-2bcfeac7acce",
   "metadata": {},
   "source": [
    "输入 → Conv1 → ReLU → Pool1 → Flatten → Dense1 → Output\n",
    "\n",
    "\n",
    "(1,1,28,28) → (1,32,28,28) → (1,32,14,14) → (1,6272) → (1,512) → (1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f76c69-f2fb-4a0c-8965-e0011a2980f1",
   "metadata": {},
   "source": [
    "### 2.2 concise implementation of forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0415a1-5195-40b4-9a54-b22671cb6214",
   "metadata": {},
   "source": [
    "with the above MNIST example , you can check it using the below code which you are familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c3b135-bbf2-437e-8850-df7edc027263",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, 3, padding=1),  # 保持 (28,28)\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),                  # 输出 (14,14)\n",
    "    nn.Flatten(),                     # 32×14×14=6272\n",
    "    nn.Linear(6272, 512),\n",
    "    nn.Linear(512, 10)\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 1, 28, 28)\n",
    "print(model(x).shape)  # 输出 torch.Size([1, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13d859-0deb-4634-94ba-3890f97b88e8",
   "metadata": {},
   "source": [
    "here I want to highlight the parameters in Conv2d layer, for more details,You can refer to the parameter definitions on the PyTorch official website, or review Professor Wang's lecture slides. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d165b-7171-4cf3-b2a8-b4169060d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "torch.nn.Conv2d(\n",
    "    in_channels,       # 输入通道数（int）\n",
    "    out_channels,      # 输出通道数（int）\n",
    "    kernel_size,       # 卷积核大小（int或tuple）\n",
    "    stride=1,          # 步长（默认1）\n",
    "    padding=0,         # 零填充（默认0）\n",
    "    dilation=1,        # 空洞卷积（默认1）\n",
    "    groups=1,          # 分组卷积（默认1）\n",
    "    bias=True,         # 是否使用偏置（默认True）\n",
    "    padding_mode='zeros', # 填充模式（默认'zeros'）\n",
    "    device=None,       # 设备（如'cuda'）\n",
    "    dtype=None         # 数据类型（如torch.float32）\n",
    ")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9aa4aa-5261-4d44-8505-049fc21850cc",
   "metadata": {},
   "source": [
    "# 3-Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f2d44-b546-4aa2-aa6b-54f86a1db1aa",
   "metadata": {},
   "source": [
    "We have learnt backpropagation for fully-connected networks,thus, it remains to show the backward propagation for\n",
    "1) the pooling layer\n",
    "2) the convolution layer\n",
    "\n",
    "In the following , we assume a POOL is conducted right after a CONV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4a47e2-3e0f-4f35-8b7d-f3afa95aa31a",
   "metadata": {},
   "source": [
    "### 3.1 Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd6d11-98fe-4c32-9477-fae587886d41",
   "metadata": {},
   "source": [
    "#### 3.1.1  max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dfcb6e-d573-4af0-879b-4952e7b1299f",
   "metadata": {},
   "source": [
    "\n",
    "1. Basic Concept\n",
    "\n",
    "    \n",
    "For a 4×4 matrix with 2×2 pooling window (stride=2):\n",
    "\n",
    "$$\n",
    "A^{l} = \\begin{bmatrix} \n",
    "1 & 3 & 2 & 1 \\\\ \n",
    "4 & 2 & 0 & 5 \\\\ \n",
    "3 & 1 & 4 & 2 \\\\ \n",
    "2 & 0 & 3 & 6 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "2. Forward Pass\n",
    "Divide input into non-overlapping 2×2 regions and take maximum values:\n",
    "\n",
    "| Position       | Matrix                     | Max Value |\n",
    "|----------------|----------------------------|-----------|\n",
    "| Top-left       | \\begin{pmatrix} 1 & 3 \\\\ 4 & 2 \\end{pmatrix} | **4**     |\n",
    "| Top-right      | \\begin{pmatrix} 2 & 1 \\\\ 0 & 5 \\end{pmatrix}  | **5**     |\n",
    "| Bottom-left    | \\begin{pmatrix} 3 & 1 \\\\ 2 & 0 \\end{pmatrix}  | **3**     |\n",
    "| Bottom-right   | \\begin{pmatrix} 4 & 2 \\\\ 3 & 6 \\end{pmatrix}  | **6**     |\n",
    "\n",
    "\n",
    "$$\n",
    "Z^{l} = \\begin{bmatrix} \n",
    "4 & 5  \\\\ \n",
    "3 & 6  \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "3. Backpropagation Principle\n",
    "Gradients only flow back to positions that were selected during forward pass:\n",
    "\n",
    "we assume up-streaming gradient to $Z^{l}$ is\n",
    "$$\n",
    "∂L/∂Z^{l} = \\begin{bmatrix}\n",
    "0.5 & 1.2 \\\\\n",
    "0.8 & -0.3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "then we may derive the gradient to $A^{l}$ is\n",
    "\n",
    "$$\n",
    "∂L/∂A^{l} = \\begin{bmatrix} \n",
    "0.0 & 0.0 & 0.0 & 1.2 \\\\ \n",
    "0.5 & 0.0 & 0.0 & 0.0 \\\\ \n",
    "0.8 & 0.0 & 0.0 & 0.0 \\\\ \n",
    "0.0 & 0.0 & 0.0 & -0.3 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "对于每个输出元素 $Z^{l}(m,n)$：\n",
    "\n",
    "1)找到对应输入区域中最大值的位置 (i,j)\n",
    "\n",
    "2)将 $∂L/∂Z^{l}(m,n)$ 的值赋给 $∂L/∂A^{l}(i,j)$\n",
    "\n",
    "3)其他所有位置的梯度保持为0\n",
    "\n",
    "In general,\n",
    "\n",
    "$$\n",
    "∂L/∂A^{l} = \\begin{bmatrix} \n",
    "0 & 0 & 0 & ∂L/∂Z^{l}_{1,2} \\\\ \n",
    "∂L/∂Z^{l}_{1,1} & 0 & 0 & 0 \\\\ \n",
    "∂L/∂Z^{l}_{2,1} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & ∂L/∂Z^{l}_{2,2} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "here we also introduce mask matrix,a mask matrix is a crucial component used to selectively highlight or filter specific elements within a data structure. This concept is particularly useful in operations where certain elements need to be ignored or emphasized based on specific criteria.\n",
    "\n",
    "in this example,\n",
    "\n",
    "$$\n",
    "M_{1,1} = \\begin{bmatrix} \n",
    "0 & 0  \\\\ \n",
    "1 & 0   \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "M_{1,2} = \\begin{bmatrix} \n",
    "0 & 1  \\\\ \n",
    "0 & 0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "M_{2,1} = \\begin{bmatrix} \n",
    "1 & 0  \\\\ \n",
    "0 & 0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "M_{2,2} = \\begin{bmatrix} \n",
    "0 & 0 \\\\ \n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "∂L/∂A^{l} = \\begin{bmatrix} \n",
    "M_{1,1}*∂L/∂Z^{l}_{1,1} & M_{1,2}*∂L/∂Z^{l}_{1,2}  \\\\ \n",
    "M_{2,1}*∂L/∂Z^{l}_{2,1} &  M_{2,2}*∂L/∂Z^{l}_{2,2}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddbbf9d-939c-4b45-a51f-d6a87953791e",
   "metadata": {},
   "source": [
    "4. code for forward,backward propagation (max_pooling senario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55e53f-d9db-4da6-b223-a4987e736149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MaxPool2D:\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "        self.pool_size = pool_size if isinstance(pool_size, tuple) else (pool_size, pool_size)\n",
    "        self.stride = stride\n",
    "        self.input = None\n",
    "        self.max_indices = None  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"forward propagation\"\"\"\n",
    "        self.input = x\n",
    "        batch_size, channels, in_h, in_w = x.shape\n",
    "        out_h = (in_h - self.pool_size[0]) // self.stride + 1\n",
    "        out_w = (in_w - self.pool_size[1]) // self.stride + 1\n",
    "\n",
    "        \n",
    "        output = np.zeros((batch_size, channels, out_h, out_w))\n",
    "        \n",
    "        \n",
    "        self.max_indices = np.zeros((batch_size, channels, out_h, out_w, 2), dtype=int)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for h in range(out_h):\n",
    "                    for w in range(out_w):\n",
    "                        h_start = h * self.stride\n",
    "                        h_end = h_start + self.pool_size[0]\n",
    "                        w_start = w * self.stride\n",
    "                        w_end = w_start + self.pool_size[1]\n",
    "                        \n",
    "                        region = x[b, c, h_start:h_end, w_start:w_end]\n",
    "                        output[b, c, h, w] = np.max(region)\n",
    "                        \n",
    "                        \n",
    "                        max_pos = np.unravel_index(np.argmax(region), region.shape)\n",
    "                        self.max_indices[b, c, h, w] = [\n",
    "                            h_start + max_pos[0],  \n",
    "                            w_start + max_pos[1]   \n",
    "                        ]\n",
    "        return output\n",
    "\n",
    "### YOUR CODE BEGINS HERE ###\n",
    "    def backward(self, dout):\n",
    "        \"\"\"backward propagation\"\"\"\n",
    "        dx = np.zeros_like(self.input)\n",
    "        batch_size, channels, out_h, out_w = dout.shape\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for h in range(out_h):\n",
    "                    for w in range(out_w):\n",
    "                        max_h, max_w = self.max_indices[b, c, h, w]\n",
    "                        dx[b, c, max_h, max_w] = dout[b, c, h, w]\n",
    "        return dx\n",
    "\n",
    "### YOUR CODE ENDS HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c2470-889d-4bdd-8260-2947f9d3aca8",
   "metadata": {},
   "source": [
    "Below is a test,using which you can verify whether your backward propagation code is right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe7eb0-bc1c-4fd5-b9a8-3356673283dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Please do not change this code###\n",
    "def numerical_gradient_check():\n",
    "    \"\"\"数值梯度检查\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "   \n",
    "    x = np.random.randn(1, 1, 4, 4)\n",
    "    pool = MaxPool2D(pool_size=2, stride=2)\n",
    "    \n",
    "   \n",
    "    out = pool.forward(x)\n",
    "    \n",
    "   \n",
    "    def loss_func(x):\n",
    "        return np.sum(pool.forward(x))\n",
    "    \n",
    "    \n",
    "    dout = np.ones_like(out)  \n",
    "    dx_analytic = pool.backward(dout)\n",
    "    \n",
    "    \n",
    "    dx_numeric = np.zeros_like(x)\n",
    "    h = 1e-5  # 微小扰动\n",
    "    \n",
    "    for b in range(x.shape[0]):\n",
    "        for c in range(x.shape[1]):\n",
    "            for i in range(x.shape[2]):\n",
    "                for j in range(x.shape[3]):\n",
    "                    \n",
    "                    orig_val = x[b, c, i, j].copy()\n",
    "                    \n",
    "                    \n",
    "                    x[b, c, i, j] = orig_val + h\n",
    "                    loss_plus = loss_func(x)\n",
    "                    \n",
    "                    \n",
    "                    x[b, c, i, j] = orig_val - h\n",
    "                    loss_minus = loss_func(x)\n",
    "                    \n",
    "                    \n",
    "                    dx_numeric[b, c, i, j] = (loss_plus - loss_minus) / (2 * h)\n",
    "                    \n",
    "                    \n",
    "                    x[b, c, i, j] = orig_val\n",
    "    \n",
    "    # 比较梯度的差异\n",
    "    diff = np.abs(dx_analytic - dx_numeric)\n",
    "    print(\"数值梯度检查结果:\")\n",
    "    print(f\"最大差异: {np.max(diff):.2e}\")\n",
    "    print(f\"平均差异: {np.mean(diff):.2e}\")\n",
    "    \n",
    "    # 可视化比较\n",
    "    print(\"\\n解析梯度 (dx_analytic):\")\n",
    "    print(dx_analytic[0,0])\n",
    "    print(\"\\n数值梯度 (dx_numeric):\")\n",
    "    print(dx_numeric[0,0])\n",
    "    \n",
    "    # 检查是否通过测试\n",
    "    assert np.allclose(dx_analytic, dx_numeric, rtol=1e-5, atol=1e-5), \"梯度检查未通过!\"\n",
    "    print(\"\\n梯度检查通过!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc65c5b9-cd29-4a76-8ad2-739327206f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also provide your own example,give it a try!\n",
    "###your input\n",
    "###forward propagation \n",
    "###backward propagation \n",
    "###gradient check\n",
    "\n",
    "###your code begins here###\n",
    "\n",
    "    x = np.array([[\n",
    "        [[1, 3, 2, 1],\n",
    "         [4, 2, 0, 5],\n",
    "         [3, 1, 4, 2],\n",
    "         [2, 0, 3, 6]]\n",
    "    ]], dtype=np.float32)\n",
    "    \n",
    "    pool = MaxPool2D(pool_size=2, stride=2)\n",
    "    \n",
    "    print(\"输入矩阵:\")\n",
    "    print(x[0,0])\n",
    "    \n",
    "    # 前向传播\n",
    "    out = pool.forward(x)\n",
    "    print(\"\\n前向传播输出:\")\n",
    "    print(out[0,0])\n",
    "    \n",
    "    # 反向传播\n",
    "    dout = np.ones_like(out)  # 假设上游梯度全为1\n",
    "    dx = pool.backward(dout)\n",
    "    print(\"\\n反向传播梯度:\")\n",
    "    print(dx[0,0])\n",
    "    \n",
    "    # 数值梯度检查\n",
    "    print(\"\\n正在进行数值梯度检查...\")\n",
    "    numerical_gradient_check()\n",
    "###YOUR CODE ENDS HERE###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7465fd6d-5cd8-49d2-9d22-f165cc32e01a",
   "metadata": {},
   "source": [
    "#### 3.1.2 average pooling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13099841-0f5b-4924-8d5b-3f5dd241809d",
   "metadata": {},
   "source": [
    "1.with the same $A^{l}$ as mentioned,here we can derive the $Z^{l}$ \n",
    "\n",
    "\n",
    "| Region          | Elements Included | Calculation          | Output Value |\n",
    "|-----------------|-------------------|----------------------|--------------|\n",
    "| Top-left (0-1,0-1) | 1, 3, 4, 2        | (1+3+4+2)/4 = 10/4  | 2.5          |\n",
    "| Top-right (0-1,2-3) | 2, 1, 0, 5       | (2+1+0+5)/4 = 8/4   | 2.0          |\n",
    "| Bottom-left (2-3,0-1) | 3, 1, 2, 0      | (3+1+2+0)/4 = 6/4   | 1.5          |\n",
    "| Bottom-right (2-3,2-3) | 4, 2, 3, 6     | (4+2+3+6)/4 = 15/4  | 3.75         |\n",
    "\n",
    "\n",
    "$$\n",
    "Z^{l} = \\begin{bmatrix} \n",
    "2.5 & 2.0  \\\\ \n",
    "1.5 & 3.75  \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a1f54-4fc7-4af3-81ad-e9f47c1c4dbb",
   "metadata": {},
   "source": [
    "2.For Average Pooling, the upstream gradients are evenly distributed to all positions in the corresponding region during the forward pass.\n",
    "\n",
    "we also assume up-streaming gradient to $Z^{l}$ is\n",
    "$$\n",
    "∂L/∂Z^{l} = \\begin{bmatrix}\n",
    "0.5 & 1.2 \\\\\n",
    "0.8 & -0.3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "then we may derive the gradient to $A^{l}$ is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e247056-9f35-4c72-97c7-c18fb1b27f6f",
   "metadata": {},
   "source": [
    "$$\n",
    "∂L/∂A^{l} = \\begin{bmatrix} \n",
    "0.125 & 0.125 & 0.300 & 0.300 \\\\ \n",
    "0.125 & 0.125 & 0.300 & 0.300 \\\\ \n",
    "0.200 & 0.200 & -0.075 & -0.075 \\\\ \n",
    "0.200 & 0.200 & -0.075 & -0.075 \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9f7d8c-da2b-4369-979c-96c8bd86ede3",
   "metadata": {},
   "source": [
    "$∂L/∂A^{l}_{i,j} = ∂L/∂Z^{l}_{m,n}$ × (1 / pool_area)\n",
    "\n",
    "here pool_area = 2×2 = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6984a27b-6955-4df6-86d8-30ebb9034a16",
   "metadata": {},
   "source": [
    "3.We can also use mask matrix to simplify the representation\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix} \n",
    "1/4 & 1/4  \\\\ \n",
    "1/4 & 1/4  \n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce277105-890c-479c-9086-602edbc49629",
   "metadata": {},
   "source": [
    "$$\n",
    "∂L/∂A^{l} = \\begin{bmatrix} \n",
    "M*∂L/∂Z^{l}_{1,1} & M*∂L/∂Z^{l}_{1,2}  \\\\ \n",
    "M*∂L/∂Z^{l}_{2,1} &  M*∂L/∂Z^{l}_{2,2}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf23607-3a57-4bb5-9739-19ffeca42dd1",
   "metadata": {},
   "source": [
    "4. code for backward propagation(avg_pooling senario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb6f564-6ea2-433d-b2b4-6565ac44de44",
   "metadata": {},
   "source": [
    "Here we can code the average-pooling backward process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4021983-3679-41ff-800f-e092c50e81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AvgPool2D:\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "        self.pool_size = pool_size if isinstance(pool_size, tuple) else (pool_size, pool_size)\n",
    "        self.stride = stride\n",
    "        self.input_shape = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"forward propagation\"\"\"\n",
    "        self.input_shape = x.shape\n",
    "        batch_size, channels, in_h, in_w = x.shape\n",
    "        out_h = (in_h - self.pool_size[0]) // self.stride + 1\n",
    "        out_w = (in_w - self.pool_size[1]) // self.stride + 1\n",
    "        \n",
    "        output = np.zeros((batch_size, channels, out_h, out_w))\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for h in range(out_h):\n",
    "                    for w in range(out_w):\n",
    "                        h_start = h * self.stride\n",
    "                        h_end = h_start + self.pool_size[0]\n",
    "                        w_start = w * self.stride\n",
    "                        w_end = w_start + self.pool_size[1]\n",
    "                        \n",
    "                        region = x[b, c, h_start:h_end, w_start:w_end]\n",
    "                        output[b, c, h, w] = np.mean(region)\n",
    "        return output\n",
    "\n",
    "    \n",
    "   \n",
    "    \n",
    "    ### YOUR CODE BEGINS HERE ###\n",
    "    def backward(self, dout):\n",
    "        \"\"\"backward propagation\"\"\"\n",
    "        dx = np.zeros(self.input_shape)\n",
    "        batch_size, channels, out_h, out_w = dout.shape\n",
    "        pool_area = self.pool_size[0] * self.pool_size[1]\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for h in range(out_h):\n",
    "                    for w in range(out_w):\n",
    "                        h_start = h * self.stride\n",
    "                        h_end = h_start + self.pool_size[0]\n",
    "                        w_start = w * self.stride\n",
    "                        w_end = w_start + self.pool_size[1]\n",
    "                        \n",
    "                        # 将梯度均匀分配到对应区域\n",
    "                        dx[b, c, h_start:h_end, w_start:w_end] += dout[b, c, h, w] / pool_area\n",
    "        return dx\n",
    "    ###YOUR CODE ENDS HERE###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6f2e2a-fc1e-45e3-96ec-c5c451bc3024",
   "metadata": {},
   "source": [
    "Below is a test,using which you can verify whether your backward propagation code is right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c1571-c96d-4b26-b8b3-f6ec457d4933",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Please do not change this code ###\n",
    "def numerical_gradient_check():\n",
    "    \"\"\"数值梯度检查\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    \n",
    "    x = np.random.randn(1, 1, 4, 4)\n",
    "    pool = AvgPool2D(pool_size=2, stride=2)\n",
    "    \n",
    "    \n",
    "    out = pool.forward(x)\n",
    "    \n",
    "    def loss_func(x):\n",
    "        return np.sum(pool.forward(x))\n",
    "    \n",
    "    \n",
    "    dout = np.ones_like(out)  \n",
    "    dx_analytic = pool.backward(dout)\n",
    "    \n",
    "    \n",
    "    dx_numeric = np.zeros_like(x)\n",
    "    h = 1e-5  # 微小扰动\n",
    "    \n",
    "    for b in range(x.shape[0]):\n",
    "        for c in range(x.shape[1]):\n",
    "            for i in range(x.shape[2]):\n",
    "                for j in range(x.shape[3]):\n",
    "                    \n",
    "                    orig_val = x[b, c, i, j].copy()\n",
    "                    \n",
    "                    \n",
    "                    x[b, c, i, j] = orig_val + h\n",
    "                    loss_plus = loss_func(x)\n",
    "                    \n",
    "                    \n",
    "                    x[b, c, i, j] = orig_val - h\n",
    "                    loss_minus = loss_func(x)\n",
    "                    \n",
    "                    \n",
    "                    dx_numeric[b, c, i, j] = (loss_plus - loss_minus) / (2 * h)\n",
    "                    \n",
    "                    \n",
    "                    x[b, c, i, j] = orig_val\n",
    "    \n",
    "    # 比较梯度的差异\n",
    "    diff = np.abs(dx_analytic - dx_numeric)\n",
    "    print(\"数值梯度检查结果:\")\n",
    "    print(f\"最大差异: {np.max(diff):.2e}\")\n",
    "    print(f\"平均差异: {np.mean(diff):.2e}\")\n",
    "    \n",
    "    # 可视化比较\n",
    "    print(\"\\n解析梯度 (dx_analytic):\")\n",
    "    print(dx_analytic[0,0])\n",
    "    print(\"\\n数值梯度 (dx_numeric):\")\n",
    "    print(dx_numeric[0,0])\n",
    "    \n",
    "    # 检查是否通过测试\n",
    "    assert np.allclose(dx_analytic, dx_numeric, rtol=1e-5, atol=1e-5), \"梯度检查未通过!\"\n",
    "    print(\"\\n梯度检查通过!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bbbc47-47e2-4ff1-8c60-c5fccc4cea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also provide your own example,give it a try!\n",
    "\n",
    "###your input\n",
    "###forward propagation \n",
    "###backward propagation \n",
    "###gradient check\n",
    "\n",
    "###your code begins here###\n",
    "\n",
    "    x = np.array([[\n",
    "        [[1, 3, 2, 1],\n",
    "         [4, 2, 0, 5],\n",
    "         [3, 1, 4, 2],\n",
    "         [2, 0, 3, 6]]\n",
    "    ]], dtype=np.float32)\n",
    "    \n",
    "    pool = AvgPool2D(pool_size=2, stride=2)\n",
    "    \n",
    "    print(\"输入矩阵:\")\n",
    "    print(x[0,0])\n",
    "    \n",
    "    # 前向传播\n",
    "    out = pool.forward(x)\n",
    "    print(\"\\n前向传播输出:\")\n",
    "    print(out[0,0])\n",
    "    \n",
    "    # 反向传播\n",
    "    dout = np.ones_like(out)  # 假设上游梯度全为1\n",
    "    dx = pool.backward(dout)\n",
    "    print(\"\\n反向传播梯度:\")\n",
    "    print(dx[0,0])\n",
    "    \n",
    "    # 数值梯度检查\n",
    "    print(\"\\n正在进行数值梯度检查...\")\n",
    "    numerical_gradient_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9930c138-d037-40fa-89b1-d48b07125037",
   "metadata": {},
   "source": [
    "### 3.2 Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfae5ba-2f77-4522-93c0-c158816aa1be",
   "metadata": {},
   "source": [
    "The convolutional layer consists of convolutional kernels and activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71746b9-c90c-448e-9956-373ee223e5cd",
   "metadata": {},
   "source": [
    "#### Forward Propagation Order\n",
    "Convolution operation → Activation function (e.g., sigmoid)\n",
    "#### Backward Propagation Order（using the Chain Rule）\n",
    "Gradient of activation function → Gradient of convolution operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d994a9-c79f-4f8e-a9c1-2a10502f3873",
   "metadata": {},
   "source": [
    "#### 3.2.1 backward propagation for activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cb7387-bd23-4e31-9504-725225457bdc",
   "metadata": {},
   "source": [
    "Here we briefly review the backward propagation of activation function, you can check the logistic regression slide for the detailed gradient. The activation function is applied element-wise during the forward pass, so during backpropagation, its gradient is computed as the gradient from the upstream layer multiplied by the derivative of the activation function at each element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662b7390-f267-485e-a47a-542d45d116b5",
   "metadata": {},
   "source": [
    "For an activation function $\\sigma(\\cdot)$ and upstream gradient $\\frac{\\partial L}{\\partial Y}$, the gradient w.r.t. the input $X$ is:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\odot \\sigma'(X)$$\n",
    "\n",
    "where $\\odot$ denotes element-wise multiplication, and $\\sigma'(X)$ is the derivative of $\\sigma$ evaluated at the original input $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba868642-ec17-4595-b688-5d0f2672928a",
   "metadata": {},
   "source": [
    "#### 3.2.2 forward propagation for convolution operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9815584d-e897-4912-a042-1896d850a60e",
   "metadata": {},
   "source": [
    "Applying convolutional kernels (also known as filters) to the input data. This process extracts features from the input, which can then be used by subsequent layers in the network. \n",
    "\n",
    "Here's a step-by-step explanation:\n",
    "\n",
    "#### 1.Input Data:\n",
    "The input to the convolutional layer is typically a multi-dimensional array (tensor). For example, in image processing, the input might be a 3D tensor with dimensions [height, width, channels] (e.g., a 28x28x3 image for RGB).\n",
    "\n",
    "#### 2.Convolutional Kernels (Filters):\n",
    "Convolutional kernels are small matrices that slide over the input data to perform the convolution operation.  Each kernel is designed to detect specific features in the input data .\n",
    "For example, if the input is a 28x28x3 image, a kernel might be a 3x3x3 matrix. The kernel size (3x3) defines the spatial extent, and the depth (3) matches the number of input channels(very important!).\n",
    "\n",
    "#### 3.Convolution Process:\n",
    "The convolution operation involves sliding the kernel over the input data and computing the dot product between the kernel and the corresponding region of the input.\n",
    "Specifically, for each position of the kernel on the input, the element-wise multiplication is performed between the kernel and the input region, and then the results are summed to produce a single output value.\n",
    "This process is repeated for every position of the kernel on the input, resulting in a 2D output map (called a feature map) for each kernel.\n",
    "\n",
    "#### 4.Multiple Kernels and Feature Maps:\n",
    "Typically, multiple kernels are used in a convolutional layer. Each kernel produces a different feature map.\n",
    "If there are K kernels, the output will be a 3D tensor with dimensions [output height, output width, K], where each channel corresponds to a feature map generated by one of the kernels.\n",
    "\n",
    "#### 5.Bias Term:\n",
    "A bias term is often added to each feature map after the convolution operation. This bias term is a scalar value that is added to each element of the feature map.\n",
    "The bias helps in shifting the activation function and provides additional flexibility in learning.\n",
    "\n",
    "#### 6.Output Calculation:\n",
    "For each position (i,j) in the output feature map, the value is computed as:\n",
    "$$ Output(i,j) = ∑(Kernel(m,n,c) * Input(i+m,j+n,c)) + Bias $$\n",
    "where (m,n) are the spatial indices of the kernel, and c is the channel index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa401c1-d9ee-40f0-b590-5e58cb3c5666",
   "metadata": {},
   "source": [
    "#### 3.2.3 Backward propagation for the bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5afcf5f-2caa-4161-9ba8-5be038e7acb2",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L}{\\partial b} = \\sum_{i,j} \\frac{\\partial L}{\\partial X(i,j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a536dca-c013-4923-8656-f495e6db6170",
   "metadata": {},
   "source": [
    "here b denotes the bias , the partial derivative denotes the upstream gradient(activation function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da682c-bd34-447a-ae41-9271eba3f7e3",
   "metadata": {},
   "source": [
    "#### 3.2.4 Backward propagation for the convolution kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff882dd9-8daf-4cd3-95f1-5be0d6165b46",
   "metadata": {},
   "source": [
    "$$ Z^{l-1} → Z^{l}_{0} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275cf49-662b-48a4-be9c-216fe49f8143",
   "metadata": {},
   "source": [
    "Here $ Z^{l-1} $denotes the output of $l-1$ layer, $Z^{l}_{0}$ denotes the output after the convolution kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afebc25-b812-46da-9865-65c5ca69d0b0",
   "metadata": {},
   "source": [
    "for simplicity , we consider one kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014783fe-e3f1-462d-9d69-6f3c33def53b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### gradient towards $Z^{l-1}$ \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z^{l-1}[p,q]} = \n",
    "\\sum_{i,j} \\frac{\\partial L}{\\partial Z^l_0[i,j]} \\cdot \n",
    "\\frac{\\partial Z^l_0[i,j]}{\\partial Z^{l-1}[p,q]}\n",
    "$$\n",
    "\n",
    "##### gradient towards $W$ \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W[m,n]} = \n",
    "\\sum_{i,j} \\frac{\\partial L}{\\partial Z^l_0[i,j]} \\cdot \n",
    "\\frac{\\partial Z^l_0[i,j]}{\\partial W[m,n]}\n",
    "$$\n",
    "\n",
    "##### notation\n",
    "\n",
    "- $L$: loss function\n",
    "- $Z^{l-1}$: input of the ${l^{th}}$ layer\n",
    "- $Z^l_0$: output of the kernel operation（before bias term,activation function）\n",
    "- $W$: kernel\n",
    "- $i,j$: coordinate index\n",
    "- $p,q$: coordinate index\n",
    "- $m,n$: coordinate index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b65d2-c8e6-4ee9-9823-8f6f9ea04b40",
   "metadata": {},
   "source": [
    "Just remember: to find derivative with respect to something , we need to find where its information is contained.\n",
    "It is obvious that every element in $Z^{l}_{0}$ contains information of W(the kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f616d6-0a56-4dc0-b8ad-0d960e41a227",
   "metadata": {},
   "source": [
    "# 4- forward ,Backward propagation code for convolution layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7676db2b-f25e-4fcd-98ed-53d97124af89",
   "metadata": {},
   "source": [
    "### 4.1 forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2a100-139e-4959-bfa3-e4e7a30dec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conv_forward(X, W, b, stride=1, pad=0, activation='relu'):\n",
    "    \"\"\"\n",
    "    forward propagation for convolution layer \n",
    "    parameters:\n",
    "        X: input (n, C, H_in, W_in)\n",
    "        W: kernel (F, C, HH, WW)\n",
    "        b: bias (F,)\n",
    "        stride: \n",
    "        pad: \n",
    "        activation: activation function ('relu'/'sigmoid'/'tanh'/None)\n",
    "    return:\n",
    "        out: output (n, F, H_out, W_out)\n",
    "        cache: \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE BEGINS HERE ###\n",
    "    n, C, H_in, W_in = X.shape\n",
    "    F, _, HH, WW = W.shape\n",
    "    \n",
    "    # calculate the shape of output\n",
    "    H_out = int((H_in + 2*pad - HH)/stride) + 1\n",
    "    W_out = int((W_in + 2*pad - WW)/stride) + 1\n",
    "    \n",
    "    #padding\n",
    "    X_pad = np.pad(X, ((0,0), (0,0), (pad,pad), (pad,pad)), mode='constant')\n",
    "    \n",
    "    # calculation\n",
    "    out = np.zeros((n, F, H_out, W_out))\n",
    "    for i in range(n):\n",
    "        for f in range(F):\n",
    "            for h in range(H_out):\n",
    "                for w in range(W_out):\n",
    "                    h_start = h * stride\n",
    "                    h_end = h_start + HH\n",
    "                    w_start = w * stride\n",
    "                    w_end = w_start + WW\n",
    "                    \n",
    "                    out[i,f,h,w] = np.sum(\n",
    "                        X_pad[i,:,h_start:h_end,w_start:w_end] * W[f]\n",
    "                    ) + b[f]\n",
    "    \n",
    "    # activation function\n",
    "    if activation == 'relu':\n",
    "        out = np.maximum(0, out)\n",
    "    elif activation == 'sigmoid':\n",
    "        out = 1/(1+np.exp(-out))\n",
    "    elif activation == 'tanh':\n",
    "        out = np.tanh(out)\n",
    "    \n",
    "    cache = (X, W, b, stride, pad, X_pad, out, activation)\n",
    "    return out, cache\n",
    "    ### YOUR CODE ENDS HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a2f5a-50d9-4d3d-96d1-e9fd3582d228",
   "metadata": {},
   "source": [
    "### 4.2 backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f596f4a-7806-4572-9b6a-d8b67633c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code begins here ###\n",
    "\n",
    "def conv_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    backward propagation for convolution layer\n",
    "    parameters:\n",
    "        dout: upstream gradient (n, F, H_out, W_out)\n",
    "        cache: forward cache\n",
    "    return:\n",
    "        dX: gradient of input (n, C, H_in, W_in)\n",
    "        dW: gradient of kernel (F, C, HH, WW)\n",
    "        db: gradient of bias (F,)\n",
    "    \"\"\"\n",
    "    \n",
    "    X, W, b, stride, pad, X_pad, Z, activation = cache\n",
    "    \n",
    "    # gradient for activation function\n",
    "    if activation == 'relu':\n",
    "        dout = dout * (Z > 0)\n",
    "    elif activation == 'sigmoid':\n",
    "        dout = dout * Z * (1 - Z)\n",
    "    elif activation == 'tanh':\n",
    "        dout = dout * (1 - Z**2)\n",
    "    \n",
    "    n, C, H_in, W_in = X.shape\n",
    "    F, _, HH, WW = W.shape\n",
    "    _, _, H_out, W_out = dout.shape\n",
    "    \n",
    "    # Initialize the gradient\n",
    "    dX = np.zeros_like(X)\n",
    "    dW = np.zeros_like(W)\n",
    "    db = np.zeros_like(b)\n",
    "    dX_pad = np.pad(dX, ((0,0), (0,0), (pad,pad), (pad,pad)), mode='constant')\n",
    "    \n",
    "    # calculate the gradient\n",
    "    for i in range(n):\n",
    "        for f in range(F):\n",
    "            db[f] += np.sum(dout[i,f])\n",
    "            for h in range(H_out):\n",
    "                for w in range(W_out):\n",
    "                    h_start = h * stride\n",
    "                    h_end = h_start + HH\n",
    "                    w_start = w * stride\n",
    "                    w_end = w_start + WW\n",
    "                    \n",
    "                    \n",
    "                    dW[f] += X_pad[i,:,h_start:h_end,w_start:w_end] * dout[i,f,h,w]\n",
    "                    \n",
    "                    \n",
    "                    dX_pad[i,:,h_start:h_end,w_start:w_end] += W[f] * dout[i,f,h,w]\n",
    "    \n",
    "   \n",
    "    if pad > 0:\n",
    "        dX = dX_pad[:,:,pad:-pad,pad:-pad]\n",
    "    else:\n",
    "        dX = dX_pad\n",
    "    \n",
    "    return dX, dW, db\n",
    "\n",
    "### your code ends here ### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3847454-0f63-4c46-805b-6f0c437ab120",
   "metadata": {},
   "source": [
    "Here is a test using which you can examine if your backward propagation code is correctly written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b11696-042e-42c1-848d-35a49dc6efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###Please do not change this code###\n",
    "def eval_numerical_gradient(f, x, df, h=1e-5):\n",
    "    \"\"\"数值计算梯度\"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        \n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "        \n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "def gradient_check():\n",
    "    np.random.seed(1)\n",
    "    X = np.random.randn(1, 1, 5, 5)\n",
    "    W = np.random.randn(2, 1, 3, 3)\n",
    "    b = np.random.randn(2)\n",
    "    stride = 1\n",
    "    pad = 1\n",
    "    activation = 'relu'\n",
    "    \n",
    "    \n",
    "    out, cache = conv_forward(X, W, b, stride, pad, activation)\n",
    "    \n",
    "   \n",
    "    dout = np.random.randn(*out.shape)\n",
    "    \n",
    "    dX, dW, db = conv_backward(dout, cache)\n",
    "    \n",
    "    \n",
    "    f = lambda W: conv_forward(X, W, b, stride, pad, activation)[0]\n",
    "    dW_num = eval_numerical_gradient(f, W, dout)\n",
    "    \n",
    "    f = lambda b: conv_forward(X, W, b, stride, pad, activation)[0]\n",
    "    db_num = eval_numerical_gradient(f, b, dout)\n",
    "    \n",
    "    f = lambda X: conv_forward(X, W, b, stride, pad, activation)[0]\n",
    "    dX_num = eval_numerical_gradient(f, X, dout)\n",
    "    \n",
    "    # 比较差异\n",
    "    dW_error = np.max(np.abs(dW - dW_num))\n",
    "    db_error = np.max(np.abs(db - db_num))\n",
    "    dX_error = np.max(np.abs(dX - dX_num))\n",
    "    \n",
    "    print(\"dW相对误差:\", dW_error/np.max(np.abs(dW_num)))\n",
    "    print(\"db相对误差:\", db_error/np.max(np.abs(db_num)))\n",
    "    print(\"dX相对误差:\", dX_error/np.max(np.abs(dX_num)))\n",
    "    \n",
    "    # 阈值检查\n",
    "    threshold = 1e-6\n",
    "    assert dW_error/np.max(np.abs(dW_num)) < threshold, \"dW梯度检验失败\"\n",
    "    assert db_error/np.max(np.abs(db_num)) < threshold, \"db梯度检验失败\"\n",
    "    assert dX_error/np.max(np.abs(dX_num)) < threshold, \"dX梯度检验失败\"\n",
    "    print(\"所有梯度检验通过!\")\n",
    "\n",
    "# 运行梯度检验\n",
    "gradient_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14976e60-ebce-47ad-b4db-c9499d98abb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
