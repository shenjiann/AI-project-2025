{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5a6e1a",
   "metadata": {},
   "source": [
    "# AlexNet Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0ecb5",
   "metadata": {},
   "source": [
    "**AlexNet** is a groundbreaking convolutional neural network (CNN) architecture that has  a profound impact on the field of deep learning and computer vision. Here is an introduction to AlexNet:\n",
    "\n",
    "### Background\n",
    "- **Year of Introduction**: AlexNet was introduced in 2012 by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.\n",
    "- **Significance**: It was the winning entry in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012, which was a major turning point in the field of computer vision. Its success demonstrated the power of deep learning and convolutional neural networks for image classification tasks.\n",
    "\n",
    "### Key Innovations\n",
    "- **Deep Architecture**: AlexNet was one of the first deep CNNs with multiple convolutional and fully connected layers. This allowed it to learn more complex and abstract features from the input images.\n",
    "- **ReLU Activation Function**: The use of ReLU activation functions instead of traditional sigmoid or tanh functions was a significant innovation. ReLU is computationally efficient and helps to alleviate the vanishing gradient problem, allowing the network to train deeper architectures more effectively.\n",
    "- **Dropout Regularization**: Dropout was used to prevent overfitting. During training, randomly selected neurons are \"dropped out\" (i.e., their outputs are set to zero) with a certain probability. This forces the network to learn more robust features and prevents it from relying too heavily on any single neuron.\n",
    "- **Data Augmentation**: AlexNet also employed data augmentation techniques to increase the size and diversity of the training dataset. This included random cropping, flipping, and color jittering of the input images, which helped the network to generalize better to new, unseen images.\n",
    "\n",
    "In summary, AlexNet is a landmark model in the history of deep learning and computer vision. Its innovative architecture and techniques have had a lasting impact on the field and continue to influence the development of new models and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37cf4c9",
   "metadata": {},
   "source": [
    "### Here is an illustration of the architecture of AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d77ae",
   "metadata": {},
   "source": [
    "```markdown\n",
    "\n",
    "| Layer Type       | Parameters                         | Output Size (C×H×W) |\n",
    "|------------------|------------------------------------|---------------------|\n",
    "| Input Image      | -                                  | 3×224×224           |\n",
    "| Conv1            | kernels:96, size:11×11, stride:4   | 96×55×55            |\n",
    "| MaxPool1         | size:3×3, stride:2                 | 96×27×27            |\n",
    "| Conv2            | kernels:256, size:5×5, padding:2   | 256×27×27           |\n",
    "| MaxPool2         | size:3×3, stride:2                 | 256×13×13           |\n",
    "| Conv3            | kernels:384, size:3×3, padding:1   | 384×13×13           |\n",
    "| Conv4            | kernels:384, size:3×3, padding:1   | 384×13×13           |\n",
    "| Conv5            | kernels:256, size:3×3, padding:1   | 256×13×13           |\n",
    "| MaxPool3         | size:3×3, stride:2                 | 256×6×6             |\n",
    "| Flatten          | -                                  | 9216 (256×6×6)      |\n",
    "| FC1              | 4096 neurons                       | 4096                |\n",
    "| Dropout          | p=0.5                              | 4096                |\n",
    "| FC2              | 4096 neurons                       | 4096                |\n",
    "| Dropout          | p=0.5                              | 4096                |\n",
    "| FC3 (Output)     | num_classes neurons                | num_classes         |\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25276b3",
   "metadata": {},
   "source": [
    "The following is the complete process of using the Fashion-MNIST dataset for classification based on the AlexNet architecture, which is divided into the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c786f111",
   "metadata": {},
   "source": [
    "**step 1 :Packages**\n",
    "\n",
    "**step 2 :Load and preprocess the Fashion-MNIST dataset**\n",
    "\n",
    "**step 3 :Define the AlexNet model**\n",
    "\n",
    "**step 4 :Define training and evaluation functions**\n",
    "\n",
    "**step 5 :Train the model**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f78d36",
   "metadata": {},
   "source": [
    "Here we should know that  **Fashion-MNIST** is a clothing image dataset that serves as an alternative to MNIST, consisting of 10 classes of grayscale images with a size of 28x28.\n",
    "\n",
    "**AlexNet**: The original input size is 224x224x3. **It is necessary to perform size enlargement and channel expansion on the Fashion-MNIST images.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3695f",
   "metadata": {},
   "source": [
    "# 1-Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5e485bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##pip install torch torchvision numpy matplotlib   可以自行下载\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dcc69c",
   "metadata": {},
   "source": [
    "# 2- Load and preprocess the Fashion-MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d1988",
   "metadata": {},
   "source": [
    "In the context of training AlexNet, data augmentation techniques are typically applied during the data loading and preprocessing stage. These techniques help to artificially expand the training dataset by creating modified versions of the images, which can **improve the model's ability to generalize and prevent overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39f5aa6",
   "metadata": {},
   "source": [
    "For the original AlexNet model, the following data augmentation techniques were commonly used:\n",
    "**Random Cropping**: Randomly cropping the input images to a smaller size (e.g., 224x224) from the original larger images (e.g., 256x256). This helps the model to learn from different parts of the image.\n",
    "\n",
    "**Horizontal Flipping**: Randomly flipping the images horizontally. This is a simple and effective way to increase the diversity of the training data.\n",
    "\n",
    "**Color Jittering**: Randomly changing the brightness, contrast, saturation, and hue of the images. This helps the model to become more robust to variations in color and lighting conditions.\n",
    "\n",
    "**Normalization**: Normalizing the pixel values of the images to have zero mean and unit variance. This helps in stabilizing and speeding up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45fb4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "###1.download the data from the website(dataset)\n",
    "###2.batch processing,shuffling(dataloader)\n",
    "###3.define data augmentation pipeline(if you are not familiar with 'transform' operation , I highly recommend you to learn it from pytorch website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f2ed21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 26.4M/26.4M [00:04<00:00, 5.53MB/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File not found or corrupted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      2\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m224\u001b[39m),   \u001b[38;5;66;03m#resize the image to 224*224\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mGrayscale(num_output_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m), \u001b[38;5;66;03m#transfer the channels to 3\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m      6\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m,), (\u001b[38;5;241m0.5\u001b[39m,))  \n\u001b[0;32m      7\u001b[0m ])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#download the data form the website(dataset)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#batch processing,shuffling(dataloader)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFashionMNIST\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mFashionMNIST(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m     14\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\pytorch\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:100\u001b[0m, in \u001b[0;36mMNIST.__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\pytorch\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:188\u001b[0m, in \u001b[0;36mMNIST.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmirror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m URLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    190\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(e)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\pytorch\\Lib\\site-packages\\torchvision\\datasets\\utils.py:391\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[0;32m    389\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[1;32m--> 391\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[0;32m    394\u001b[0m extract_archive(archive, extract_root, remove_finished)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\pytorch\\Lib\\site-packages\\torchvision\\datasets\\utils.py:140\u001b[0m, in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# check integrity of downloaded file\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(fpath, md5):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found or corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File not found or corrupted."
     ]
    }
   ],
   "source": [
    "###YOUR CODE BEGINES HERE\n",
    "\n",
    "#download the data form the website(dataset)\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "#batch processing,shuffling(dataloader)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# define data augmentation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),   #resize the image to 224*224\n",
    "    transforms.Grayscale(num_output_channels=3), #transfer the channels to 3\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  \n",
    "])\n",
    "\n",
    "###YOUR CODE ENDS HERE###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f9628",
   "metadata": {},
   "source": [
    "# 3-Define the AlexNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d0d8c",
   "metadata": {},
   "source": [
    "We have learnt the architecture of AlexNet model before,then you can build that depending on the illustration shown before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b76a7328",
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOUR CODE BEGINES HERE###\n",
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1) \n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "###YOUR CODE ENDS HERE###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9f1e8b",
   "metadata": {},
   "source": [
    "# 4-Define training and evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e42ffa5",
   "metadata": {},
   "source": [
    "Then you can use the training data to train your model and evaluate your model on the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e82036b",
   "metadata": {},
   "source": [
    "below you may define the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03f2294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###1.set model to train mode\n",
    "###2.iterate over batches(here train_loader yields batches of(data,labels))\n",
    "###3.move data to device(GPU/CPU)\n",
    "###4.reset gradients\n",
    "###5.forward pass\n",
    "###6.compute loss\n",
    "###7.backward pass(gradients)\n",
    "###8.update model parameters\n",
    "###9.log progress\n",
    "\n",
    "\n",
    "\n",
    "###YOUR CODE BEGINS HERE###\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)}]  Loss: {loss.item():.6f}\")\n",
    "\n",
    "            \n",
    "###YOUR CODE ENDS HERE###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f328f0c2",
   "metadata": {},
   "source": [
    "Below you may define the testing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cf61e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###1.set model to evaluation mode\n",
    "###2.initilaize correct prediction counter\n",
    "###3.disable gradient calculation\n",
    "###4.iterate over test batches\n",
    "###5.move data to device(GPU/CPU)\n",
    "###6.forward pass\n",
    "###7.calculate accuracy\n",
    "\n",
    "###YOUR CODE BEGINS HERE\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"Test Accuracy: {acc:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "###YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad907a4",
   "metadata": {},
   "source": [
    "# 5-Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66117040",
   "metadata": {},
   "outputs": [],
   "source": [
    "###1.device setup\n",
    "###2.model initialization\n",
    "###3.define optimizer and loss function\n",
    "###4.training and testing your defined functions\n",
    "\n",
    "\n",
    "###YOUR CODE BEGINS HERE\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AlexNet(num_classes=10).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, 100):  \n",
    "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    test(model, device, test_loader)\n",
    "\n",
    "###YOUR CODE ENDS HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0913932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14901981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda-pytorch]",
   "language": "python",
   "name": "conda-env-Anaconda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
