{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e1a821a",
   "metadata": {},
   "source": [
    "# Homework 5: Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a146f97",
   "metadata": {},
   "source": [
    "Welcome to the course **AI and Deep Learning**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e68fc5",
   "metadata": {},
   "source": [
    "Batch Normalization(BN) is a widely used technique in deep learning to improve the training process of neural networks.It aims to address the problem of **covariate shift**.During the training of deep neural networks, the distribution of each layer's inputs changes as the parameters of the previous layers change. This can slow down the training process and make it difficult to choose appropriate learning rates and initializations. Batch normalization stabilizes the training process by **normalizing values before activation** for each neuron and it also introduce **two more parameters to allow for heterogeneity**, making the network more robust and easier to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac58c1f",
   "metadata": {},
   "source": [
    "Learning Goal: In this homework , we first review how the forward propogation of batch normalization works using one sample and mini-batch respectively, then we summarize the purpose of BN,finally the backward propogation of batch normalization is illustrated.You can add the BN step in your network, give it a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f30e51",
   "metadata": {},
   "source": [
    "# Table of content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b856bebd",
   "metadata": {},
   "source": [
    "<style>\n",
    "ol li {\n",
    "  list-style-type: decimal-leading-zero;\n",
    "  padding-left: 20px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<ol>\n",
    "  \n",
    "  <li>Forward Propagation\n",
    "    <ol style=\"list-style-type:lower-alpha;\">\n",
    "      <li>traditional forward propagation without BN senario</li>\n",
    "      <li>how BN works(one sample)</li>\n",
    "        <ol class=\"roman\">\n",
    "          <li>calculate mini-batch mean</li>\n",
    "          <li>calculate mini-batch variance</li>\n",
    "          <li>normalize the input (sample i)</li>\n",
    "          <li>scale and shift(sample i)</li>\n",
    "        </ol>\n",
    "       <li>how BN works(mini-batch)</li>\n",
    "       <li>manually implementing BN (mini-batch)</li>\n",
    "    </ol>\n",
    "  </li>\n",
    "  <li>the purpose of BN</li> \n",
    "    <ol style=\"list-style-type:lower-alpha;\">\n",
    "      <li>problem</li>\n",
    "      <li>solution</li>\n",
    "     </ol>\n",
    "  </li>  \n",
    "  <li>Backward Propagation</li>\n",
    "    <ol style=\"list-style-type:lower-alpha;\">\n",
    "      <li>BP illustration</li>\n",
    "      <li>manually implementing Backward Propagation</li>\n",
    "     </ol>\n",
    "  </li>  \n",
    "  <li>a concise implementation of BN</li>\n",
    "  \n",
    "  \n",
    "  \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd6855",
   "metadata": {},
   "source": [
    "# 1-Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d71ef4",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px;\"> **1.1  traditional forward propogation without BN senario**\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2c2fbf",
   "metadata": {},
   "source": [
    "You must have mastered the basic knowledge of forward propogation and the whole process. Then you will be familiar with this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2124a5",
   "metadata": {},
   "source": [
    "\\\n",
    "\\begin{cases}\n",
    "Z^{[L]} = A^{[L-1]} W^{[L]} + b^{[L]}, \\\\\n",
    "A^{[L]} = \\delta^{[L]}(Z^{[L]}).\n",
    "\\end{cases}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a040882",
   "metadata": {},
   "source": [
    "The forward propogation of the L-th layer can be divided into two steps: **linear transformation** and **activation function transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e176cce",
   "metadata": {},
   "source": [
    "Here $Z^{[L]} \\in \\mathbb{R}^{m \\times d}$ ,where  m is the size of the mini-batch and d is the feature dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0045c04",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px;\"> **1.2  how BN works(one sample)**\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca3b367",
   "metadata": {},
   "source": [
    "Batch Normalization is implemented after the linear transformation,before the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced1ceeb",
   "metadata": {},
   "source": [
    "**1.2.1 calculate mini-batch mean**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6161221",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} \\mathbf{z}_i\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91311613",
   "metadata": {},
   "source": [
    "- $\\mu_B \\in \\mathbb{R}^d$: Mean vector for each feature dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f1013",
   "metadata": {},
   "source": [
    "**1.2.2 calculate mini-batch variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f890ba",
   "metadata": {},
   "source": [
    "$$\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^m (\\mathbf{z}_i - \\mu_B)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d68c8d",
   "metadata": {},
   "source": [
    "- $\\sigma_B^2 \\in \\mathbb{R}^d$ is the variance vector (element-wise square)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de4345",
   "metadata": {},
   "source": [
    "**1.2.3 normalize the input  (sample i)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd81718",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\mathbf{z}}_i = \\frac{\\mathbf{z}_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010e57e4",
   "metadata": {},
   "source": [
    "**Here we may notice that the numerator makes $b^{[L]}$ have no effect.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2674d25",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "where:\n",
    "- $\\hat{\\mathbf{z}}_i$: Normalized feature vector $\\in \\mathbb{R}^d$ (sample $i$)\n",
    "- $\\mathbf{z}_i$: Original input vector (sample $i$)\n",
    "- $\\mu_B$: mini-batch mean vector\n",
    "- $\\sigma_B^2$: mini-batch variance vector\n",
    "- $\\epsilon$: Smoothing term (typically $10^{-5}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3694c248",
   "metadata": {},
   "source": [
    "**1.2.4 scale and shift(sample i)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0d27d8",
   "metadata": {},
   "source": [
    "BN first standardizes each feature(neuron) to zero mean and unit variance,while this helps stabilize training , it also forces a rigid distribution N(0,1),that may reduce the heterogeneity among neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa63f37",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{y}_i = \\gamma \\odot \\hat{\\mathbf{z}}_i + \\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88de608f",
   "metadata": {},
   "source": [
    "- $\\gamma \\in \\mathbb{R}^d$: Scaling parameter (initialized as 1)\n",
    "- $\\beta \\in \\mathbb{R}^d$: Shifting parameter (initialized as 0)  \n",
    "- $\\odot$: Element-wise multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c4595d",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px;\"> **1.3  how BN works(mini-batch)**\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8254e97d",
   "metadata": {},
   "source": [
    "After seeing how to normalize a single sample, we consider **a mini-batch of samples** and represent the entire process using **matrix** notation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7192709",
   "metadata": {},
   "source": [
    "- $\\mathbf{Z}^{[L]} = \\mathbf{A}^{[L-1]} \\mathbf{W}^{[L]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0812a0",
   "metadata": {},
   "source": [
    "- $\n",
    "\\mu_B = \\frac{1}{m} \\mathbf({Z}^{[L]})^{T} \\mathbf{1}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7626383",
   "metadata": {},
   "source": [
    "- $\n",
    "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (\\mathbf{z}_i - \\mu_B)^2 = \\frac{1}{m} \\text{diag} (\\mathbf{Z}^\\top \\mathbf{Z}) - \\mu_B^2\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac630e",
   "metadata": {},
   "source": [
    "- $\n",
    "\\hat{\\mathbf{Z}^{[L]}} = (\\mathbf{Z}^{[L]} - \\mathbf{1} \\mu_B^\\top) \\oslash (\\mathbf{1} \\sqrt{\\sigma_B^2 + \\epsilon}^\\top)\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effc1c54",
   "metadata": {},
   "source": [
    "- $\n",
    "\\mathbf{Y} = \\hat{\\mathbf{Z}^{[L]}} \\odot \\mathbf{1}\\gamma^\\top + \\mathbf{1}\\beta^\\top\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db253013",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px;\"> **1.4  manually implementing BN (mini-batch)**\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc48f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch_norm_forward(X, gamma, beta, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Batch Normalization Forward Propogation\n",
    "    \n",
    "    Parameters:\n",
    "    X: mini-batch samples, shape (m, d)\n",
    "    gamma: shape (d,)\n",
    "    beta: shape (d,)\n",
    "    eps: smoothing term\n",
    "    \n",
    "    Return:\n",
    "    Y: output, shape (m, d)\n",
    "    cache: for backward propogation\n",
    "    \"\"\"\n",
    "    ###1.compute mean vector###\n",
    "    ###2.compute variance vector###\n",
    "    ###3.normalization###\n",
    "    ###4.scale and shift###\n",
    "    \n",
    "    \n",
    "    ###YOUR CODE BENGINS HERE###\n",
    "    # 1. compute mean vector\n",
    "    # 2. compute variance vector\n",
    "    mu = np.mean(X, axis=0)          # (d,)\n",
    "    var = np.var(X, axis=0)          # (d,)\n",
    "    \n",
    "    # 3. normalization\n",
    "    X_hat = (X - mu) / np.sqrt(var + eps)  # (m, d)\n",
    "    \n",
    "    # 3. scale and shift\n",
    "    Y = gamma * X_hat + beta         # (m, d)\n",
    "    \n",
    "    # cache for backward propogation\n",
    "    cache = (X, X_hat, mu, var, gamma, eps)\n",
    "    \n",
    "    return Y, cache\n",
    "   ###YOUR CODES ENDS HERE###\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a128a53",
   "metadata": {},
   "source": [
    "You can use your simulated data to verify if your code is right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0469b6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入 X:\n",
      " [[ 1.  2.  3.  4.]\n",
      " [ 5.  6.  7.  8.]\n",
      " [ 9. 10. 11. 12.]]\n",
      "标准化后输出 Y:\n",
      " [[-1.2247 -1.2247 -1.2247 -1.2247]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 1.2247  1.2247  1.2247  1.2247]]\n"
     ]
    }
   ],
   "source": [
    "###YOUR CODES BEGINS HERE###\n",
    "\n",
    "#simulated data (batch_size=3, feature_dim=4)\n",
    "X = np.array([[1.0, 2.0, 3.0, 4.0],\n",
    "              [5.0, 6.0, 7.0, 8.0],\n",
    "              [9.0, 10.0, 11.0, 12.0]])\n",
    "    \n",
    "# initialization\n",
    "gamma = np.ones(X.shape[1])     \n",
    "beta = np.zeros(X.shape[1])     \n",
    "    \n",
    "# forward propogation\n",
    "Y, cache = batch_norm_forward(X, gamma, beta)\n",
    "    \n",
    "print(\"输入 X:\\n\", X)\n",
    "print(\"标准化后输出 Y:\\n\", np.round(Y, 4))\n",
    "\n",
    "###YOUR CODES ENDS HERE###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f168e3",
   "metadata": {},
   "source": [
    "# 2- the purpose of BN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c083591c",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px;\"> **2.1  problem**\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1620b505",
   "metadata": {},
   "source": [
    "- Deep networks suffer from internal covariate shift, where the distribution of layer inputs changes during training as weights update, forcing later layers to constantly adapt.\n",
    "\n",
    "- Poor weight initialization can lead to vanishing/exploding gradients.\n",
    "\n",
    "- Overfitting due to small batch sizes or noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca76b67f",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px;\"> **2.2  solution**\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c899aa3a",
   "metadata": {},
   "source": [
    "- BN Ensures consistent input distributions across layers, allowing higher learning rates and faster convergence.\n",
    "\n",
    "- By standardizing activations, BN makes the network less sensitive to initial weight scales, enabling more robust training from random starts.\n",
    "\n",
    "- The noise introduced by mini-batch statistics (mean/variance) adds slight stochasticity, acting like a mild regularizer and reducing the need for techniques like Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca9478",
   "metadata": {},
   "source": [
    "# 3- Backward Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f2b10",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px;\"> **3.1  BP illustration**\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0deaeac",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\mathbf{z}}_i = \\frac{\\mathbf{z}_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a647d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{y}_i = \\gamma \\odot \\hat{\\mathbf{z}}_i + \\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cb5924",
   "metadata": {},
   "source": [
    "Assume we have upper gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc370497",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial y_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14837ab6",
   "metadata": {},
   "source": [
    "then we may derive that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa8ea0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\gamma} = \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_i} \\odot \\hat{z}_i, \\quad \\frac{\\partial L}{\\partial \\beta} = \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e4fcba",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{z}_i} = \\left( \\frac{\\partial L}{\\partial \\hat{z}_i} \\cdot \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\right) + \\frac{\\partial L}{\\partial \\sigma_B^2} \\cdot \\frac{2(z_i - \\mu_B)}{m} + \\frac{\\partial L}{\\partial \\mu_B} \\cdot \\frac{1}{m}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{z}_i} = \\frac{\\partial L}{\\partial y_i} \\odot \\gamma, \\quad \\frac{\\partial L}{\\partial \\sigma_B^2} = \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial \\hat{z}_i} (z_i - \\mu_B) \\cdot \\left( -\\frac{1}{2} \\right) (\\sigma_B^2 + \\epsilon)^{-3/2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mu_B} = \\left( \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial \\hat{z}_i} \\cdot \\frac{-1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\right) + \\frac{\\partial L}{\\partial \\sigma_B^2} \\cdot \\frac{-2 \\sum_{i=1}^{m} (z_i - \\mu_B)}{m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328680fa",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px;\"> **3.2  manually implementing Backward Propagation**\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa06f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_backward(dY, cache):\n",
    "    \"\"\"\"\"\n",
    "    \n",
    "    Parameters:\n",
    "    dY: known upper gradient, shape (m, d)\n",
    "    cache: tuple we cache during forward propogation (Z, Z_hat, mu, var, gamma, eps)\n",
    "    \n",
    "    Return:\n",
    "    dZ:  shape (m, d)\n",
    "    dgamma: shape (d,)\n",
    "    dbeta:  shape (d,)\n",
    "    \"\"\"\n",
    "    Z, Z_hat, mu, var, gamma, eps = cache\n",
    "    m, d = Z.shape\n",
    "    ###1.Compute the gradients for gamma and beta###\n",
    "    ###2.compute dZ_hat(∂L/∂Z_hat)###\n",
    "    ###3.conmpute dvar (∂L/∂var)###\n",
    "    ###4.compute dmu (∂L/∂mu)###\n",
    "    ###5.compute dZ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###YOUR CODES BEGINS HERE###\n",
    "    \n",
    "    # 1. Compute the gradients for gamma and beta\n",
    "    dgamma = np.sum(dY * Z_hat, axis=0)  # (d,)\n",
    "    dbeta = np.sum(dY, axis=0)           # (d,)\n",
    "    \n",
    "    # 2. compute dZ_hat (∂L/∂Z_hat)\n",
    "    dZ_hat = dY * gamma                  # (m, d)\n",
    "    \n",
    "    # 3. compute dvar (∂L/∂var)\n",
    "    dvar = np.sum(dZ_hat * (Z - mu) * (-0.5) * (var + eps)**(-1.5), axis=0)  \n",
    "    \n",
    "    # 4. compute dmu (∂L/∂mu)\n",
    "    dmu_part1 = np.sum(dZ_hat * (-1) / np.sqrt(var + eps), axis=0)            \n",
    "    dmu_part2 = dvar * np.mean(-2 * (Z - mu), axis=0)                         \n",
    "    dmu = dmu_part1 + dmu_part2\n",
    "    \n",
    "    # 5. compute dZ \n",
    "    dZ_part1 = dZ_hat / np.sqrt(var + eps)                \n",
    "    dZ_part2 = dvar * 2 * (Z - mu) / m                    \n",
    "    dZ_part3 = dmu / m                                    \n",
    "    dZ = dZ_part1 + dZ_part2 + dZ_part3\n",
    "    \n",
    "    return dZ, dgamma, dbeta\n",
    "    \n",
    "    ###YOUR CODES ENDS HERE###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b4731",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px;\"> **3.3  a test to verify the BP code**\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe547c",
   "metadata": {},
   "source": [
    "below is a test to check to implement the gradient check,using your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84f98724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Check Results:\n",
      "dX error:     2.39e-09 (should be < 1e-7)\n",
      "dgamma error: 9.87e-10 (should be < 1e-7)\n",
      "dbeta error:  1.73e-09 (should be < 1e-7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(2.393039918468964e-09),\n",
       " np.float64(9.866759895117114e-10),\n",
       " np.float64(1.7341300572275556e-09))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###PLEASE DON\"T CHANGE THIS CODE BELOW###\n",
    "\n",
    "import numpy as np\n",
    "def gradient_check_batchnorm():\n",
    "    np.random.seed(1)\n",
    "    m, d = 3, 4  \n",
    "    X = np.random.randn(m, d)\n",
    "    gamma = np.random.randn(d)\n",
    "    beta = np.random.randn(d)\n",
    "    eps = 1e-5\n",
    "    epsilon = 1e-7  \n",
    "\n",
    "    # Forward pass to populate cache\n",
    "    #Here we use the BN forward function you just write\n",
    "    Y, cache = batch_norm_forward(X, gamma, beta, eps)\n",
    "    dY = np.random.randn(*Y.shape)  \n",
    "\n",
    "    # Analytical gradients (from your backward pass)\n",
    "    dZ_analytical, dgamma_analytical, dbeta_analytical = batch_norm_backward(dY, cache)\n",
    "\n",
    "    # Numerical gradient approximation\n",
    "    def compute_numerical_gradient(param, func, idx=None):\n",
    "        param_plus = param.copy()\n",
    "        param_minus = param.copy()\n",
    "        if idx is not None:\n",
    "            param_plus[idx] += epsilon\n",
    "            param_minus[idx] -= epsilon\n",
    "        else:\n",
    "            param_plus += epsilon\n",
    "            param_minus -= epsilon\n",
    "        loss_plus = np.sum(dY * func(param_plus))\n",
    "        loss_minus = np.sum(dY * func(param_minus))\n",
    "        return (loss_plus - loss_minus) / (2 * epsilon)\n",
    "\n",
    "    # Check dgamma (∂L/∂γ)\n",
    "    dgamma_numerical = np.zeros_like(gamma)\n",
    "    for j in range(d):\n",
    "        def func_gamma(gamma_perturbed):\n",
    "            Y_perturbed, _ = batch_norm_forward(X, gamma_perturbed, beta, eps)\n",
    "            return Y_perturbed\n",
    "        dgamma_numerical[j] = compute_numerical_gradient(gamma, func_gamma, j)\n",
    "\n",
    "    # Check dbeta (∂L/∂β)\n",
    "    dbeta_numerical = np.zeros_like(beta)\n",
    "    for j in range(d):\n",
    "        def func_beta(beta_perturbed):\n",
    "            Y_perturbed, _ = batch_norm_forward(X, gamma, beta_perturbed, eps)\n",
    "            return Y_perturbed\n",
    "        dbeta_numerical[j] = compute_numerical_gradient(beta, func_beta, j)\n",
    "\n",
    "    # Check dX (∂L/∂X)\n",
    "    dX_numerical = np.zeros_like(X)\n",
    "    for i in range(m):\n",
    "        for j in range(d):\n",
    "            def func_X(X_perturbed):\n",
    "                Y_perturbed, _ = batch_norm_forward(X_perturbed, gamma, beta, eps)\n",
    "                return Y_perturbed\n",
    "            dX_numerical[i, j] = compute_numerical_gradient(X, func_X, (i, j))\n",
    "\n",
    "    # Relative error calculation\n",
    "    def relative_error(grad_analytical, grad_numerical):\n",
    "        numerator = np.abs(grad_analytical - grad_numerical).sum()\n",
    "        denominator = np.abs(grad_analytical) + np.abs(grad_numerical)\n",
    "        return numerator / np.maximum(1e-8, denominator.sum())\n",
    "\n",
    "    error_dX = relative_error(dZ_analytical, dX_numerical)\n",
    "    error_dgamma = relative_error(dgamma_analytical, dgamma_numerical)\n",
    "    error_dbeta = relative_error(dbeta_analytical, dbeta_numerical)\n",
    "\n",
    "    print(\"Gradient Check Results:\")\n",
    "    print(f\"dX error:     {error_dX:.2e} (should be < 1e-7)\")\n",
    "    print(f\"dgamma error: {error_dgamma:.2e} (should be < 1e-7)\")\n",
    "    print(f\"dbeta error:  {error_dbeta:.2e} (should be < 1e-7)\")\n",
    "\n",
    "    return error_dX, error_dgamma, error_dbeta\n",
    "\n",
    "# Run the check\n",
    "gradient_check_batchnorm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd1a66",
   "metadata": {},
   "source": [
    "# 4- a concise implementation of BN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7551ada",
   "metadata": {},
   "source": [
    "In PyTorch, you can implement batch normalization using`torch.nn.BatchNorm1d`, `torch.nn.BatchNorm2d`, or `torch.nn.BatchNorm3d`, depending on the dimensionality of your  data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b79ad",
   "metadata": {},
   "source": [
    "Below is an example of how to implement batch normalization in a **fully connected neural network** using PyTorch. This example uses torch.nn.BatchNorm1d, which is suitable for fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9774b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a fully connected neural network with batch normalization\n",
    "class FullyConnectedNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FullyConnectedNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f47053",
   "metadata": {},
   "source": [
    "You can also specify the size of the input, hidden layer, and output​ to implementing your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97bef8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4800, -0.3732,  0.2061, -0.4101,  0.4611, -0.0061,  0.1786, -0.3494,\n",
      "         -0.1817,  0.5988],\n",
      "        [ 0.2694, -0.5896, -0.1320, -0.1999, -0.0296,  0.0819, -0.4401,  0.4767,\n",
      "          0.3122,  0.8471],\n",
      "        [ 0.0426,  0.0931, -0.6851, -0.0217,  0.6040, -0.3760,  0.2737,  0.1184,\n",
      "          0.1128,  0.1202],\n",
      "        [-0.2570, -0.0455,  0.0874, -0.4685,  0.5739,  0.5488,  0.4299,  0.8774,\n",
      "         -0.4747, -0.5152]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_size = 784  # For example, 28x28 images flattened\n",
    "hidden_size = 500\n",
    "output_size = 10   # For example, 10 classes\n",
    "\n",
    "# Create an instance of the network\n",
    "net = FullyConnectedNet(input_size, hidden_size, output_size)\n",
    "\n",
    "# Create a random input tensor (e.g., a batch of 4 images)\n",
    "input = torch.randn(4, input_size)\n",
    "\n",
    "output = net(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58eaff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda-pytorch]",
   "language": "python",
   "name": "conda-env-Anaconda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
