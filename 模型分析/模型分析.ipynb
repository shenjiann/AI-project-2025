{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3834e474",
   "metadata": {},
   "source": [
    "# Model Analysis\n",
    "\n",
    "In the previous chapter, we introduced the basic use of `PyTorch` and use a neural network model to predict the probabilities of listed firms' tax non-compliance behaviors. Our previous model may be too simple, this leads to a bad prediction performance. By tunning the hyperparameters or regularizaition, we can further improve our model.\n",
    "\n",
    "So in this chapter, we would first introduce some regularization methods including $\\ell_2$ regularization, dropout, early stopping and batch normalization. Then, we apply these methods to the example of tax non-compliant firms classfication.\n",
    "\n",
    "**Learning Goal**:\n",
    "1. Implementing some regularization methods (L2 penalty, dropout, early stopping);\n",
    "2. Implementing batch normalization;\n",
    "\n",
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac1f17",
   "metadata": {},
   "source": [
    "## 0 - Import Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8bca88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11f716e10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # used for data import\n",
    "import numpy as np # used for numerical operations\n",
    "import torch # used for tensor operations\n",
    "import torch.nn as nn # used for building neural networks\n",
    "from torch.utils.data import DataLoader, TensorDataset # used for creating data loaders\n",
    "from sklearn.preprocessing import StandardScaler # used for standardizing features\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score # used for evaluating models\n",
    "import statsmodels.api as sm # used for probit & logit regression\n",
    "import matplotlib.pyplot as plt # used for plotting PR curves\n",
    "from sklearn.model_selection import train_test_split # used for splitting data into train/valiadtion sets\n",
    "\n",
    "from utils import FNN, train, test\n",
    "\n",
    "np.random.seed(42) # set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5934df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../全连接神经网络/train_data.csv') # import training data\n",
    "test_df = pd.read_csv('../全连接神经网络/test_data.csv')\n",
    "\n",
    "train_df, valid_df = train_test_split(train_df, \n",
    "                                      stratify=train_df['noncompliance'], \n",
    "                                      test_size=0.2, \n",
    "                                      random_state=42,\n",
    "                                      shuffle=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_array = scaler.fit_transform(train_df.drop(['noncompliance'], axis=1))\n",
    "X_valid_array = scaler.transform(valid_df.drop(['noncompliance'], axis=1))\n",
    "X_test_array = scaler.transform(test_df.drop(['noncompliance'], axis=1))\n",
    "y_train_array = train_df['noncompliance'].values\n",
    "y_valid_array = valid_df['noncompliance'].values\n",
    "y_test_array = test_df['noncompliance'].values\n",
    "\n",
    "def prepare_loader(X_array, y_array, batch_size, shuffle):\n",
    "    X_tensor = torch.tensor(X_array, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_array, dtype=torch.float32)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader\n",
    "\n",
    "train_loader = prepare_loader(X_train_array, y_train_array, batch_size=128, shuffle=True)\n",
    "valid_loader = prepare_loader(X_valid_array, y_valid_array, batch_size=128, shuffle=True)\n",
    "test_loader = prepare_loader(X_test_array, y_test_array, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2042e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "848163b8",
   "metadata": {},
   "source": [
    "## 1 - $\\ell_2$ Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322b767",
   "metadata": {},
   "source": [
    "Recall the backpropagation of $\\ell_2$ penalty:\n",
    "$J_2 = \\frac{\\lambda}{2n} \\sum_{l} ||W^{[l]}||_{F}^2$ \n",
    "and the derivative is \n",
    "$\\frac{\\partial J_2}{\\partial W^{[l]}} = \\frac{\\lambda}{n} W^{[l]}.$\n",
    "The update step for $\\ell_2$ penalty is \n",
    "$$\n",
    "W^{[l] (t+1)} = W^{[l] (t)} - \\text{lr} \\times (\\text{grad}_{W^{[l]}} + \\lambda \\times W^{[l] (t)}).\n",
    "$$\n",
    "\n",
    "**In the optimizer without momentum** (e.g. `torch.optim.SGD`), we can implement $\\ell_2$ regularization by adding $\\lambda \\times W$ to the gradients, instead of actually modifying the loss function. This can be done by setting the param `weight_decay`, which allows the optimizer to update parameters in the following way: \n",
    "$$\n",
    "\\text{param} = \\text{param} - \\text{lr} \\times (\\text{grad} + \\text{weight decay} * \\text{param})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c5789c",
   "metadata": {},
   "source": [
    "**In the optimizer with momentum** (e.g. `torch.optim.Adam`), `weight_decay` and $\\ell_2$ regularization are different (for more details, see [Loshchilov and Hutter (2019)](https://arxiv.org/pdf/1711.05101)). The solution is to use `torch.optim.AdamW` optimizer. \n",
    "\n",
    "In the following codes, we \n",
    "1. assign all the weight parameters to a list `decay`, and assign all the bias parameters to  `no_decay`; \n",
    "2. set `weight_decay` to be 3e-3 for weight parameters, and 0 for bias parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51bca3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNN()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "decay, no_decay = [], []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'bias' in name or 'bn' in name:  # exclude bias and batch norm parameters from weight decay\n",
    "        no_decay.append(param)\n",
    "    else: # all other parameters (weights) will have weight decay\n",
    "        decay.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decay, 'weight_decay': 3e-3}, # set weight decay for weights\n",
    "    {'params': no_decay, 'weight_decay': 0.0} # set no weight decay for bias\n",
    "], lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e63fc7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/500] Batch 176 Loss: 0.1042\n",
      "Epoch [100/500] Batch 176 Loss: 0.2062\n",
      "Epoch [150/500] Batch 176 Loss: 0.1470\n",
      "Epoch [200/500] Batch 176 Loss: 0.1231\n",
      "Epoch [250/500] Batch 176 Loss: 0.0650\n",
      "Epoch [300/500] Batch 176 Loss: 0.0521\n",
      "Epoch [350/500] Batch 176 Loss: 0.0505\n",
      "Epoch [400/500] Batch 176 Loss: 0.0755\n",
      "Epoch [450/500] Batch 176 Loss: 0.0409\n",
      "Epoch [500/500] Batch 176 Loss: 0.0347\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, criterion, optimizer, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac9d914a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.8614, grad_fn=<SqrtBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def l2_norm_all_weights(model):\n",
    "    total_norm = 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name and param.requires_grad:\n",
    "            total_norm += torch.norm(param, p=2) ** 2\n",
    "    return total_norm.sqrt()\n",
    "\n",
    "l2_norm_all_weights(model) # check the l2 norm of all weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74bc905c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0022\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_test_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m nn_l2_pred_probs \u001b[38;5;241m=\u001b[39m test(model, test_loader, criterion)\n\u001b[0;32m----> 2\u001b[0m average_precision_score(\u001b[43my_test_array\u001b[49m, nn_l2_pred_probs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test_array' is not defined"
     ]
    }
   ],
   "source": [
    "nn_l2_pred_probs = test(model, test_loader, criterion)\n",
    "average_precision_score(y_test_array, nn_l2_pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad6e345",
   "metadata": {},
   "source": [
    "## 2 - Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7943ec",
   "metadata": {},
   "source": [
    "In the following codes, we build a network with dropout.\n",
    "\n",
    "In the `__init__` method, `self.dropout1 = nn.Dropout(p=0.5)` defines a dropout layer with dropout probability of 0.5, it would randomly zeroes some elements of the input with probability $p=0.5$ during training mode.\n",
    "\n",
    "In the `forward` method, `x = self.dropout1(x)` applies the dropout operator to the output of first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4656ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN_dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connected neural network with dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FNN_dropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(24, 64)\n",
    "        self.dropout1 = nn.Dropout(p=0.5) # define dropout layer with 50% probability\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.dropout2 = nn.Dropout(p=0.3) # define dropout layer with 30% probability\n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "        self.dropout3 = nn.Dropout(p=0.1) # define dropout layer with 10% probability\n",
    "        self.fc4 = nn.Linear(8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x) # apply dropout\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x) # apply dropout\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x) # apply dropout\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout = FNN_dropout()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_dropout.parameters(), lr=3e-4)\n",
    "train(model_dropout, train_loader, criterion, optimizer, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7449f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dropout_pred_probs = test(model_dropout, test_loader, criterion)\n",
    "average_precision_score(y_test_array, nn_dropout_pred_probs) # 0.3123840521753748"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b450c97",
   "metadata": {},
   "source": [
    "## 3 - Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d30cf5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54b0c001",
   "metadata": {},
   "source": [
    "## 4 - Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5ea1e3",
   "metadata": {},
   "source": [
    "In the following codes, we build a fully connected neural network with batch normalization.\n",
    "\n",
    "In the `__init__` method, `self.bn1 = nn.BatchNorm1d(64)` defines a batch normalization over a 2D input with 64 features.\n",
    "\n",
    "In the `forward` method, `x = torch.relu(self.bn1(self.fc1(x)))` firstly do the linear transformation, then apply batch normalization, and then do nonlinear activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f30fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN_bn(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connnected neural network with batch normalization\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FNN_bn, self).__init__()\n",
    "        self.fc1 = nn.Linear(24, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64) # defines batch normalization over a 2D input with 64 features\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32) # defines batch normalization over a 2D input with 32 features\n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "        self.bn3 = nn.BatchNorm1d(8) # defines batch normalization over a 2D input with 8 features\n",
    "        self.fc4 = nn.Linear(8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bn = FNN_bn()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_bn.parameters(), lr=3e-4)\n",
    "train(model_bn, train_loader, criterion, optimizer, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_bn_pred_probs = test(model_bn, test_loader, criterion)\n",
    "average_precision_score(y_test_array, nn_bn_pred_probs) # 0.3123840521753748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d092451b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
