{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3834e474",
   "metadata": {},
   "source": [
    "# Model Analysis\n",
    "\n",
    "In the previous chapter, we introduced the basic use of `PyTorch` and use a neural network model to predict the probabilities of listed firms' tax non-compliance behaviors. Our previous model may be too simple, this leads to a bad prediction performance. By tunning the hyperparameters or regularizaition, we can further improve our model.\n",
    "\n",
    "So in this chapter, we would first introduce some regularization methods including $\\ell_2$ regularization, dropout, early stopping and batch normalization. Then, we apply these methods to the example of tax non-compliant firms classfication.\n",
    "\n",
    "**Learning Goal**:\n",
    "1. Implementing regularization methods including L2 penalty, dropout, early stopping;\n",
    "2. Implementing batch normalization;\n",
    "3. Tunning hyperparameters.\n",
    "\n",
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac1f17",
   "metadata": {},
   "source": [
    "## 0 - Import Packages and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f9f429",
   "metadata": {},
   "source": [
    "Before we start, please copy and paste `FNN` class, `train` and `test` function in last chapter to a .py file, and put the file in your current working diretory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8bca88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11a016e10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # used for data import\n",
    "import numpy as np # used for numerical operations\n",
    "import torch # used for tensor operations\n",
    "import torch.nn as nn # used for building neural networks\n",
    "from torch.utils.data import DataLoader, TensorDataset # used for creating data loaders\n",
    "from sklearn.preprocessing import StandardScaler # used for standardizing features\n",
    "from sklearn.metrics import average_precision_score # used for evaluating models\n",
    "import matplotlib.pyplot as plt # used for plotting PR curves\n",
    "from sklearn.model_selection import train_test_split # used for splitting data into train/valiadtion sets\n",
    "\n",
    "from utils import FNN, train, test \n",
    "import itertools\n",
    "import json # used for write output to txt\n",
    "\n",
    "np.random.seed(42) # set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee4766",
   "metadata": {},
   "source": [
    "Then we import data, all the procedures are the same as last chapter. For simplicity, we define a function `prepare_loader()` to transform ndarray data into DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5934df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../全连接神经网络/train_data.csv') # import training data\n",
    "test_df = pd.read_csv('../全连接神经网络/test_data.csv')\n",
    "\n",
    "train_df, valid_df = train_test_split(train_df, \n",
    "                                      stratify=train_df['noncompliance'], \n",
    "                                      test_size=0.2, \n",
    "                                      random_state=42,\n",
    "                                      shuffle=True)\n",
    "\n",
    "# standardize the input\n",
    "scaler = StandardScaler()\n",
    "X_train_array = scaler.fit_transform(train_df.drop(['noncompliance'], axis=1))\n",
    "X_valid_array = scaler.transform(valid_df.drop(['noncompliance'], axis=1))\n",
    "X_test_array = scaler.transform(test_df.drop(['noncompliance'], axis=1))\n",
    "y_train_array = train_df['noncompliance'].values\n",
    "y_valid_array = valid_df['noncompliance'].values\n",
    "y_test_array = test_df['noncompliance'].values\n",
    "\n",
    "def prepare_loader(X_array, y_array, batch_size, shuffle):\n",
    "    \"\"\"\n",
    "    transform array data into DataLoader\n",
    "    params:\n",
    "        X_array: ndarray, feature;\n",
    "        y_array: ndarray, label;\n",
    "        batch_size: int, batch size;\n",
    "        shuffle: bool, change the order of samples;\n",
    "    return:\n",
    "        loader: DataLoader, data including features and labels used in pytorch.\n",
    "    \"\"\"\n",
    "    X_tensor = torch.tensor(X_array, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_array, dtype=torch.float32)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader\n",
    "\n",
    "# prepare training, validation and test data\n",
    "train_loader = prepare_loader(X_train_array, y_train_array, batch_size=128, shuffle=True)\n",
    "valid_loader = prepare_loader(X_valid_array, y_valid_array, batch_size=128, shuffle=False)\n",
    "test_loader = prepare_loader(X_test_array, y_test_array, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86428628",
   "metadata": {},
   "source": [
    "In the following parts, we would talk about the implementation of $\\ell_2$ regularization, dropout, early stopping and batch normalization respectively. Then, we would use these methods during tunning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848163b8",
   "metadata": {},
   "source": [
    "## 1 - $\\ell_2$ Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322b767",
   "metadata": {},
   "source": [
    "Recall the backpropagation of $\\ell_2$ penalty:\n",
    "$J_2 = \\frac{\\lambda}{2n} \\sum_{l} ||W^{[l]}||_{F}^2$ \n",
    "and the derivative is \n",
    "$\\frac{\\partial J_2}{\\partial W^{[l]}} = \\frac{\\lambda}{n} W^{[l]}.$\n",
    "The update step for $\\ell_2$ penalty is \n",
    "$$\n",
    "W^{[l] (t+1)} = W^{[l] (t)} - \\text{lr} \\times (\\text{grad}_{W^{[l]}} + \\lambda \\times W^{[l] (t)}).\n",
    "$$\n",
    "\n",
    "**In the optimizer without momentum** (e.g. `torch.optim.SGD`), we can implement $\\ell_2$ regularization by adding $\\lambda \\times W$ to the gradients, instead of actually modifying the loss function. This can be done by setting the param `weight_decay`, which allows the optimizer to update parameters in the following way: \n",
    "$$\n",
    "\\text{param} = \\text{param} - \\text{lr} \\times (\\text{grad} + \\text{weight decay} * \\text{param})\n",
    "$$\n",
    "\n",
    "**In the optimizer with momentum** (e.g. `torch.optim.Adam`), `weight_decay` and $\\ell_2$ regularization are different, see [Loshchilov and Hutter (2019)](https://arxiv.org/pdf/1711.05101) for more details. The solution is to use `torch.optim.AdamW` optimizer. \n",
    "\n",
    "In the following codes, we \n",
    "1. assign all the weight parameters to a list `decay`, and assign all the bias parameters to  `no_decay`; \n",
    "2. set `weight_decay` to be 3e-3 for weight parameters, and 0 for bias parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51bca3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNN()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "decay, no_decay = [], []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'bias' in name or 'bn' in name:  # exclude bias and batch norm parameters from weight decay\n",
    "        no_decay.append(param)\n",
    "    else: # all other parameters (weights) will have weight decay\n",
    "        decay.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decay, 'weight_decay': 3e-3}, # set weight decay for weights\n",
    "    {'params': no_decay, 'weight_decay': 0.0} # set no weight decay for bias\n",
    "], lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099d7127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[-0.0058,  0.0787,  0.0335,  ..., -0.0343,  0.1886, -0.0106],\n",
       "           [-0.1383, -0.0017,  0.0264,  ..., -0.1703, -0.0990, -0.0679],\n",
       "           [-0.1147,  0.1227, -0.1480,  ..., -0.0491,  0.1752, -0.1304],\n",
       "           ...,\n",
       "           [ 0.0693, -0.1568, -0.0350,  ...,  0.1506,  0.1808, -0.1931],\n",
       "           [-0.1398, -0.1738,  0.1148,  ..., -0.0646,  0.0186, -0.1057],\n",
       "           [ 0.1158, -0.1126,  0.1776,  ..., -0.0869,  0.0824, -0.0789]],\n",
       "          requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[ 0.0322, -0.0001,  0.0216,  ..., -0.0278, -0.0266, -0.0835],\n",
       "           [ 0.0168,  0.0094, -0.0465,  ..., -0.1020, -0.0676, -0.0126],\n",
       "           [ 0.1078,  0.0735,  0.0284,  ..., -0.0300, -0.1072, -0.1108],\n",
       "           ...,\n",
       "           [ 0.0405, -0.0099,  0.0761,  ...,  0.0390,  0.0831,  0.0148],\n",
       "           [ 0.0255,  0.1000, -0.0486,  ..., -0.0704, -0.1136,  0.0815],\n",
       "           [-0.1006, -0.0309,  0.0263,  ...,  0.0507, -0.0593, -0.0708]],\n",
       "          requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[ 5.4960e-02, -4.9577e-02,  1.0183e-01,  1.0397e-01, -1.4448e-01,\n",
       "            -1.4306e-01,  6.0388e-02, -3.5595e-02,  4.4725e-02,  1.1191e-01,\n",
       "            -3.0895e-02, -1.0829e-01, -1.6231e-01,  8.2342e-03, -1.0723e-01,\n",
       "            -6.1145e-02,  9.4671e-02, -9.0386e-02, -8.3203e-02, -1.3054e-01,\n",
       "             1.6768e-01, -1.4878e-01,  1.3333e-01,  7.6917e-02, -1.2833e-01,\n",
       "            -1.7178e-01,  1.6343e-01,  1.5689e-01, -6.9415e-02, -1.1734e-01,\n",
       "            -4.6144e-02, -4.0743e-02],\n",
       "           [ 1.4688e-01,  4.2677e-02,  3.1920e-02, -1.1543e-01, -4.1244e-02,\n",
       "             1.3680e-01, -1.1885e-01,  1.3798e-01,  1.6869e-01, -1.4906e-01,\n",
       "            -2.4287e-03,  1.6695e-01, -8.6424e-03,  7.6413e-02, -1.5987e-01,\n",
       "            -1.2437e-01, -1.4804e-01, -1.6711e-01,  1.2685e-01,  4.3922e-02,\n",
       "             1.7176e-02,  1.3278e-01, -5.2021e-02,  1.3833e-02,  1.5958e-01,\n",
       "            -3.8721e-02, -1.0073e-01,  5.2513e-02, -1.7601e-01,  1.0704e-01,\n",
       "             4.6062e-02,  1.4047e-01],\n",
       "           [ 1.1854e-01,  1.0087e-01, -1.1370e-01, -1.7357e-01, -1.2210e-01,\n",
       "             1.4915e-01,  2.8802e-03,  5.7845e-02, -1.5407e-01, -1.2127e-01,\n",
       "            -1.2407e-01,  1.6334e-02,  1.4531e-01,  2.7592e-02, -5.8703e-02,\n",
       "             1.0455e-01,  7.0834e-02,  6.9848e-02,  1.3822e-01,  4.8099e-04,\n",
       "            -7.2808e-02, -2.3166e-02, -9.3850e-02,  2.9864e-02,  1.7175e-02,\n",
       "             8.6653e-02, -4.3761e-02,  1.0673e-02,  1.4250e-01, -1.3644e-01,\n",
       "            -5.6962e-02,  3.9101e-02],\n",
       "           [-1.3145e-01,  1.1886e-01, -8.4649e-02, -9.7597e-02,  1.5061e-01,\n",
       "             1.8474e-02,  1.3057e-01, -1.6301e-01,  1.0851e-02, -7.4034e-03,\n",
       "            -4.1449e-04,  1.1880e-01, -9.9911e-03,  7.7396e-02,  7.3988e-02,\n",
       "            -1.6671e-01,  1.1654e-01,  1.4630e-01,  9.5227e-02, -5.3308e-02,\n",
       "             1.5871e-01, -2.1009e-02, -1.7013e-01,  6.0508e-02,  1.2161e-01,\n",
       "            -4.0323e-02,  1.7060e-01, -8.5623e-03,  1.1564e-01,  1.1682e-01,\n",
       "             1.7617e-01, -8.3592e-02],\n",
       "           [-4.7213e-02, -5.5105e-02,  2.7896e-02,  1.4571e-01, -6.3371e-02,\n",
       "             1.4552e-01, -1.3281e-02,  1.3094e-01,  9.7012e-02, -9.1640e-02,\n",
       "             4.2630e-02, -9.6696e-02,  1.1635e-01,  5.4939e-02,  7.1520e-03,\n",
       "            -1.3996e-01, -9.2135e-02,  4.8392e-02,  6.0528e-02,  1.0156e-01,\n",
       "            -1.2963e-02, -1.4878e-01, -4.7031e-02, -1.2439e-01,  1.3179e-01,\n",
       "            -2.6892e-02,  1.1521e-01, -1.0817e-01,  1.0783e-02,  1.1249e-01,\n",
       "            -4.7650e-03,  2.4740e-02],\n",
       "           [-1.3796e-01, -1.3238e-01,  1.0378e-01,  9.9318e-02, -3.4053e-03,\n",
       "            -7.3757e-02,  1.0390e-01, -7.7802e-02, -2.7235e-02,  6.0092e-02,\n",
       "             1.9732e-02,  1.3381e-01,  8.7559e-02, -3.4298e-02,  1.5305e-01,\n",
       "            -1.3054e-02,  1.2901e-01,  1.2969e-03,  4.1619e-02,  5.8287e-02,\n",
       "            -5.9060e-02,  1.4764e-01, -5.7992e-03,  1.2084e-01, -3.7288e-03,\n",
       "            -9.2660e-02,  1.0907e-02,  4.2580e-02, -2.2976e-02,  5.0793e-02,\n",
       "            -1.1032e-01,  2.0812e-02],\n",
       "           [-1.7185e-01, -1.4882e-01, -4.8710e-02, -7.8155e-02,  1.6306e-01,\n",
       "            -4.5853e-02, -6.5668e-02,  2.9616e-02,  5.0894e-02, -6.9630e-02,\n",
       "            -8.6219e-02,  3.0851e-02, -1.6096e-01,  4.4114e-02, -1.2780e-01,\n",
       "            -1.0082e-01,  5.5805e-02, -1.0489e-01, -5.4193e-02,  2.0061e-02,\n",
       "             5.8700e-02,  4.4388e-02, -1.0187e-01,  1.5445e-01, -1.5724e-01,\n",
       "             9.6440e-02, -1.1924e-01, -1.2444e-01, -1.2525e-01,  9.7810e-02,\n",
       "            -1.0845e-01, -4.6220e-02],\n",
       "           [ 7.7585e-02,  1.0360e-01,  9.3472e-02,  6.7931e-02,  5.4947e-02,\n",
       "             1.4292e-01, -4.3030e-02, -6.8446e-02,  1.1236e-01,  7.6467e-02,\n",
       "            -1.5955e-01, -1.6220e-01,  1.1967e-01, -7.2057e-02, -6.6934e-02,\n",
       "             1.7152e-01, -1.5350e-01,  1.2047e-01, -8.7060e-02,  4.8518e-05,\n",
       "            -7.7321e-02, -1.7174e-01,  1.9651e-02,  1.5643e-01,  2.0971e-03,\n",
       "            -5.1222e-02, -9.7407e-03, -1.7591e-01, -5.5517e-02,  1.3934e-01,\n",
       "             1.2767e-01,  7.3296e-02]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[-0.0762, -0.1995,  0.0003,  0.2055, -0.0577,  0.1066,  0.1704, -0.1840]],\n",
       "          requires_grad=True)],\n",
       "  'weight_decay': 0.003},\n",
       " {'params': [Parameter containing:\n",
       "   tensor([-0.1519,  0.1305, -0.1527, -0.0672,  0.0542, -0.1504,  0.0738, -0.0298,\n",
       "           -0.0400,  0.0296,  0.0116,  0.0124,  0.1776,  0.0128, -0.1863,  0.1143,\n",
       "           -0.0137,  0.1524,  0.2009, -0.1099, -0.1951,  0.0833,  0.1518,  0.1347,\n",
       "           -0.1837,  0.1734, -0.0167,  0.1008, -0.0315,  0.1526,  0.0699, -0.0564,\n",
       "           -0.0131, -0.1877,  0.1048, -0.1290,  0.0745,  0.1046,  0.0573, -0.1245,\n",
       "           -0.1977, -0.0345, -0.2038, -0.0699, -0.1524,  0.0200, -0.1382,  0.1427,\n",
       "            0.1175, -0.1020,  0.1697,  0.0330, -0.1708,  0.1120, -0.0011, -0.0062,\n",
       "            0.1079, -0.0068,  0.0983, -0.0831,  0.1861, -0.1537,  0.0070,  0.1110],\n",
       "          requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([-0.0414,  0.0588,  0.0626,  0.0488,  0.0375,  0.1234, -0.0265, -0.0623,\n",
       "           -0.0289,  0.0603, -0.1059,  0.0919,  0.0604,  0.0495,  0.0823, -0.1229,\n",
       "            0.0453,  0.0436,  0.0609,  0.0218, -0.0558, -0.0754,  0.1035,  0.0555,\n",
       "            0.0175, -0.1149, -0.0045,  0.0814, -0.0966, -0.0760, -0.0749,  0.0244],\n",
       "          requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([ 0.0079,  0.0781, -0.1091, -0.0526,  0.1470, -0.0197,  0.0708, -0.1632],\n",
       "          requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([0.1685], requires_grad=True)],\n",
       "  'weight_decay': 0.0}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63fc7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader, criterion, optimizer, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm_all_weights(model):\n",
    "    total_norm = 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name and param.requires_grad:\n",
    "            total_norm += torch.norm(param, p=2) ** 2\n",
    "    return total_norm.sqrt()\n",
    "\n",
    "l2_norm_all_weights(model) # check the l2 norm of all weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bc905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_l2_pred_probs = test(model, test_loader, criterion)\n",
    "average_precision_score(y_test_array, nn_l2_pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad6e345",
   "metadata": {},
   "source": [
    "## 2 - Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7943ec",
   "metadata": {},
   "source": [
    "In the following codes, we build a network with dropout.\n",
    "\n",
    "In the `__init__` method, `self.dropout1 = nn.Dropout(p=0.5)` defines a dropout layer with dropout probability of 0.5, it would randomly zeroes some elements of the input with probability $p=0.5$ during training mode.\n",
    "\n",
    "In the `forward` method, `x = self.dropout1(x)` applies the dropout operator to the output of first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4656ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN_dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connected neural network with dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FNN_dropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(24, 64)\n",
    "        self.dropout1 = nn.Dropout(p=0.5) # define dropout layer with 50% probability\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.dropout2 = nn.Dropout(p=0.3) # define dropout layer with 30% probability\n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "        self.dropout3 = nn.Dropout(p=0.1) # define dropout layer with 10% probability\n",
    "        self.fc4 = nn.Linear(8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x) # apply dropout\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x) # apply dropout\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x) # apply dropout\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout = FNN_dropout()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_dropout.parameters(), lr=3e-4)\n",
    "train(model_dropout, train_loader, criterion, optimizer, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7449f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dropout_pred_probs = test(model_dropout, test_loader, criterion)\n",
    "average_precision_score(y_test_array, nn_dropout_pred_probs) # 0.3123840521753748"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b450c97",
   "metadata": {},
   "source": [
    "## 3 - Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637892a4",
   "metadata": {},
   "source": [
    "The implementation of early stopping is easy, we just need to modify the `train` function, if no significant improvement for some epochs, then we break the loop.\n",
    "\n",
    "In the following codes, we define a `EarlyStopping` class to detect significant validation loss improvement. `__call__()` method would:\n",
    "1. if significant improvement detected, then save the model and update `best_valid_loss`\n",
    "2. if no significant improvement detected, then `counter += 1`\n",
    "   1. if `counter` >= `patience`, then break the detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8bad27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    '''Early stops training if validation loss doesnt imporve after patience'''\n",
    "    def __init__(self, patience=50, delta=0, path='checkpoint.pt', verbose=False):\n",
    "        '''\n",
    "        params:\n",
    "            patience: int, number of epochs to wait since no improvement\n",
    "            delta: float, loss reduction required to detect siginificant improvement\n",
    "            path: path to save the checkpoint\n",
    "        '''\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0 # counter of patience\n",
    "        self.stop = False # flag for stop training\n",
    "        self.best_valid_loss = None\n",
    "        # self.verbose = verbose\n",
    "        self.path = path # path to save the check point\n",
    "    \n",
    "    def __call__(self, valid_loss, model):\n",
    "        if self.best_valid_loss is None:\n",
    "            self.best_valid_loss = valid_loss\n",
    "            self.save_checkpoint(valid_loss, model)\n",
    "        elif valid_loss < self.best_valid_loss - self.delta:\n",
    "            # significant improvement detect\n",
    "            self.best_valid_loss = valid_loss\n",
    "            self.save_checkpoint(valid_loss, model)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # no significant improvement\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.stop = True\n",
    "\n",
    "    def save_checkpoint(self, valid_loss, model):\n",
    "        '''save models when validation loss decreases'''\n",
    "        # if self.verbose:\n",
    "        #     print(f'loss improvement: {valid_loss:.6f}. Saving model.')\n",
    "        torch.save(model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820c342f",
   "metadata": {},
   "source": [
    "After defining the `EarlyStopping` class, we define a `train_and_validate` function in the following way\n",
    "1. train the model, then return a trained model (nearly the same as last chapter's `train` function);\n",
    "2. validate on the validation set, and return `valid_loss`;\n",
    "3. call the `early_stopping` object to detect significant improment, if `early_stopping.early_stop == True`, then we break the loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c4e7274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, valid_loader, criterion, optimizer, num_epochs, use_early_stop=True):\n",
    "    '''\n",
    "    train the model and early stopping on the validation loss\n",
    "    params:\n",
    "        train_loader: DataLoader, training data;\n",
    "        valid_loader: DataLoader, validation data;\n",
    "        criterion: loss function;\n",
    "        optimizer: optimizer that update parameters\n",
    "        num_epochs: int, number of epochs\n",
    "        use_early_stop: bool, if False then train until the last epoch\n",
    "    return:\n",
    "        model: trained model\n",
    "        train_losses: list, training loss of all epoches\n",
    "        valid_losses: list, validation loss of all epoches\n",
    "    '''\n",
    "    train_losses, valid_losses = [], [] # used to store loss of all epochs\n",
    "    early_stopping = EarlyStopping() # initialize an EarlyStopping object\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train the model\n",
    "        model.train()\n",
    "        acc_train_loss = 0 # used to store training loss of all batches\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad() # clear previous gradients\n",
    "            output = model(data) # forward propagation\n",
    "            loss = criterion(output.squeeze(1), target) # calculate average batch loss\n",
    "            loss.backward() # backpropagation\n",
    "            optimizer.step() # update parameters\n",
    "            acc_train_loss += loss.item() * data.size(0) # accumulate loss\n",
    "        train_loss = acc_train_loss / len(train_loader.dataset) # calculate average epoch loss\n",
    "        train_losses.append(train_loss)\n",
    "        if (epoch + 1) % 50 == 0: # print loss each 50 epochs\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # validate the model\n",
    "        valid_loss, _ = test(model, valid_loader, criterion)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        # early stopping\n",
    "        if use_early_stop:\n",
    "            early_stopping(valid_loss, model)\n",
    "            if early_stopping.stop:\n",
    "                print(f'early stopping in epoch {epoch}.')\n",
    "                break\n",
    "            model.load_state_dict(torch.load('checkpoint.pt', weights_only=True)) # load the best model\n",
    "    return model, train_losses, valid_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d7d92",
   "metadata": {},
   "source": [
    "In the following codes, we train the model with early stopping. The training procedures stops at epoch 142, and epoch 92 has the minimum validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30271b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "model, train_losses, valid_losses = train_and_validate(model, train_loader, valid_loader, criterion, optimizer, 500, use_early_stop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c48748",
   "metadata": {},
   "source": [
    "We can plot the training loss and validation loss of all epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_losses)),train_losses, label='Training Loss')\n",
    "plt.plot(range(len(valid_losses)),valid_losses,label='Validation Loss')\n",
    "plt.axvline(valid_losses.index(min(valid_losses)), linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.xlim(0, len(train_losses))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b0c001",
   "metadata": {},
   "source": [
    "## 4 - Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5ea1e3",
   "metadata": {},
   "source": [
    "In the following codes, we build a fully connected neural network with batch normalization.\n",
    "\n",
    "In the `__init__` method, `self.bn1 = nn.BatchNorm1d(64)` defines a batch normalization over a 2D input with 64 features.\n",
    "\n",
    "In the `forward` method, `x = torch.relu(self.bn1(self.fc1(x)))` firstly do the linear transformation, then apply batch normalization, and then do nonlinear activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f30fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN_bn(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connnected neural network with batch normalization\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FNN_bn, self).__init__()\n",
    "        self.fc1 = nn.Linear(24, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64) # defines batch normalization over a 2D input with 64 features\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32) # defines batch normalization over a 2D input with 32 features\n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "        self.bn3 = nn.BatchNorm1d(8) # defines batch normalization over a 2D input with 8 features\n",
    "        self.fc4 = nn.Linear(8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bn = FNN_bn()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_bn.parameters(), lr=3e-4)\n",
    "train(model_bn, train_loader, criterion, optimizer, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_bn_pred_probs = test(model_bn, test_loader, criterion)\n",
    "average_precision_score(y_test_array, nn_bn_pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e01c181",
   "metadata": {},
   "source": [
    "## 5 - Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40ab64af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, dropout_rates, use_batchnorm=False):\n",
    "        '''\n",
    "        params:\n",
    "            input_dim: int, dimension of input features;\n",
    "            hidden_layers: list, number of neurons for each hidden layers;\n",
    "            dropout_rates: list, same length as hidden_layers, dropout probability for each hidden layer;\n",
    "            use_batchnorm: bool, use batch normalization in each layer.\n",
    "        '''\n",
    "        super(Net, self).__init__()\n",
    "        layers = [] \n",
    "        prev_dim = input_dim\n",
    "        for  i, hidden_dim in enumerate(hidden_layers):\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout_rates[i] > 0:\n",
    "                layers.append(nn.Dropout(dropout_rates[i]))\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd9742",
   "metadata": {},
   "source": [
    "`param_grid` includes all the possible values of hyperparameters. Since `hidden_layers` and `dropout_rates` must have the same length, after expanding the grid, we need to filter out those combinations with different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00655702",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'batch_size': [128],\n",
    "    'hidden_layers': [[64, 32, 16], [32, 8]],\n",
    "    'dropout_rates': [[0.5, 0.3, 0.1], [0.3, 0.2]],\n",
    "    'batch_norm': [True, False],\n",
    "    'criterion': ['BCE'],\n",
    "    'optimizer': ['Adam'],\n",
    "    'lr': [1e-3, 3e-4],\n",
    "    'weight_decay': [0, 1e-4],\n",
    "    'num_epochs': [100, 300, 500],\n",
    "    'early_stop': [True, False],\n",
    "}\n",
    "\n",
    "# expand the grid to all combinations\n",
    "keys, values = zip(*param_grid.items())\n",
    "all_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "# filter out combination that hidden_layers and dropout_rates have different lengths\n",
    "all_combinations = [\n",
    "    combo for combo in all_combinations \n",
    "    if len(combo['hidden_layers']) == len(combo['dropout_rates'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc5649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(combinations, output_file='output.txt'):\n",
    "    with open(output_file, 'w') as f:\n",
    "        pass\n",
    "\n",
    "    for config in combinations:\n",
    "        print(f'config: {config}')\n",
    "    \n",
    "        # prepare data\n",
    "        train_loader = prepare_loader(X_train_array, y_train_array, batch_size=config['batch_size'], shuffle=True)\n",
    "        valid_loader = prepare_loader(X_valid_array, y_valid_array, batch_size=config['batch_size'], shuffle=False)\n",
    "        \n",
    "        # build model\n",
    "        model = Net(\n",
    "            input_dim=next(iter(train_loader))[0].shape[1],\n",
    "            hidden_layers=config['hidden_layers'],\n",
    "            dropout_rates=config['dropout_rates'],\n",
    "            use_batchnorm=config['batch_norm']\n",
    "        )\n",
    "        \n",
    "        # loss\n",
    "        if config['criterion'] == 'BCE':\n",
    "            criterion = nn.BCELoss()\n",
    "        else:\n",
    "            raise ValueError('Unknown criterion')\n",
    "        \n",
    "        # optimizer\n",
    "        decay, no_decay = [], []\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'bias' in name or 'bn' in name:\n",
    "                no_decay.append(param)\n",
    "            else:\n",
    "                decay.append(param)\n",
    "\n",
    "        if config['optimizer'] == 'Adam':\n",
    "            optimizer = torch.optim.Adam([\n",
    "                {'params': decay, 'weight_decay': config['weight_decay']},\n",
    "                {'params': no_decay, 'weight_decay': 0.0}\n",
    "            ], lr=3e-4)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown optimizer\")\n",
    "        \n",
    "        # train & validate\n",
    "        model, _, _ = train_and_validate(model, train_loader, valid_loader, criterion, optimizer, config['num_epochs'], config['early_stop'])\n",
    "        \n",
    "        # save results\n",
    "        valid_loss, pred_probs = test(model, valid_loader, criterion)\n",
    "        config['valid_loss'] = valid_loss\n",
    "        valid_auprc = average_precision_score(y_valid_array, pred_probs)\n",
    "        result = {**config, \"valid_auprc\": valid_auprc}\n",
    "\n",
    "        with open(output_file, 'a') as f:\n",
    "            f.write(json.dumps(result) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce11a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1740\n",
      "Epoch [100/100] Loss: 0.1727\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1676\n",
      "Epoch [100/100] Loss: 0.1654\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1784\n",
      "Epoch [100/300] Loss: 0.1763\n",
      "Epoch [150/300] Loss: 0.1726\n",
      "Epoch [200/300] Loss: 0.1725\n",
      "Epoch [250/300] Loss: 0.1707\n",
      "early stopping in epoch 255.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1671\n",
      "Epoch [100/300] Loss: 0.1634\n",
      "Epoch [150/300] Loss: 0.1605\n",
      "Epoch [200/300] Loss: 0.1602\n",
      "Epoch [250/300] Loss: 0.1598\n",
      "Epoch [300/300] Loss: 0.1594\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1724\n",
      "Epoch [100/500] Loss: 0.1717\n",
      "early stopping in epoch 107.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1677\n",
      "Epoch [100/500] Loss: 0.1635\n",
      "Epoch [150/500] Loss: 0.1634\n",
      "Epoch [200/500] Loss: 0.1606\n",
      "Epoch [250/500] Loss: 0.1621\n",
      "Epoch [300/500] Loss: 0.1614\n",
      "Epoch [350/500] Loss: 0.1604\n",
      "Epoch [400/500] Loss: 0.1615\n",
      "Epoch [450/500] Loss: 0.1611\n",
      "Epoch [500/500] Loss: 0.1593\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1729\n",
      "Epoch [100/100] Loss: 0.1719\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1665\n",
      "Epoch [100/100] Loss: 0.1649\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1737\n",
      "Epoch [100/300] Loss: 0.1707\n",
      "Epoch [150/300] Loss: 0.1710\n",
      "Epoch [200/300] Loss: 0.1699\n",
      "Epoch [250/300] Loss: 0.1686\n",
      "early stopping in epoch 260.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1666\n",
      "Epoch [100/300] Loss: 0.1640\n",
      "Epoch [150/300] Loss: 0.1629\n",
      "Epoch [200/300] Loss: 0.1628\n",
      "Epoch [250/300] Loss: 0.1630\n",
      "Epoch [300/300] Loss: 0.1621\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1767\n",
      "early stopping in epoch 64.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1669\n",
      "Epoch [100/500] Loss: 0.1634\n",
      "Epoch [150/500] Loss: 0.1626\n",
      "Epoch [200/500] Loss: 0.1622\n",
      "Epoch [250/500] Loss: 0.1606\n",
      "Epoch [300/500] Loss: 0.1618\n",
      "Epoch [350/500] Loss: 0.1608\n",
      "Epoch [400/500] Loss: 0.1609\n",
      "Epoch [450/500] Loss: 0.1595\n",
      "Epoch [500/500] Loss: 0.1591\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1790\n",
      "early stopping in epoch 98.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1731\n",
      "Epoch [100/100] Loss: 0.1670\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1747\n",
      "Epoch [100/300] Loss: 0.1762\n",
      "Epoch [150/300] Loss: 0.1753\n",
      "early stopping in epoch 175.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1712\n",
      "Epoch [100/300] Loss: 0.1682\n",
      "Epoch [150/300] Loss: 0.1647\n",
      "Epoch [200/300] Loss: 0.1640\n",
      "Epoch [250/300] Loss: 0.1624\n",
      "Epoch [300/300] Loss: 0.1617\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1762\n",
      "Epoch [100/500] Loss: 0.1749\n",
      "Epoch [150/500] Loss: 0.1721\n",
      "Epoch [200/500] Loss: 0.1708\n",
      "Epoch [250/500] Loss: 0.1705\n",
      "early stopping in epoch 284.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1726\n",
      "Epoch [100/500] Loss: 0.1665\n",
      "Epoch [150/500] Loss: 0.1657\n",
      "Epoch [200/500] Loss: 0.1640\n",
      "Epoch [250/500] Loss: 0.1616\n",
      "Epoch [300/500] Loss: 0.1601\n",
      "Epoch [350/500] Loss: 0.1603\n",
      "Epoch [400/500] Loss: 0.1599\n",
      "Epoch [450/500] Loss: 0.1609\n",
      "Epoch [500/500] Loss: 0.1585\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1760\n",
      "Epoch [100/100] Loss: 0.1740\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1719\n",
      "Epoch [100/100] Loss: 0.1669\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1757\n",
      "Epoch [100/300] Loss: 0.1741\n",
      "early stopping in epoch 146.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1723\n",
      "Epoch [100/300] Loss: 0.1664\n",
      "Epoch [150/300] Loss: 0.1642\n",
      "Epoch [200/300] Loss: 0.1623\n",
      "Epoch [250/300] Loss: 0.1616\n",
      "Epoch [300/300] Loss: 0.1617\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1774\n",
      "early stopping in epoch 78.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0.5, 0.3, 0.1], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1723\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_combinations\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 34\u001b[0m, in \u001b[0;36mtune\u001b[0;34m(combinations, output_file)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown optimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# train & validate\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m model, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mearly_stop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# save results\u001b[39;00m\n\u001b[1;32m     37\u001b[0m valid_loss, pred_probs \u001b[38;5;241m=\u001b[39m test(model, valid_loader, criterion)\n",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, valid_loader, criterion, optimizer, num_epochs, use_early_stop)\u001b[0m\n\u001b[1;32m     26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), target) \u001b[38;5;66;03m# calculate average batch loss\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# backpropagation\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# update parameters\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     acc_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# accumulate loss\u001b[39;00m\n\u001b[1;32m     30\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m acc_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;66;03m# calculate average epoch loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/workenv/lib/python3.12/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/workenv/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/workenv/lib/python3.12/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    154\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/miniconda3/envs/workenv/lib/python3.12/site-packages/torch/optim/adam.py:99\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam does not support sparse gradients, please consider SparseAdam instead\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     97\u001b[0m grads\u001b[38;5;241m.\u001b[39mappend(p\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[0;32m---> 99\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Lazy state initialization\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# note(crcrpar): [special device hosting for step]\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# Deliberately host `step` on CPU if both capturable and fused are off.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# This is because kernel launches are costly on CUDA and XLA.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/workenv/lib/python3.12/site-packages/torch/_tensor.py:1034\u001b[0m, in \u001b[0;36mTensor.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1025\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1026\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1030\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1031\u001b[0m         )\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m-> 1034\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# Do NOT handle __torch_function__ here as user's default\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# implementation that handle most functions will most likely do it wrong.\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;66;03m# It can be easily overridden by defining this method on the user\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;66;03m# subclass if needed.\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__dir__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tune(all_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab313b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mresults\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f7637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
