{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3834e474",
   "metadata": {},
   "source": [
    "# Model Analysis\n",
    "\n",
    "In the previous chapter, we introduced the basic use of `PyTorch` and use a neural network model to predict the probabilities of listed firms' tax non-compliance behaviors. Our previous model may be too simple, this leads to a bad prediction performance. By tunning the hyperparameters or regularizaition, we can further improve our model.\n",
    "\n",
    "So in section 1- 4, we would first introduce the implementation of $\\ell_2$ regularization, dropout, early stopping and batch normalization. These techniques could help avoid over-fitting or under-fitting models. \n",
    "\n",
    "And in section 5, we apply these methods to the example of tax non-compliant firms classfication.\n",
    "\n",
    "**Learning Goal**:\n",
    "1. Implementing regularization methods including L2 penalty, dropout, early stopping;\n",
    "2. Implementing batch normalization;\n",
    "3. Tunning hyperparameters.\n",
    "\n",
    "## Contents\n",
    "* [0 - Import Packages and Data](#0)\n",
    "* [1 - $\\ell_2$ Regularization](#1)\n",
    "* [2 - Dropout](#2)\n",
    "* [3 - Early Stopping](#3)\n",
    "* [4 - Batch Normalization](#4)\n",
    "* [5 - Tunning](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac1f17",
   "metadata": {},
   "source": [
    "## 0 - Import Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8bca88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15e852923d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # used for data import\n",
    "import numpy as np # used for numerical operations\n",
    "import torch # used for tensor operations\n",
    "import torch.nn as nn # used for building neural networks\n",
    "from torch.utils.data import DataLoader, TensorDataset # used for creating data loaders\n",
    "from sklearn.preprocessing import StandardScaler # used for standardizing features\n",
    "from sklearn.metrics import average_precision_score # used for evaluating models\n",
    "import matplotlib.pyplot as plt # used for plotting PR curves\n",
    "from sklearn.model_selection import train_test_split # used for splitting data into train/valiadtion sets\n",
    "\n",
    "from utils import FNN, train, test # copy and paste FNN, train, test functions in last chapter to utils.py in your current working directory\n",
    "import itertools\n",
    "import json # used for write output to txt\n",
    "\n",
    "np.random.seed(42) # set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee4766",
   "metadata": {},
   "source": [
    "Importing data is the same as what we do in last chapter. The only difference is we further split `train_df` into training data and validation data. Since validation data is used to evaluate the model performance and to choose hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef660cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../全连接神经网络/train_data.csv') # import training data\n",
    "test_df = pd.read_csv('../全连接神经网络/test_data.csv')\n",
    "\n",
    "train_df, valid_df = train_test_split(train_df, \n",
    "                                      stratify=train_df['noncompliance'], \n",
    "                                      test_size=0.2, \n",
    "                                      random_state=42,\n",
    "                                      shuffle=True)\n",
    "\n",
    "# standardize the input\n",
    "scaler = StandardScaler()\n",
    "X_train_array = scaler.fit_transform(train_df.drop(['noncompliance'], axis=1))\n",
    "X_valid_array = scaler.transform(valid_df.drop(['noncompliance'], axis=1))\n",
    "X_test_array = scaler.transform(test_df.drop(['noncompliance'], axis=1))\n",
    "y_train_array = train_df['noncompliance'].values\n",
    "y_valid_array = valid_df['noncompliance'].values\n",
    "y_test_array = test_df['noncompliance'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b708de15",
   "metadata": {},
   "source": [
    "For simplicity, we define `prepare_loader()` function used to transform our array like data into DataLoader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5934df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loader(X_array, y_array, batch_size, shuffle):\n",
    "    \"\"\"\n",
    "    transform array data into DataLoader\n",
    "    params:\n",
    "        X_array: ndarray, feature;\n",
    "        y_array: ndarray, label;\n",
    "        batch_size: int, batch size;\n",
    "        shuffle: bool, change the order of samples;\n",
    "    return:\n",
    "        loader: DataLoader, data including features and labels used in pytorch.\n",
    "    \"\"\"\n",
    "    X_tensor = torch.tensor(X_array, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_array, dtype=torch.float32)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader\n",
    "\n",
    "# prepare training, validation and test data\n",
    "train_loader = prepare_loader(X_train_array, y_train_array, batch_size=128, shuffle=True)\n",
    "valid_loader = prepare_loader(X_valid_array, y_valid_array, batch_size=128, shuffle=False)\n",
    "test_loader = prepare_loader(X_test_array, y_test_array, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86428628",
   "metadata": {},
   "source": [
    "In the following parts, we would talk about the implementation of $\\ell_2$ regularization, dropout, early stopping and batch normalization respectively. Then, we would use these methods during tunning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848163b8",
   "metadata": {},
   "source": [
    "## 1 - $\\ell_2$ Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322b767",
   "metadata": {},
   "source": [
    "Recall the backpropagation of $\\ell_2$ penalty:\n",
    "$J_2 = \\frac{\\lambda}{2n} \\sum_{l} ||W^{[l]}||_{F}^2$ \n",
    "and the derivative is \n",
    "$\\frac{\\partial J_2}{\\partial W^{[l]}} = \\frac{\\lambda}{n} W^{[l]}.$\n",
    "The update step for $\\ell_2$ penalty is \n",
    "$$\n",
    "W^{[l] (t+1)} = W^{[l] (t)} - \\text{lr} \\times (\\text{grad}_{W^{[l]}} + \\lambda \\times W^{[l] (t)}).\n",
    "$$\n",
    "\n",
    "**In the optimizer without momentum** (e.g. `torch.optim.SGD`), we can implement $\\ell_2$ regularization by adding $\\lambda \\times W$ to the gradients, instead of actually modifying the loss function. This can be done by setting the param `weight_decay`, which allows the optimizer to update parameters in the following way: \n",
    "$$\n",
    "\\text{param} = \\text{param} - \\text{lr} \\times (\\text{grad} + \\text{weight decay} * \\text{param})\n",
    "$$\n",
    "\n",
    "**In the optimizer with momentum** (e.g. `torch.optim.Adam`), `weight_decay` and $\\ell_2$ regularization are different, see [Loshchilov and Hutter (2019)](https://arxiv.org/pdf/1711.05101) for more details. The solution is to use `torch.optim.AdamW` optimizer. \n",
    "\n",
    "Another notification is that `weight_decay` would apply on all the model parameters including bias. So, the following function classifies all the parameters into decay group and no decay group, then return a list that can be directly passed to `torch.optim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099d7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_groups(model, weight_decay=3e-3):\n",
    "    \"\"\"\n",
    "    group parameters that need weight_decay and no weight_decay\n",
    "    params:\n",
    "        model: nn.Module \n",
    "        weight_decay: float, weight_decay for weights parameters\n",
    "    return:\n",
    "        param_groups: list, parameter groups to optimizer\n",
    "    \"\"\"\n",
    "    decay, no_decay = [], []\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' in name or 'bn' in name: # exclude bias and batch norm parameters from weight decay\n",
    "            no_decay.append(param)\n",
    "        else: # all other parameters (weights) will have weight decay\n",
    "            decay.append(param)\n",
    "\n",
    "    param_groups = [\n",
    "        {'params': decay, 'weight_decay': weight_decay}, # set weight_decay for 'decay group'\n",
    "        {'params': no_decay, 'weight_decay': 0.0} # set no weigth_decay for 'no decay group'\n",
    "    ]\n",
    "    return param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63fc7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(get_param_groups(model, weight_decay=3e-3), lr=3e-4)\n",
    "train(model, train_loader, criterion, optimizer, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm_all_weights(model):\n",
    "    total_norm = 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name and param.requires_grad:\n",
    "            total_norm += torch.norm(param, p=2) ** 2\n",
    "    return total_norm.sqrt()\n",
    "\n",
    "l2_norm_all_weights(model) # check the l2 norm of all weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad6e345",
   "metadata": {},
   "source": [
    "## 2 - Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7943ec",
   "metadata": {},
   "source": [
    "We can implement dropout by modifying network structure. In the following codes, we define `FNN_dropout` a network with dropout layer.\n",
    "\n",
    "In the `__init__` method, `self.dropout1 = nn.Dropout(p=0.5)` defines a dropout layer with dropout probability of 0.5, it would randomly zeroes some elements of the input with probability $p=0.5$ during training mode.\n",
    "\n",
    "In the `forward` method, `x = self.dropout1(x)` applies the dropout operator to the output of first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4656ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN_dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connected neural network with dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FNN_dropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(24, 64)\n",
    "        self.dropout1 = nn.Dropout(p=0.5) # define dropout layer with 50% probability\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.dropout2 = nn.Dropout(p=0.3) # define dropout layer with 30% probability\n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "        self.dropout3 = nn.Dropout(p=0.1) # define dropout layer with 10% probability\n",
    "        self.fc4 = nn.Linear(8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x) # apply dropout\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x) # apply dropout\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x) # apply dropout\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout = FNN_dropout()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_dropout.parameters(), lr=3e-4)\n",
    "train(model_dropout, train_loader, criterion, optimizer, num_epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b450c97",
   "metadata": {},
   "source": [
    "## 3 - Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637892a4",
   "metadata": {},
   "source": [
    "Early Stopping technique monitors a model's metric (e.g. loss, auc) on a validation set and stops training when the metric starts to increase. The implementation of early stopping is easy, we start from defining a `EarlyStopping` class. This `EarlyStopping` class is a utility used for detect improment, save the best model and stop training if no improvement.\n",
    "\n",
    "How it works:\n",
    "- `__call__` method:\n",
    "  1. On first call, records the initial validation loss and saves the model\n",
    "  2. When improvement is detected (metric decreases by at least `delta`): Updates `best_metric` -> Saves the model -> Resets the patience counter\n",
    "  3. When no improvement: Increments the `counter` -> Triggers stop if `patience` is exceeded\n",
    "- `save_checkpoint` method:\n",
    "  1. Saves the model's state dictionary to the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8bad27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    '''\n",
    "    Early stops training if metric doesn't imporve after patience (expect lower metric are better)\n",
    "    '''\n",
    "    def __init__(self, patience=50, delta=0, path='checkpoint.pt', verbose=False):\n",
    "        self.patience = patience # number of epochs to wait since no improvement\n",
    "        self.delta = delta # score reduction required to detect siginificant improvement\n",
    "        self.counter = 0 # counter of patience\n",
    "        self.stop = False # flag for stop training\n",
    "        self.best_metric = None\n",
    "        # self.verbose = verbose\n",
    "        self.path = path # path to save the check point\n",
    "    \n",
    "    def __call__(self, valid_metric, model):\n",
    "        if self.best_metric is None:\n",
    "            # first call\n",
    "            self.best_metric = valid_metric\n",
    "            self.save_checkpoint(valid_metric, model)\n",
    "        elif valid_metric < self.best_metric - self.delta:\n",
    "            # significant improvement detect\n",
    "            self.best_metric = valid_metric # update best loss\n",
    "            self.save_checkpoint(valid_metric, model) # save the model\n",
    "            self.counter = 0 # resets the patience counter\n",
    "        else:\n",
    "            # no significant improvement\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.stop = True\n",
    "\n",
    "    def save_checkpoint(self, valid_metric, model):\n",
    "        '''save models when metric decreases'''\n",
    "        # if self.verbose:\n",
    "        #     print(f'loss improvement: {valid_metric:.6f}. Saving model.')\n",
    "        torch.save(model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820c342f",
   "metadata": {},
   "source": [
    "We can implement early stopping by modifing previous `train` function in the following way:\n",
    "1. instanciate a `EarlyStopping` class\n",
    "2. loop until `num_epochs` or until early stopping triggers\n",
    "   1. train the model (same as last chapter)\n",
    "   2. get validation loss and probability using `test()` function\n",
    "   3. call `early_stopping` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c4e7274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, valid_loader, criterion, optimizer, num_epochs, early_stop='loss'):\n",
    "    '''\n",
    "    train the model and early stopping on the validation loss\n",
    "    params:\n",
    "        model: the neural network model to train\n",
    "        train_loader: DataLoader, training data;\n",
    "        valid_loader: DataLoader, validation data;\n",
    "        criterion: loss function;\n",
    "        optimizer: optimization algorithm\n",
    "        num_epochs: int, number of epochs\n",
    "        early_stop: early stopping strategy:\n",
    "                    - False: No early stopping\n",
    "                    - 'loss': monitor validation loss\n",
    "                    - 'auprc': monitor validation auprc\n",
    "    return:\n",
    "        model: trained model (best version if early stopping used)\n",
    "        train_losses: list, training losses per epoch\n",
    "        valid_losses: list, validation losses per epoch\n",
    "    '''\n",
    "    train_losses, valid_losses = [], [] # used to store loss of all epochs\n",
    "    early_stopping = EarlyStopping() # initialize an EarlyStopping object\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train the model\n",
    "        model.train()\n",
    "        acc_train_loss = 0 # used to store training loss of all batches\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad() # clear previous gradients\n",
    "            output = model(data) # forward propagation\n",
    "            loss = criterion(output.squeeze(1), target) # calculate average batch loss\n",
    "            loss.backward() # backpropagation\n",
    "            optimizer.step() # update parameters\n",
    "            acc_train_loss += loss.item() * data.size(0) # accumulate loss\n",
    "        train_loss = acc_train_loss / len(train_loader.dataset) # calculate average epoch loss\n",
    "        train_losses.append(train_loss)\n",
    "        if (epoch + 1) % 50 == 0: # print loss each 50 epochs\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # validate the model\n",
    "        valid_loss, valid_probs = test(model, valid_loader, criterion)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_auprc = average_precision_score(y_valid_array, valid_probs)\n",
    "\n",
    "        # early stopping\n",
    "        if early_stop:\n",
    "            if early_stop == 'loss':\n",
    "                early_stopping(valid_loss, model)\n",
    "            elif early_stop == 'auprc':\n",
    "                early_stopping(-valid_auprc, model)\n",
    "            if early_stopping.stop:\n",
    "                print(f'early stopping in epoch {epoch}.')\n",
    "                break\n",
    "            model.load_state_dict(torch.load('checkpoint.pt', weights_only=True)) # load the best model\n",
    "    return model, train_losses, valid_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d7d92",
   "metadata": {},
   "source": [
    "In the following codes, we train the model with early stopping. The training procedures stops at epoch 142, and epoch 92 has the minimum validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30271b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "model, train_losses, valid_losses = train_and_validate(model, train_loader, valid_loader, criterion, optimizer, 500, early_stop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c48748",
   "metadata": {},
   "source": [
    "We can plot the training loss and validation loss of all epochs. The training losses keep decreasing, but the validation loss begin increasing after epoch 92."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_losses)),train_losses, label='Training Loss')\n",
    "plt.plot(range(len(valid_losses)),valid_losses,label='Validation Loss')\n",
    "plt.axvline(valid_losses.index(min(valid_losses)), linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.xlim(0, len(train_losses))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b0c001",
   "metadata": {},
   "source": [
    "## 4 - Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5ea1e3",
   "metadata": {},
   "source": [
    "Batch normalization normalizes the inputs to each layer by re-centering them around zero and scaling them to a standard size. We can implement batch normalization by modifying the network structure.\n",
    "\n",
    "In the `__init__` method, `self.bn1 = nn.BatchNorm1d(64)` defines a batch normalization over a 2D input with 64 features.\n",
    "\n",
    "In the `forward` method, `x = torch.relu(self.bn1(self.fc1(x)))` firstly do the linear transformation, then apply batch normalization, and then do relu activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f30fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN_bn(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connnected neural network with batch normalization\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FNN_bn, self).__init__()\n",
    "        self.fc1 = nn.Linear(24, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64) # defines batch normalization over a 2D input with 64 features\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32) # defines batch normalization over a 2D input with 32 features\n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "        self.bn3 = nn.BatchNorm1d(8) # defines batch normalization over a 2D input with 8 features\n",
    "        self.fc4 = nn.Linear(8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bn = FNN_bn()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_bn.parameters(), lr=3e-4)\n",
    "train(model_bn, train_loader, criterion, optimizer, num_epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e01c181",
   "metadata": {},
   "source": [
    "## 5 - Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a819173",
   "metadata": {},
   "source": [
    "There are many third-party libraries that can help you automatically tunning hyperparameters. Here, we tune by hands.\n",
    "\n",
    "Firstly, we define a parameters grid containing all the possible values of hyperparameters. Since `hidden_layers` and `dropout_rates` must have the same length, after expanding the grid, we need to filter out the combinations with different `hidden_layers` and `dropout_rates` lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00655702",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'batch_size': [256, 128, 64],\n",
    "    'hidden_layers': [[64, 32, 16], [32, 8]],\n",
    "    'dropout_rates': [[0, 0, 0], [0, 0]],\n",
    "    'batch_norm': [True, False],\n",
    "    'criterion': ['BCE'],\n",
    "    'optimizer': ['Adam'],\n",
    "    'lr': [3e-3, 1e-3, 3e-4],\n",
    "    'weight_decay': [0, 1e-4, 3e-4],\n",
    "    'num_epochs': [100, 300, 500],\n",
    "    'early_stop': ['auprc', False],\n",
    "}\n",
    "\n",
    "# expand the grid to all combinations\n",
    "keys, values = zip(*param_grid.items())\n",
    "all_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "# filter out combination that hidden_layers and dropout_rates have different lengths\n",
    "all_combinations = [\n",
    "    combo for combo in all_combinations \n",
    "    if len(combo['hidden_layers']) == len(combo['dropout_rates'])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427712f7",
   "metadata": {},
   "source": [
    "Secondly, we build a flexible neural network `Net`, its architecture depends on the input parameters `input_dim`, `hidden_layers`, `dropout_rates` and `use_batchnorm`.\n",
    "\n",
    "`__init__` method:\n",
    "1. Iterates through each specified hidden layer size\n",
    "2. For each layer, adds:\n",
    "    1. Linear transformation (nn.Linear)\n",
    "    2. Batch normalization (if enabled)\n",
    "    3. ReLU activation\n",
    "    4. Dropout (if `dropout_rates > 0`)\n",
    "3. Final linear layer reduces to 1 output neuron\n",
    "4. Sigmoid activation for binary classification\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ab64af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, dropout_rates, use_batchnorm=False):\n",
    "        '''\n",
    "        params:\n",
    "            input_dim: int, dimension of input features;\n",
    "            hidden_layers: list, number of neurons for each hidden layers;\n",
    "            dropout_rates: list, same length as hidden_layers, dropout probability for each hidden layer;\n",
    "            use_batchnorm: bool, use batch normalization in each layer.\n",
    "        '''\n",
    "        super(Net, self).__init__()\n",
    "        layers = [] # store all the layers\n",
    "        prev_dim = input_dim\n",
    "        for i, hidden_dim in enumerate(hidden_layers):\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout_rates[i] > 0:\n",
    "                layers.append(nn.Dropout(dropout_rates[i]))\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8dc5649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(combinations, output_file='output.txt'):\n",
    "    with open(output_file, 'w') as f:\n",
    "        pass\n",
    "\n",
    "    for config in combinations:\n",
    "        print(f'config: {config}')\n",
    "    \n",
    "        # prepare data\n",
    "        train_loader = prepare_loader(X_train_array, y_train_array, batch_size=config['batch_size'], shuffle=True)\n",
    "        valid_loader = prepare_loader(X_valid_array, y_valid_array, batch_size=config['batch_size'], shuffle=False)\n",
    "        \n",
    "        # build model\n",
    "        model = Net(\n",
    "            input_dim=next(iter(train_loader))[0].shape[1],\n",
    "            hidden_layers=config['hidden_layers'],\n",
    "            dropout_rates=config['dropout_rates'],\n",
    "            use_batchnorm=config['batch_norm']\n",
    "        )\n",
    "        \n",
    "        # loss\n",
    "        if config['criterion'] == 'BCE':\n",
    "            criterion = nn.BCELoss()\n",
    "        else:\n",
    "            raise ValueError('Unknown criterion')\n",
    "        \n",
    "        # optimizer\n",
    "        if config['optimizer'] == 'Adam':\n",
    "            optimizer = torch.optim.Adam(get_param_groups(model, weight_decay=config['weight_decay']), lr=config['lr'])\n",
    "        else:\n",
    "            raise ValueError(\"Unknown optimizer\")\n",
    "        \n",
    "        # train & validate\n",
    "        model, _, _ = train_and_validate(model, train_loader, valid_loader, criterion, optimizer, config['num_epochs'], config['early_stop'])\n",
    "        \n",
    "        # save results\n",
    "        valid_loss, pred_probs = test(model, valid_loader, criterion)\n",
    "        config['valid_loss'] = valid_loss\n",
    "        valid_auprc = average_precision_score(y_valid_array, pred_probs)\n",
    "        result = {**config, \"valid_auprc\": valid_auprc}\n",
    "\n",
    "        with open(output_file, 'a') as f:\n",
    "            f.write(json.dumps(result) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce11a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1599\n",
      "early stopping in epoch 59.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1240\n",
      "Epoch [100/100] Loss: 0.1029\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1591\n",
      "Epoch [100/300] Loss: 0.1582\n",
      "early stopping in epoch 100.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1209\n",
      "Epoch [100/300] Loss: 0.1012\n",
      "Epoch [150/300] Loss: 0.0938\n",
      "Epoch [200/300] Loss: 0.0862\n",
      "Epoch [250/300] Loss: 0.0764\n",
      "Epoch [300/300] Loss: 0.0790\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1452\n",
      "early stopping in epoch 72.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1229\n",
      "Epoch [100/500] Loss: 0.1053\n",
      "Epoch [150/500] Loss: 0.0930\n",
      "Epoch [200/500] Loss: 0.0894\n",
      "Epoch [250/500] Loss: 0.0886\n",
      "Epoch [300/500] Loss: 0.0771\n",
      "Epoch [350/500] Loss: 0.0780\n",
      "Epoch [400/500] Loss: 0.0741\n",
      "Epoch [450/500] Loss: 0.0694\n",
      "Epoch [500/500] Loss: 0.0740\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1513\n",
      "Epoch [100/100] Loss: 0.1484\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1263\n",
      "Epoch [100/100] Loss: 0.1103\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1556\n",
      "Epoch [100/300] Loss: 0.1480\n",
      "early stopping in epoch 131.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1267\n",
      "Epoch [100/300] Loss: 0.1174\n",
      "Epoch [150/300] Loss: 0.1015\n",
      "Epoch [200/300] Loss: 0.0950\n",
      "Epoch [250/300] Loss: 0.1005\n",
      "Epoch [300/300] Loss: 0.0869\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1440\n",
      "Epoch [100/500] Loss: 0.1424\n",
      "early stopping in epoch 129.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1331\n",
      "Epoch [100/500] Loss: 0.1087\n",
      "Epoch [150/500] Loss: 0.1006\n",
      "Epoch [200/500] Loss: 0.0963\n",
      "Epoch [250/500] Loss: 0.0910\n",
      "Epoch [300/500] Loss: 0.0909\n",
      "Epoch [350/500] Loss: 0.0871\n",
      "Epoch [400/500] Loss: 0.0828\n",
      "Epoch [450/500] Loss: 0.0872\n",
      "Epoch [500/500] Loss: 0.0846\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1563\n",
      "early stopping in epoch 88.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1378\n",
      "Epoch [100/100] Loss: 0.1270\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1536\n",
      "early stopping in epoch 92.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1373\n",
      "Epoch [100/300] Loss: 0.1197\n",
      "Epoch [150/300] Loss: 0.1158\n",
      "Epoch [200/300] Loss: 0.1136\n",
      "Epoch [250/300] Loss: 0.1081\n",
      "Epoch [300/300] Loss: 0.1076\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1494\n",
      "Epoch [100/500] Loss: 0.1460\n",
      "early stopping in epoch 147.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1369\n",
      "Epoch [100/500] Loss: 0.1219\n",
      "Epoch [150/500] Loss: 0.1174\n",
      "Epoch [200/500] Loss: 0.1108\n",
      "Epoch [250/500] Loss: 0.1096\n",
      "Epoch [300/500] Loss: 0.1111\n",
      "Epoch [350/500] Loss: 0.1096\n",
      "Epoch [400/500] Loss: 0.1044\n",
      "Epoch [450/500] Loss: 0.0995\n",
      "Epoch [500/500] Loss: 0.1030\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1453\n",
      "early stopping in epoch 81.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1272\n",
      "Epoch [100/100] Loss: 0.1129\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1526\n",
      "Epoch [100/300] Loss: 0.1470\n",
      "early stopping in epoch 125.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1260\n",
      "Epoch [100/300] Loss: 0.1050\n",
      "Epoch [150/300] Loss: 0.0994\n",
      "Epoch [200/300] Loss: 0.0889\n",
      "Epoch [250/300] Loss: 0.0924\n",
      "Epoch [300/300] Loss: 0.0793\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1543\n",
      "early stopping in epoch 93.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1250\n",
      "Epoch [100/500] Loss: 0.1078\n",
      "Epoch [150/500] Loss: 0.0990\n",
      "Epoch [200/500] Loss: 0.0936\n",
      "Epoch [250/500] Loss: 0.0884\n",
      "Epoch [300/500] Loss: 0.0854\n",
      "Epoch [350/500] Loss: 0.0797\n",
      "Epoch [400/500] Loss: 0.0801\n",
      "Epoch [450/500] Loss: 0.0762\n",
      "Epoch [500/500] Loss: 0.0796\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1493\n",
      "Epoch [100/100] Loss: 0.1484\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1241\n",
      "Epoch [100/100] Loss: 0.1071\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1563\n",
      "early stopping in epoch 91.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1267\n",
      "Epoch [100/300] Loss: 0.1067\n",
      "Epoch [150/300] Loss: 0.0963\n",
      "Epoch [200/300] Loss: 0.0927\n",
      "Epoch [250/300] Loss: 0.0967\n",
      "Epoch [300/300] Loss: 0.0837\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1526\n",
      "early stopping in epoch 91.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1253\n",
      "Epoch [100/500] Loss: 0.1112\n",
      "Epoch [150/500] Loss: 0.0969\n",
      "Epoch [200/500] Loss: 0.0936\n",
      "Epoch [250/500] Loss: 0.0876\n",
      "Epoch [300/500] Loss: 0.0802\n",
      "Epoch [350/500] Loss: 0.0831\n",
      "Epoch [400/500] Loss: 0.0768\n",
      "Epoch [450/500] Loss: 0.0829\n",
      "Epoch [500/500] Loss: 0.0706\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1536\n",
      "Epoch [100/100] Loss: 0.1552\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1297\n",
      "Epoch [100/100] Loss: 0.1100\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1421\n",
      "early stopping in epoch 96.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1238\n",
      "Epoch [100/300] Loss: 0.1074\n",
      "Epoch [150/300] Loss: 0.1087\n",
      "Epoch [200/300] Loss: 0.0988\n",
      "Epoch [250/300] Loss: 0.0966\n",
      "Epoch [300/300] Loss: 0.0940\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1510\n",
      "Epoch [100/500] Loss: 0.1481\n",
      "Epoch [150/500] Loss: 0.1463\n",
      "early stopping in epoch 154.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1258\n",
      "Epoch [100/500] Loss: 0.1086\n",
      "Epoch [150/500] Loss: 0.0994\n",
      "Epoch [200/500] Loss: 0.0970\n",
      "Epoch [250/500] Loss: 0.0948\n",
      "Epoch [300/500] Loss: 0.0874\n",
      "Epoch [350/500] Loss: 0.0916\n",
      "Epoch [400/500] Loss: 0.0878\n",
      "Epoch [450/500] Loss: 0.0793\n",
      "Epoch [500/500] Loss: 0.0909\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1572\n",
      "Epoch [100/100] Loss: 0.1524\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1382\n",
      "Epoch [100/100] Loss: 0.1217\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1590\n",
      "Epoch [100/300] Loss: 0.1560\n",
      "Epoch [150/300] Loss: 0.1523\n",
      "Epoch [200/300] Loss: 0.1504\n",
      "Epoch [250/300] Loss: 0.1510\n",
      "Epoch [300/300] Loss: 0.1490\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1430\n",
      "Epoch [100/300] Loss: 0.1242\n",
      "Epoch [150/300] Loss: 0.1145\n",
      "Epoch [200/300] Loss: 0.1100\n",
      "Epoch [250/300] Loss: 0.1025\n",
      "Epoch [300/300] Loss: 0.0972\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1569\n",
      "Epoch [100/500] Loss: 0.1545\n",
      "early stopping in epoch 117.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1468\n",
      "Epoch [100/500] Loss: 0.1287\n",
      "Epoch [150/500] Loss: 0.1157\n",
      "Epoch [200/500] Loss: 0.1108\n",
      "Epoch [250/500] Loss: 0.1024\n",
      "Epoch [300/500] Loss: 0.1019\n",
      "Epoch [350/500] Loss: 0.0936\n",
      "Epoch [400/500] Loss: 0.0925\n",
      "Epoch [450/500] Loss: 0.0925\n",
      "Epoch [500/500] Loss: 0.0898\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1544\n",
      "Epoch [100/100] Loss: 0.1492\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1402\n",
      "Epoch [100/100] Loss: 0.1226\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1601\n",
      "Epoch [100/300] Loss: 0.1535\n",
      "Epoch [150/300] Loss: 0.1514\n",
      "Epoch [200/300] Loss: 0.1498\n",
      "Epoch [250/300] Loss: 0.1491\n",
      "early stopping in epoch 270.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1400\n",
      "Epoch [100/300] Loss: 0.1224\n",
      "Epoch [150/300] Loss: 0.1132\n",
      "Epoch [200/300] Loss: 0.1069\n",
      "Epoch [250/300] Loss: 0.1004\n",
      "Epoch [300/300] Loss: 0.0948\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.2118\n",
      "early stopping in epoch 82.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1380\n",
      "Epoch [100/500] Loss: 0.1210\n",
      "Epoch [150/500] Loss: 0.1101\n",
      "Epoch [200/500] Loss: 0.1025\n",
      "Epoch [250/500] Loss: 0.0983\n",
      "Epoch [300/500] Loss: 0.0966\n",
      "Epoch [350/500] Loss: 0.0917\n",
      "Epoch [400/500] Loss: 0.0849\n",
      "Epoch [450/500] Loss: 0.0821\n",
      "Epoch [500/500] Loss: 0.0823\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1596\n",
      "Epoch [100/100] Loss: 0.1569\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1399\n",
      "Epoch [100/100] Loss: 0.1233\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.7688\n",
      "early stopping in epoch 50.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1383\n",
      "Epoch [100/300] Loss: 0.1209\n",
      "Epoch [150/300] Loss: 0.1098\n",
      "Epoch [200/300] Loss: 0.1028\n",
      "Epoch [250/300] Loss: 0.0952\n",
      "Epoch [300/300] Loss: 0.0924\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1706\n",
      "early stopping in epoch 68.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1395\n",
      "Epoch [100/500] Loss: 0.1221\n",
      "Epoch [150/500] Loss: 0.1095\n",
      "Epoch [200/500] Loss: 0.1045\n",
      "Epoch [250/500] Loss: 0.0975\n",
      "Epoch [300/500] Loss: 0.0928\n",
      "Epoch [350/500] Loss: 0.0897\n",
      "Epoch [400/500] Loss: 0.0886\n",
      "Epoch [450/500] Loss: 0.0838\n",
      "Epoch [500/500] Loss: 0.0812\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1593\n",
      "early stopping in epoch 76.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1222\n",
      "Epoch [100/100] Loss: 0.0919\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1475\n",
      "Epoch [100/300] Loss: 0.1427\n",
      "early stopping in epoch 119.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1253\n",
      "Epoch [100/300] Loss: 0.0900\n",
      "Epoch [150/300] Loss: 0.0676\n",
      "Epoch [200/300] Loss: 0.0600\n",
      "Epoch [250/300] Loss: 0.0610\n",
      "Epoch [300/300] Loss: 0.0408\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1552\n",
      "Epoch [100/500] Loss: 0.1493\n",
      "Epoch [150/500] Loss: 0.1442\n",
      "Epoch [200/500] Loss: 0.1428\n",
      "early stopping in epoch 227.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1195\n",
      "Epoch [100/500] Loss: 0.0917\n",
      "Epoch [150/500] Loss: 0.0704\n",
      "Epoch [200/500] Loss: 0.0553\n",
      "Epoch [250/500] Loss: 0.0568\n",
      "Epoch [300/500] Loss: 0.0532\n",
      "Epoch [350/500] Loss: 0.0567\n",
      "Epoch [400/500] Loss: 0.0446\n",
      "Epoch [450/500] Loss: 0.0556\n",
      "Epoch [500/500] Loss: 0.0404\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1625\n",
      "Epoch [100/100] Loss: 0.1559\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1346\n",
      "Epoch [100/100] Loss: 0.1039\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1537\n",
      "Epoch [100/300] Loss: 0.1492\n",
      "Epoch [150/300] Loss: 0.1450\n",
      "early stopping in epoch 195.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1411\n",
      "Epoch [100/300] Loss: 0.1157\n",
      "Epoch [150/300] Loss: 0.0985\n",
      "Epoch [200/300] Loss: 0.0871\n",
      "Epoch [250/300] Loss: 0.0767\n",
      "Epoch [300/300] Loss: 0.0678\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1608\n",
      "Epoch [100/500] Loss: 0.1576\n",
      "early stopping in epoch 116.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1349\n",
      "Epoch [100/500] Loss: 0.1130\n",
      "Epoch [150/500] Loss: 0.1138\n",
      "Epoch [200/500] Loss: 0.1002\n",
      "Epoch [250/500] Loss: 0.0892\n",
      "Epoch [300/500] Loss: 0.0698\n",
      "Epoch [350/500] Loss: 0.0680\n",
      "Epoch [400/500] Loss: 0.0597\n",
      "Epoch [450/500] Loss: 0.0563\n",
      "Epoch [500/500] Loss: 0.0574\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1670\n",
      "early stopping in epoch 65.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1478\n",
      "Epoch [100/100] Loss: 0.1321\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1616\n",
      "Epoch [100/300] Loss: 0.1598\n",
      "Epoch [150/300] Loss: 0.1599\n",
      "early stopping in epoch 164.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1500\n",
      "Epoch [100/300] Loss: 0.1312\n",
      "Epoch [150/300] Loss: 0.1200\n",
      "Epoch [200/300] Loss: 0.1114\n",
      "Epoch [250/300] Loss: 0.1133\n",
      "Epoch [300/300] Loss: 0.1044\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1569\n",
      "Epoch [100/500] Loss: 0.1551\n",
      "early stopping in epoch 130.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1441\n",
      "Epoch [100/500] Loss: 0.1287\n",
      "Epoch [150/500] Loss: 0.1203\n",
      "Epoch [200/500] Loss: 0.1112\n",
      "Epoch [250/500] Loss: 0.1073\n",
      "Epoch [300/500] Loss: 0.1206\n",
      "Epoch [350/500] Loss: 0.1063\n",
      "Epoch [400/500] Loss: 0.1011\n",
      "Epoch [450/500] Loss: 0.1071\n",
      "Epoch [500/500] Loss: 0.1020\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1535\n",
      "Epoch [100/100] Loss: 0.1459\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1306\n",
      "Epoch [100/100] Loss: 0.1076\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1579\n",
      "early stopping in epoch 90.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1344\n",
      "Epoch [100/300] Loss: 0.1099\n",
      "Epoch [150/300] Loss: 0.0986\n",
      "Epoch [200/300] Loss: 0.0857\n",
      "Epoch [250/300] Loss: 0.0778\n",
      "Epoch [300/300] Loss: 0.0753\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1601\n",
      "Epoch [100/500] Loss: 0.1587\n",
      "Epoch [150/500] Loss: 0.1559\n",
      "early stopping in epoch 193.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1333\n",
      "Epoch [100/500] Loss: 0.1101\n",
      "Epoch [150/500] Loss: 0.0958\n",
      "Epoch [200/500] Loss: 0.0852\n",
      "Epoch [250/500] Loss: 0.0769\n",
      "Epoch [300/500] Loss: 0.0681\n",
      "Epoch [350/500] Loss: 0.0584\n",
      "Epoch [400/500] Loss: 0.0527\n",
      "Epoch [450/500] Loss: 0.0554\n",
      "Epoch [500/500] Loss: 0.0468\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1559\n",
      "Epoch [100/100] Loss: 0.1540\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1385\n",
      "Epoch [100/100] Loss: 0.1215\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1639\n",
      "early stopping in epoch 80.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1368\n",
      "Epoch [100/300] Loss: 0.1163\n",
      "Epoch [150/300] Loss: 0.1048\n",
      "Epoch [200/300] Loss: 0.0964\n",
      "Epoch [250/300] Loss: 0.0831\n",
      "Epoch [300/300] Loss: 0.0778\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1529\n",
      "Epoch [100/500] Loss: 0.1503\n",
      "Epoch [150/500] Loss: 0.1502\n",
      "early stopping in epoch 151.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1350\n",
      "Epoch [100/500] Loss: 0.1145\n",
      "Epoch [150/500] Loss: 0.1056\n",
      "Epoch [200/500] Loss: 0.0969\n",
      "Epoch [250/500] Loss: 0.0864\n",
      "Epoch [300/500] Loss: 0.0827\n",
      "Epoch [350/500] Loss: 0.0769\n",
      "Epoch [400/500] Loss: 0.0766\n",
      "Epoch [450/500] Loss: 0.0701\n",
      "Epoch [500/500] Loss: 0.0650\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1577\n",
      "Epoch [100/100] Loss: 0.1542\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1450\n",
      "Epoch [100/100] Loss: 0.1278\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1571\n",
      "Epoch [100/300] Loss: 0.1525\n",
      "Epoch [150/300] Loss: 0.1519\n",
      "early stopping in epoch 167.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1496\n",
      "Epoch [100/300] Loss: 0.1315\n",
      "Epoch [150/300] Loss: 0.1218\n",
      "Epoch [200/300] Loss: 0.1156\n",
      "Epoch [250/300] Loss: 0.1146\n",
      "Epoch [300/300] Loss: 0.1094\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1544\n",
      "Epoch [100/500] Loss: 0.1539\n",
      "early stopping in epoch 142.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1437\n",
      "Epoch [100/500] Loss: 0.1306\n",
      "Epoch [150/500] Loss: 0.1209\n",
      "Epoch [200/500] Loss: 0.1140\n",
      "Epoch [250/500] Loss: 0.1065\n",
      "Epoch [300/500] Loss: 0.1033\n",
      "Epoch [350/500] Loss: 0.1010\n",
      "Epoch [400/500] Loss: 0.0946\n",
      "Epoch [450/500] Loss: 0.0942\n",
      "Epoch [500/500] Loss: 0.0877\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1658\n",
      "Epoch [100/100] Loss: 0.1575\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1563\n",
      "Epoch [100/100] Loss: 0.1398\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1604\n",
      "Epoch [100/300] Loss: 0.1543\n",
      "Epoch [150/300] Loss: 0.1504\n",
      "Epoch [200/300] Loss: 0.1481\n",
      "Epoch [250/300] Loss: 0.1454\n",
      "Epoch [300/300] Loss: 0.1448\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1558\n",
      "Epoch [100/300] Loss: 0.1402\n",
      "Epoch [150/300] Loss: 0.1303\n",
      "Epoch [200/300] Loss: 0.1222\n",
      "Epoch [250/300] Loss: 0.1170\n",
      "Epoch [300/300] Loss: 0.1096\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1638\n",
      "Epoch [100/500] Loss: 0.1606\n",
      "Epoch [150/500] Loss: 0.1554\n",
      "Epoch [200/500] Loss: 0.1549\n",
      "early stopping in epoch 226.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1596\n",
      "Epoch [100/500] Loss: 0.1462\n",
      "Epoch [150/500] Loss: 0.1354\n",
      "Epoch [200/500] Loss: 0.1260\n",
      "Epoch [250/500] Loss: 0.1181\n",
      "Epoch [300/500] Loss: 0.1123\n",
      "Epoch [350/500] Loss: 0.1063\n",
      "Epoch [400/500] Loss: 0.1014\n",
      "Epoch [450/500] Loss: 0.0996\n",
      "Epoch [500/500] Loss: 0.0938\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1664\n",
      "early stopping in epoch 74.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1588\n",
      "Epoch [100/100] Loss: 0.1455\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1647\n",
      "Epoch [100/300] Loss: 0.1624\n",
      "early stopping in epoch 145.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1567\n",
      "Epoch [100/300] Loss: 0.1427\n",
      "Epoch [150/300] Loss: 0.1324\n",
      "Epoch [200/300] Loss: 0.1241\n",
      "Epoch [250/300] Loss: 0.1187\n",
      "Epoch [300/300] Loss: 0.1132\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1661\n",
      "early stopping in epoch 79.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1573\n",
      "Epoch [100/500] Loss: 0.1413\n",
      "Epoch [150/500] Loss: 0.1305\n",
      "Epoch [200/500] Loss: 0.1214\n",
      "Epoch [250/500] Loss: 0.1148\n",
      "Epoch [300/500] Loss: 0.1102\n",
      "Epoch [350/500] Loss: 0.1051\n",
      "Epoch [400/500] Loss: 0.1023\n",
      "Epoch [450/500] Loss: 0.0977\n",
      "Epoch [500/500] Loss: 0.0958\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1629\n",
      "Epoch [100/100] Loss: 0.1586\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1563\n",
      "Epoch [100/100] Loss: 0.1440\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1642\n",
      "Epoch [100/300] Loss: 0.1605\n",
      "Epoch [150/300] Loss: 0.1596\n",
      "Epoch [200/300] Loss: 0.1586\n",
      "Epoch [250/300] Loss: 0.1583\n",
      "Epoch [300/300] Loss: 0.1585\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1576\n",
      "Epoch [100/300] Loss: 0.1452\n",
      "Epoch [150/300] Loss: 0.1368\n",
      "Epoch [200/300] Loss: 0.1300\n",
      "Epoch [250/300] Loss: 0.1247\n",
      "Epoch [300/300] Loss: 0.1203\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1614\n",
      "Epoch [100/500] Loss: 0.1594\n",
      "Epoch [150/500] Loss: 0.1586\n",
      "early stopping in epoch 166.\n",
      "config: {'batch_size': 256, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1584\n",
      "Epoch [100/500] Loss: 0.1450\n",
      "Epoch [150/500] Loss: 0.1345\n",
      "Epoch [200/500] Loss: 0.1255\n",
      "Epoch [250/500] Loss: 0.1200\n",
      "Epoch [300/500] Loss: 0.1147\n",
      "Epoch [350/500] Loss: 0.1115\n",
      "Epoch [400/500] Loss: 0.1070\n",
      "Epoch [450/500] Loss: 0.1044\n",
      "Epoch [500/500] Loss: 0.1049\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1553\n",
      "Epoch [100/100] Loss: 0.1547\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1484\n",
      "Epoch [100/100] Loss: 0.1420\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1612\n",
      "early stopping in epoch 67.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1495\n",
      "Epoch [100/300] Loss: 0.1431\n",
      "Epoch [150/300] Loss: 0.1411\n",
      "Epoch [200/300] Loss: 0.1378\n",
      "Epoch [250/300] Loss: 0.1371\n",
      "Epoch [300/300] Loss: 0.1366\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1634\n",
      "Epoch [100/500] Loss: 0.1592\n",
      "Epoch [150/500] Loss: 0.1575\n",
      "early stopping in epoch 150.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1487\n",
      "Epoch [100/500] Loss: 0.1428\n",
      "Epoch [150/500] Loss: 0.1402\n",
      "Epoch [200/500] Loss: 0.1364\n",
      "Epoch [250/500] Loss: 0.1388\n",
      "Epoch [300/500] Loss: 0.1354\n",
      "Epoch [350/500] Loss: 0.1345\n",
      "Epoch [400/500] Loss: 0.1329\n",
      "Epoch [450/500] Loss: 0.1339\n",
      "Epoch [500/500] Loss: 0.1376\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1621\n",
      "Epoch [100/100] Loss: 0.1591\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1483\n",
      "Epoch [100/100] Loss: 0.1438\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1598\n",
      "Epoch [100/300] Loss: 0.1588\n",
      "early stopping in epoch 108.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1464\n",
      "Epoch [100/300] Loss: 0.1419\n",
      "Epoch [150/300] Loss: 0.1393\n",
      "Epoch [200/300] Loss: 0.1370\n",
      "Epoch [250/300] Loss: 0.1361\n",
      "Epoch [300/300] Loss: 0.1392\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1607\n",
      "early stopping in epoch 91.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1480\n",
      "Epoch [100/500] Loss: 0.1417\n",
      "Epoch [150/500] Loss: 0.1408\n",
      "Epoch [200/500] Loss: 0.1367\n",
      "Epoch [250/500] Loss: 0.1343\n",
      "Epoch [300/500] Loss: 0.1345\n",
      "Epoch [350/500] Loss: 0.1322\n",
      "Epoch [400/500] Loss: 0.1353\n",
      "Epoch [450/500] Loss: 0.1310\n",
      "Epoch [500/500] Loss: 0.1321\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1606\n",
      "Epoch [100/100] Loss: 0.1605\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1514\n",
      "Epoch [100/100] Loss: 0.1459\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1600\n",
      "Epoch [100/300] Loss: 0.1571\n",
      "Epoch [150/300] Loss: 0.1565\n",
      "Epoch [200/300] Loss: 0.1556\n",
      "early stopping in epoch 210.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1515\n",
      "Epoch [100/300] Loss: 0.1471\n",
      "Epoch [150/300] Loss: 0.1442\n",
      "Epoch [200/300] Loss: 0.1417\n",
      "Epoch [250/300] Loss: 0.1441\n",
      "Epoch [300/300] Loss: 0.1414\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1622\n",
      "Epoch [100/500] Loss: 0.1608\n",
      "early stopping in epoch 126.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1527\n",
      "Epoch [100/500] Loss: 0.1472\n",
      "Epoch [150/500] Loss: 0.1433\n",
      "Epoch [200/500] Loss: 0.1433\n",
      "Epoch [250/500] Loss: 0.1418\n",
      "Epoch [300/500] Loss: 0.1393\n",
      "Epoch [350/500] Loss: 0.1405\n",
      "Epoch [400/500] Loss: 0.1407\n",
      "Epoch [450/500] Loss: 0.1400\n",
      "Epoch [500/500] Loss: 0.1374\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1598\n",
      "Epoch [100/100] Loss: 0.1591\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1511\n",
      "Epoch [100/100] Loss: 0.1446\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1586\n",
      "Epoch [100/300] Loss: 0.1577\n",
      "early stopping in epoch 133.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1521\n",
      "Epoch [100/300] Loss: 0.1433\n",
      "Epoch [150/300] Loss: 0.1398\n",
      "Epoch [200/300] Loss: 0.1367\n",
      "Epoch [250/300] Loss: 0.1375\n",
      "Epoch [300/300] Loss: 0.1358\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1610\n",
      "early stopping in epoch 80.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1517\n",
      "Epoch [100/500] Loss: 0.1467\n",
      "Epoch [150/500] Loss: 0.1409\n",
      "Epoch [200/500] Loss: 0.1394\n",
      "Epoch [250/500] Loss: 0.1372\n",
      "Epoch [300/500] Loss: 0.1345\n",
      "Epoch [350/500] Loss: 0.1351\n",
      "Epoch [400/500] Loss: 0.1353\n",
      "Epoch [450/500] Loss: 0.1335\n",
      "Epoch [500/500] Loss: 0.1333\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1634\n",
      "early stopping in epoch 66.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1525\n",
      "Epoch [100/100] Loss: 0.1470\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1605\n",
      "Epoch [100/300] Loss: 0.1566\n",
      "early stopping in epoch 133.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1518\n",
      "Epoch [100/300] Loss: 0.1463\n",
      "Epoch [150/300] Loss: 0.1424\n",
      "Epoch [200/300] Loss: 0.1381\n",
      "Epoch [250/300] Loss: 0.1384\n",
      "Epoch [300/300] Loss: 0.1350\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1565\n",
      "Epoch [100/500] Loss: 0.1557\n",
      "Epoch [150/500] Loss: 0.1555\n",
      "early stopping in epoch 169.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1516\n",
      "Epoch [100/500] Loss: 0.1451\n",
      "Epoch [150/500] Loss: 0.1415\n",
      "Epoch [200/500] Loss: 0.1384\n",
      "Epoch [250/500] Loss: 0.1355\n",
      "Epoch [300/500] Loss: 0.1341\n",
      "Epoch [350/500] Loss: 0.1340\n",
      "Epoch [400/500] Loss: 0.1348\n",
      "Epoch [450/500] Loss: 0.1335\n",
      "Epoch [500/500] Loss: 0.1342\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1620\n",
      "Epoch [100/100] Loss: 0.1618\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1513\n",
      "Epoch [100/100] Loss: 0.1450\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1602\n",
      "Epoch [100/300] Loss: 0.1594\n",
      "Epoch [150/300] Loss: 0.1568\n",
      "Epoch [200/300] Loss: 0.1551\n",
      "early stopping in epoch 227.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1538\n",
      "Epoch [100/300] Loss: 0.1460\n",
      "Epoch [150/300] Loss: 0.1433\n",
      "Epoch [200/300] Loss: 0.1407\n",
      "Epoch [250/300] Loss: 0.1378\n",
      "Epoch [300/300] Loss: 0.1372\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1625\n",
      "Epoch [100/500] Loss: 0.1574\n",
      "Epoch [150/500] Loss: 0.1553\n",
      "Epoch [200/500] Loss: 0.1538\n",
      "Epoch [250/500] Loss: 0.1529\n",
      "early stopping in epoch 252.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1535\n",
      "Epoch [100/500] Loss: 0.1466\n",
      "Epoch [150/500] Loss: 0.1433\n",
      "Epoch [200/500] Loss: 0.1418\n",
      "Epoch [250/500] Loss: 0.1391\n",
      "Epoch [300/500] Loss: 0.1368\n",
      "Epoch [350/500] Loss: 0.1393\n",
      "Epoch [400/500] Loss: 0.1354\n",
      "Epoch [450/500] Loss: 0.1354\n",
      "Epoch [500/500] Loss: 0.1360\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1639\n",
      "Epoch [100/100] Loss: 0.1633\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1628\n",
      "Epoch [100/100] Loss: 0.1556\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1888\n",
      "Epoch [100/300] Loss: 0.1672\n",
      "early stopping in epoch 146.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1592\n",
      "Epoch [100/300] Loss: 0.1515\n",
      "Epoch [150/300] Loss: 0.1476\n",
      "Epoch [200/300] Loss: 0.1451\n",
      "Epoch [250/300] Loss: 0.1416\n",
      "Epoch [300/300] Loss: 0.1406\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.4127\n",
      "early stopping in epoch 53.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1610\n",
      "Epoch [100/500] Loss: 0.1537\n",
      "Epoch [150/500] Loss: 0.1479\n",
      "Epoch [200/500] Loss: 0.1449\n",
      "Epoch [250/500] Loss: 0.1431\n",
      "Epoch [300/500] Loss: 0.1404\n",
      "Epoch [350/500] Loss: 0.1414\n",
      "Epoch [400/500] Loss: 0.1372\n",
      "Epoch [450/500] Loss: 0.1384\n",
      "Epoch [500/500] Loss: 0.1366\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1713\n",
      "Epoch [100/100] Loss: 0.1613\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1588\n",
      "Epoch [100/100] Loss: 0.1519\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.4458\n",
      "early stopping in epoch 62.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1585\n",
      "Epoch [100/300] Loss: 0.1526\n",
      "Epoch [150/300] Loss: 0.1490\n",
      "Epoch [200/300] Loss: 0.1451\n",
      "Epoch [250/300] Loss: 0.1430\n",
      "Epoch [300/300] Loss: 0.1418\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1746\n",
      "Epoch [100/500] Loss: 0.1688\n",
      "Epoch [150/500] Loss: 0.1671\n",
      "Epoch [200/500] Loss: 0.1646\n",
      "Epoch [250/500] Loss: 0.1635\n",
      "early stopping in epoch 277.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1587\n",
      "Epoch [100/500] Loss: 0.1516\n",
      "Epoch [150/500] Loss: 0.1462\n",
      "Epoch [200/500] Loss: 0.1445\n",
      "Epoch [250/500] Loss: 0.1418\n",
      "Epoch [300/500] Loss: 0.1406\n",
      "Epoch [350/500] Loss: 0.1406\n",
      "Epoch [400/500] Loss: 0.1371\n",
      "Epoch [450/500] Loss: 0.1362\n",
      "Epoch [500/500] Loss: 0.1378\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1617\n",
      "Epoch [100/100] Loss: 0.1602\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1618\n",
      "Epoch [100/100] Loss: 0.1542\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1863\n",
      "Epoch [100/300] Loss: 0.1654\n",
      "Epoch [150/300] Loss: 0.1639\n",
      "early stopping in epoch 171.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1573\n",
      "Epoch [100/300] Loss: 0.1521\n",
      "Epoch [150/300] Loss: 0.1494\n",
      "Epoch [200/300] Loss: 0.1469\n",
      "Epoch [250/300] Loss: 0.1447\n",
      "Epoch [300/300] Loss: 0.1428\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.5461\n",
      "early stopping in epoch 50.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1602\n",
      "Epoch [100/500] Loss: 0.1530\n",
      "Epoch [150/500] Loss: 0.1483\n",
      "Epoch [200/500] Loss: 0.1451\n",
      "Epoch [250/500] Loss: 0.1430\n",
      "Epoch [300/500] Loss: 0.1407\n",
      "Epoch [350/500] Loss: 0.1401\n",
      "Epoch [400/500] Loss: 0.1400\n",
      "Epoch [450/500] Loss: 0.1380\n",
      "Epoch [500/500] Loss: 0.1382\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1649\n",
      "Epoch [100/100] Loss: 0.1606\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1450\n",
      "Epoch [100/100] Loss: 0.1390\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1634\n",
      "Epoch [100/300] Loss: 0.1585\n",
      "Epoch [150/300] Loss: 0.1542\n",
      "early stopping in epoch 179.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1464\n",
      "Epoch [100/300] Loss: 0.1364\n",
      "Epoch [150/300] Loss: 0.1315\n",
      "Epoch [200/300] Loss: 0.1296\n",
      "Epoch [250/300] Loss: 0.1273\n",
      "Epoch [300/300] Loss: 0.1339\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1585\n",
      "Epoch [100/500] Loss: 0.1579\n",
      "Epoch [150/500] Loss: 0.1570\n",
      "early stopping in epoch 184.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1450\n",
      "Epoch [100/500] Loss: 0.1348\n",
      "Epoch [150/500] Loss: 0.1318\n",
      "Epoch [200/500] Loss: 0.1300\n",
      "Epoch [250/500] Loss: 0.1275\n",
      "Epoch [300/500] Loss: 0.1208\n",
      "Epoch [350/500] Loss: 0.1198\n",
      "Epoch [400/500] Loss: 0.1295\n",
      "Epoch [450/500] Loss: 0.1233\n",
      "Epoch [500/500] Loss: 0.1224\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1648\n",
      "Epoch [100/100] Loss: 0.1631\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1496\n",
      "Epoch [100/100] Loss: 0.1393\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1615\n",
      "early stopping in epoch 79.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1478\n",
      "Epoch [100/300] Loss: 0.1396\n",
      "Epoch [150/300] Loss: 0.1383\n",
      "Epoch [200/300] Loss: 0.1357\n",
      "Epoch [250/300] Loss: 0.1356\n",
      "Epoch [300/300] Loss: 0.1324\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1625\n",
      "Epoch [100/500] Loss: 0.1615\n",
      "early stopping in epoch 129.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1542\n",
      "Epoch [100/500] Loss: 0.1458\n",
      "Epoch [150/500] Loss: 0.1420\n",
      "Epoch [200/500] Loss: 0.1385\n",
      "Epoch [250/500] Loss: 0.1356\n",
      "Epoch [300/500] Loss: 0.1331\n",
      "Epoch [350/500] Loss: 0.1336\n",
      "Epoch [400/500] Loss: 0.1304\n",
      "Epoch [450/500] Loss: 0.1312\n",
      "Epoch [500/500] Loss: 0.1290\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1692\n",
      "Epoch [100/100] Loss: 0.1628\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1523\n",
      "Epoch [100/100] Loss: 0.1447\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1651\n",
      "Epoch [100/300] Loss: 0.1638\n",
      "early stopping in epoch 107.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1529\n",
      "Epoch [100/300] Loss: 0.1474\n",
      "Epoch [150/300] Loss: 0.1437\n",
      "Epoch [200/300] Loss: 0.1461\n",
      "Epoch [250/300] Loss: 0.1453\n",
      "Epoch [300/300] Loss: 0.1408\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1701\n",
      "early stopping in epoch 87.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1549\n",
      "Epoch [100/500] Loss: 0.1475\n",
      "Epoch [150/500] Loss: 0.1427\n",
      "Epoch [200/500] Loss: 0.1399\n",
      "Epoch [250/500] Loss: 0.1377\n",
      "Epoch [300/500] Loss: 0.1358\n",
      "Epoch [350/500] Loss: 0.1359\n",
      "Epoch [400/500] Loss: 0.1365\n",
      "Epoch [450/500] Loss: 0.1357\n",
      "Epoch [500/500] Loss: 0.1357\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1638\n",
      "Epoch [100/100] Loss: 0.1632\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1546\n",
      "Epoch [100/100] Loss: 0.1456\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1640\n",
      "Epoch [100/300] Loss: 0.1583\n",
      "Epoch [150/300] Loss: 0.1569\n",
      "early stopping in epoch 197.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1599\n",
      "Epoch [100/300] Loss: 0.1529\n",
      "Epoch [150/300] Loss: 0.1480\n",
      "Epoch [200/300] Loss: 0.1437\n",
      "Epoch [250/300] Loss: 0.1407\n",
      "Epoch [300/300] Loss: 0.1383\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1651\n",
      "Epoch [100/500] Loss: 0.1647\n",
      "Epoch [150/500] Loss: 0.1647\n",
      "early stopping in epoch 159.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1595\n",
      "Epoch [100/500] Loss: 0.1521\n",
      "Epoch [150/500] Loss: 0.1469\n",
      "Epoch [200/500] Loss: 0.1439\n",
      "Epoch [250/500] Loss: 0.1416\n",
      "Epoch [300/500] Loss: 0.1398\n",
      "Epoch [350/500] Loss: 0.1378\n",
      "Epoch [400/500] Loss: 0.1385\n",
      "Epoch [450/500] Loss: 0.1356\n",
      "Epoch [500/500] Loss: 0.1361\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1656\n",
      "early stopping in epoch 81.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1578\n",
      "Epoch [100/100] Loss: 0.1490\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1653\n",
      "early stopping in epoch 78.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1569\n",
      "Epoch [100/300] Loss: 0.1466\n",
      "Epoch [150/300] Loss: 0.1418\n",
      "Epoch [200/300] Loss: 0.1392\n",
      "Epoch [250/300] Loss: 0.1366\n",
      "Epoch [300/300] Loss: 0.1351\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1643\n",
      "Epoch [100/500] Loss: 0.1626\n",
      "Epoch [150/500] Loss: 0.1620\n",
      "early stopping in epoch 153.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1578\n",
      "Epoch [100/500] Loss: 0.1509\n",
      "Epoch [150/500] Loss: 0.1462\n",
      "Epoch [200/500] Loss: 0.1436\n",
      "Epoch [250/500] Loss: 0.1416\n",
      "Epoch [300/500] Loss: 0.1403\n",
      "Epoch [350/500] Loss: 0.1392\n",
      "Epoch [400/500] Loss: 0.1383\n",
      "Epoch [450/500] Loss: 0.1365\n",
      "Epoch [500/500] Loss: 0.1357\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1631\n",
      "Epoch [100/100] Loss: 0.1625\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1619\n",
      "Epoch [100/100] Loss: 0.1544\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1633\n",
      "Epoch [100/300] Loss: 0.1623\n",
      "Epoch [150/300] Loss: 0.1613\n",
      "Epoch [200/300] Loss: 0.1605\n",
      "Epoch [250/300] Loss: 0.1603\n",
      "early stopping in epoch 293.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1587\n",
      "Epoch [100/300] Loss: 0.1519\n",
      "Epoch [150/300] Loss: 0.1468\n",
      "Epoch [200/300] Loss: 0.1434\n",
      "Epoch [250/300] Loss: 0.1413\n",
      "Epoch [300/300] Loss: 0.1401\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1645\n",
      "Epoch [100/500] Loss: 0.1625\n",
      "Epoch [150/500] Loss: 0.1616\n",
      "Epoch [200/500] Loss: 0.1595\n",
      "Epoch [250/500] Loss: 0.1596\n",
      "early stopping in epoch 253.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1593\n",
      "Epoch [100/500] Loss: 0.1518\n",
      "Epoch [150/500] Loss: 0.1470\n",
      "Epoch [200/500] Loss: 0.1443\n",
      "Epoch [250/500] Loss: 0.1421\n",
      "Epoch [300/500] Loss: 0.1409\n",
      "Epoch [350/500] Loss: 0.1393\n",
      "Epoch [400/500] Loss: 0.1381\n",
      "Epoch [450/500] Loss: 0.1368\n",
      "Epoch [500/500] Loss: 0.1358\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1688\n",
      "Epoch [100/100] Loss: 0.1667\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1705\n",
      "Epoch [100/100] Loss: 0.1652\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.4267\n",
      "early stopping in epoch 50.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1677\n",
      "Epoch [100/300] Loss: 0.1605\n",
      "Epoch [150/300] Loss: 0.1563\n",
      "Epoch [200/300] Loss: 0.1537\n",
      "Epoch [250/300] Loss: 0.1514\n",
      "Epoch [300/300] Loss: 0.1496\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1686\n",
      "Epoch [100/500] Loss: 0.1655\n",
      "Epoch [150/500] Loss: 0.1651\n",
      "early stopping in epoch 172.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1659\n",
      "Epoch [100/500] Loss: 0.1603\n",
      "Epoch [150/500] Loss: 0.1560\n",
      "Epoch [200/500] Loss: 0.1519\n",
      "Epoch [250/500] Loss: 0.1487\n",
      "Epoch [300/500] Loss: 0.1463\n",
      "Epoch [350/500] Loss: 0.1440\n",
      "Epoch [400/500] Loss: 0.1419\n",
      "Epoch [450/500] Loss: 0.1406\n",
      "Epoch [500/500] Loss: 0.1391\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.4574\n",
      "early stopping in epoch 50.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1667\n",
      "Epoch [100/100] Loss: 0.1623\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.5174\n",
      "early stopping in epoch 50.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1666\n",
      "Epoch [100/300] Loss: 0.1609\n",
      "Epoch [150/300] Loss: 0.1574\n",
      "Epoch [200/300] Loss: 0.1548\n",
      "Epoch [250/300] Loss: 0.1525\n",
      "Epoch [300/300] Loss: 0.1508\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.6656\n",
      "early stopping in epoch 50.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1674\n",
      "Epoch [100/500] Loss: 0.1610\n",
      "Epoch [150/500] Loss: 0.1578\n",
      "Epoch [200/500] Loss: 0.1551\n",
      "Epoch [250/500] Loss: 0.1525\n",
      "Epoch [300/500] Loss: 0.1505\n",
      "Epoch [350/500] Loss: 0.1483\n",
      "Epoch [400/500] Loss: 0.1467\n",
      "Epoch [450/500] Loss: 0.1453\n",
      "Epoch [500/500] Loss: 0.1440\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.3211\n",
      "early stopping in epoch 51.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1675\n",
      "Epoch [100/100] Loss: 0.1619\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.5647\n",
      "early stopping in epoch 50.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1683\n",
      "Epoch [100/300] Loss: 0.1637\n",
      "Epoch [150/300] Loss: 0.1594\n",
      "Epoch [200/300] Loss: 0.1562\n",
      "Epoch [250/300] Loss: 0.1532\n",
      "Epoch [300/300] Loss: 0.1511\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1700\n",
      "Epoch [100/500] Loss: 0.1684\n",
      "Epoch [150/500] Loss: 0.1679\n",
      "Epoch [200/500] Loss: 0.1678\n",
      "Epoch [250/500] Loss: 0.1677\n",
      "early stopping in epoch 280.\n",
      "config: {'batch_size': 256, 'hidden_layers': [32, 8], 'dropout_rates': [0, 0], 'batch_norm': False, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.0003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1676\n",
      "Epoch [100/500] Loss: 0.1624\n",
      "Epoch [150/500] Loss: 0.1588\n",
      "Epoch [200/500] Loss: 0.1559\n",
      "Epoch [250/500] Loss: 0.1531\n",
      "Epoch [300/500] Loss: 0.1507\n",
      "Epoch [350/500] Loss: 0.1483\n",
      "Epoch [400/500] Loss: 0.1467\n",
      "Epoch [450/500] Loss: 0.1450\n",
      "Epoch [500/500] Loss: 0.1440\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1536\n",
      "Epoch [100/100] Loss: 0.1448\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1241\n",
      "Epoch [100/100] Loss: 0.1065\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1555\n",
      "Epoch [100/300] Loss: 0.1518\n",
      "Epoch [150/300] Loss: 0.1440\n",
      "Epoch [200/300] Loss: 0.1428\n",
      "Epoch [250/300] Loss: 0.1424\n",
      "early stopping in epoch 254.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1224\n",
      "Epoch [100/300] Loss: 0.1072\n",
      "Epoch [150/300] Loss: 0.0975\n",
      "Epoch [200/300] Loss: 0.0949\n",
      "Epoch [250/300] Loss: 0.0838\n",
      "Epoch [300/300] Loss: 0.0835\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1555\n",
      "Epoch [100/500] Loss: 0.1514\n",
      "early stopping in epoch 118.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1279\n",
      "Epoch [100/500] Loss: 0.1100\n",
      "Epoch [150/500] Loss: 0.0993\n",
      "Epoch [200/500] Loss: 0.0953\n",
      "Epoch [250/500] Loss: 0.0894\n",
      "Epoch [300/500] Loss: 0.0918\n",
      "Epoch [350/500] Loss: 0.0859\n",
      "Epoch [400/500] Loss: 0.0842\n",
      "Epoch [450/500] Loss: 0.0804\n",
      "Epoch [500/500] Loss: 0.0821\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1600\n",
      "Epoch [100/100] Loss: 0.1551\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1341\n",
      "Epoch [100/100] Loss: 0.1206\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1615\n",
      "Epoch [100/300] Loss: 0.1588\n",
      "Epoch [150/300] Loss: 0.1572\n",
      "early stopping in epoch 154.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1326\n",
      "Epoch [100/300] Loss: 0.1202\n",
      "Epoch [150/300] Loss: 0.1147\n",
      "Epoch [200/300] Loss: 0.1077\n",
      "Epoch [250/300] Loss: 0.1051\n",
      "Epoch [300/300] Loss: 0.1016\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1564\n",
      "Epoch [100/500] Loss: 0.1559\n",
      "Epoch [150/500] Loss: 0.1534\n",
      "early stopping in epoch 171.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1318\n",
      "Epoch [100/500] Loss: 0.1221\n",
      "Epoch [150/500] Loss: 0.1157\n",
      "Epoch [200/500] Loss: 0.1136\n",
      "Epoch [250/500] Loss: 0.1112\n",
      "Epoch [300/500] Loss: 0.1126\n",
      "Epoch [350/500] Loss: 0.1066\n",
      "Epoch [400/500] Loss: 0.1080\n",
      "Epoch [450/500] Loss: 0.1022\n",
      "Epoch [500/500] Loss: 0.1032\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1601\n",
      "early stopping in epoch 68.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1452\n",
      "Epoch [100/100] Loss: 0.1385\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1558\n",
      "Epoch [100/300] Loss: 0.1545\n",
      "early stopping in epoch 111.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1451\n",
      "Epoch [100/300] Loss: 0.1409\n",
      "Epoch [150/300] Loss: 0.1358\n",
      "Epoch [200/300] Loss: 0.1355\n",
      "Epoch [250/300] Loss: 0.1351\n",
      "Epoch [300/300] Loss: 0.1322\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1596\n",
      "Epoch [100/500] Loss: 0.1577\n",
      "early stopping in epoch 138.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.003, 'weight_decay': 0.0003, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1438\n",
      "Epoch [100/500] Loss: 0.1378\n",
      "Epoch [150/500] Loss: 0.1346\n",
      "Epoch [200/500] Loss: 0.1340\n",
      "Epoch [250/500] Loss: 0.1337\n",
      "Epoch [300/500] Loss: 0.1295\n",
      "Epoch [350/500] Loss: 0.1294\n",
      "Epoch [400/500] Loss: 0.1296\n",
      "Epoch [450/500] Loss: 0.1292\n",
      "Epoch [500/500] Loss: 0.1297\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1476\n",
      "Epoch [100/100] Loss: 0.1462\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1262\n",
      "Epoch [100/100] Loss: 0.1137\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1484\n",
      "Epoch [100/300] Loss: 0.1473\n",
      "early stopping in epoch 103.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1262\n",
      "Epoch [100/300] Loss: 0.1109\n",
      "Epoch [150/300] Loss: 0.1043\n",
      "Epoch [200/300] Loss: 0.0971\n",
      "Epoch [250/300] Loss: 0.0912\n",
      "Epoch [300/300] Loss: 0.0902\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1532\n",
      "Epoch [100/500] Loss: 0.1523\n",
      "early stopping in epoch 99.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1269\n",
      "Epoch [100/500] Loss: 0.1114\n",
      "Epoch [150/500] Loss: 0.1035\n",
      "Epoch [200/500] Loss: 0.0964\n",
      "Epoch [250/500] Loss: 0.0932\n",
      "Epoch [300/500] Loss: 0.0885\n",
      "Epoch [350/500] Loss: 0.0876\n",
      "Epoch [400/500] Loss: 0.0845\n",
      "Epoch [450/500] Loss: 0.0818\n",
      "Epoch [500/500] Loss: 0.0809\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1481\n",
      "early stopping in epoch 95.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1255\n",
      "Epoch [100/100] Loss: 0.1117\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1488\n",
      "Epoch [100/300] Loss: 0.1452\n",
      "Epoch [150/300] Loss: 0.1445\n",
      "early stopping in epoch 163.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1282\n",
      "Epoch [100/300] Loss: 0.1140\n",
      "Epoch [150/300] Loss: 0.1062\n",
      "Epoch [200/300] Loss: 0.1003\n",
      "Epoch [250/300] Loss: 0.1006\n",
      "Epoch [300/300] Loss: 0.0923\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': True}\n",
      "Epoch [50/500] Loss: 0.1472\n",
      "Epoch [100/500] Loss: 0.1414\n",
      "early stopping in epoch 145.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0001, 'num_epochs': 500, 'early_stop': False}\n",
      "Epoch [50/500] Loss: 0.1266\n",
      "Epoch [100/500] Loss: 0.1140\n",
      "Epoch [150/500] Loss: 0.1041\n",
      "Epoch [200/500] Loss: 0.0971\n",
      "Epoch [250/500] Loss: 0.0940\n",
      "Epoch [300/500] Loss: 0.0931\n",
      "Epoch [350/500] Loss: 0.0915\n",
      "Epoch [400/500] Loss: 0.0895\n",
      "Epoch [450/500] Loss: 0.0899\n",
      "Epoch [500/500] Loss: 0.0863\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': True}\n",
      "Epoch [50/100] Loss: 0.1598\n",
      "early stopping in epoch 75.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 100, 'early_stop': False}\n",
      "Epoch [50/100] Loss: 0.1270\n",
      "Epoch [100/100] Loss: 0.1171\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': True}\n",
      "Epoch [50/300] Loss: 0.1503\n",
      "early stopping in epoch 89.\n",
      "config: {'batch_size': 128, 'hidden_layers': [64, 32, 16], 'dropout_rates': [0, 0, 0], 'batch_norm': True, 'criterion': 'BCE', 'optimizer': 'Adam', 'lr': 0.001, 'weight_decay': 0.0003, 'num_epochs': 300, 'early_stop': False}\n",
      "Epoch [50/300] Loss: 0.1278\n",
      "Epoch [100/300] Loss: 0.1158\n",
      "Epoch [150/300] Loss: 0.1066\n",
      "Epoch [200/300] Loss: 0.1022\n",
      "Epoch [250/300] Loss: 0.0979\n"
     ]
    }
   ],
   "source": [
    "tune(all_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f7637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
