{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8af354a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13c6e2df0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # used for data import\n",
    "import numpy as np # used for numerical operations\n",
    "import torch # used for tensor operations\n",
    "import torch.nn as nn # used for building neural networks\n",
    "from torch.utils.data import DataLoader, TensorDataset # used for creating data loaders\n",
    "from sklearn.preprocessing import StandardScaler # used for standardizing features\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score # used for evaluating models\n",
    "import statsmodels.api as sm # used for probit & logit regression\n",
    "import matplotlib.pyplot as plt # used for plotting PR curves\n",
    "\n",
    "np.random.seed(42) # set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c61a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../全连接神经网络/train_data.csv') # import training data\n",
    "test_df = pd.read_csv('../全连接神经网络/test_data.csv')\n",
    "train_df.head(5) # display first 5 observations of training data\n",
    "\n",
    "# transform DataFrame into numpy ndarray\n",
    "X_train_array = train_df.drop(columns=['noncompliance']).values\n",
    "y_train_array = train_df['noncompliance'].values\n",
    "X_test_array = test_df.drop(columns=['noncompliance']).values\n",
    "y_test_array = test_df['noncompliance'].values\n",
    "\n",
    "# standardize input features\n",
    "scaler = StandardScaler() # initialize the scaler\n",
    "X_train_array = scaler.fit_transform(X_train_array)\n",
    "X_test_array = scaler.transform(X_test_array) # standardize test data using the train scaler\n",
    "\n",
    "# transform numpy ndarray into torch.tensor\n",
    "X_train_tensor = torch.tensor(X_train_array, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_array, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_array, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_array, dtype=torch.float32)\n",
    "\n",
    "# bundle feature and label into TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True) # shuffle means randomize the order of observations\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4618fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connected neural network for binary classification.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(24, 64) # input layer: 25 features -> 64 neurons\n",
    "        self.fc2 = nn.Linear(64, 32) # hidden layer1: 64 neurons -> 32 neurons\n",
    "        self.fc3 = nn.Linear(32, 8) # hidden layer2: 32 neurons -> 8 neurons\n",
    "        self.fc4 = nn.Linear(8, 1) # hidden layer3: 8 neurons -> output probability\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)) # relu activation\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x)) # sigmoid activation\n",
    "        return x\n",
    "    \n",
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    \"\"\"\n",
    "    functions used to train the model\n",
    "    params:\n",
    "        model: the neural network to be trained;\n",
    "        train_loader: a DataLoader object, trainging dataset;\n",
    "        criterion: loss function;\n",
    "        optimizer: optimiser that updates parameters after backpropagation;\n",
    "        num_epochs: number of epochs in optimization;\n",
    "    return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.train() # set the model to training mode\n",
    "    for epoch in range(num_epochs): # loop over epochs\n",
    "        for batch_idx, (data, target) in enumerate(train_loader): # loop over batches\n",
    "            optimizer.zero_grad() # clear previous gradients\n",
    "            output = model(data) # do forward propagation\n",
    "            loss = criterion(output.squeeze(1), target) # calculate loss\n",
    "            loss.backward() # do backpropagation\n",
    "            optimizer.step() # update parameters\n",
    "\n",
    "            if (epoch + 1) % 50 == 0 and batch_idx == len(train_loader)-2: # print second last batch's loss every 50 epochs\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}] Batch {batch_idx+1} Loss: {loss.item():.4f}\")\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Run the trained model on the test set.\n",
    "    params:\n",
    "        model: the network after trained;\n",
    "        test_loader: DataLoader, test dataset;\n",
    "        criterion: loss function;\n",
    "    return:\n",
    "        pred_probs: tensor, predicted test probabilities;\n",
    "    \"\"\"\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    pred_probs = [] # a list used to accumulated each batch's predicted probabilities\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data).squeeze(1) # forward propagation using test data and return output probabilities\n",
    "            test_loss += criterion(output, target).item() # accumulate loss\n",
    "            pred_probs.append(output) # accumulate predicted probabilityes\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    return torch.cat(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca88a129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/500] Batch 193 Loss: 0.1711\n",
      "Epoch [100/500] Batch 193 Loss: 0.0876\n",
      "Epoch [150/500] Batch 193 Loss: 0.0569\n",
      "Epoch [200/500] Batch 193 Loss: 0.1984\n",
      "Epoch [250/500] Batch 193 Loss: 0.1597\n",
      "Epoch [300/500] Batch 193 Loss: 0.0789\n",
      "Epoch [350/500] Batch 193 Loss: 0.0718\n",
      "Epoch [400/500] Batch 193 Loss: 0.1066\n",
      "Epoch [450/500] Batch 193 Loss: 0.1127\n",
      "Epoch [500/500] Batch 193 Loss: 0.1025\n",
      "Test Loss: 0.0022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0025, 0.0002, 0.0036,  ..., 0.0020, 0.0198, 0.0034])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FNN() # instantiate the model\n",
    "criterion = nn.BCELoss() # binary cross entropy loss\n",
    "optimizer_adam = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "train(model, train_loader, criterion, optimizer_adam, num_epochs=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66970b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.3912, grad_fn=<SqrtBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def l2_norm_all_weights(model):\n",
    "    total_norm = 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name and param.requires_grad:\n",
    "            total_norm += torch.norm(param, p=2) ** 2\n",
    "    return total_norm.sqrt()\n",
    "\n",
    "l2_norm_all_weights(model) # check the l2 norm of all weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef0380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_loader, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
